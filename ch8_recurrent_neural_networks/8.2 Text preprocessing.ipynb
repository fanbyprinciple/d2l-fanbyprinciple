{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prescribed-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-holder",
   "metadata": {},
   "source": [
    "We will load HG Well time machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "integral-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "'090b5e7e70c295757f55df93cb0a180b9691891a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coated-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_time_machine():\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "outdoor-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_time_machine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "modified-auckland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3221\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "colonial-mandate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time machine by h g wells\n",
      "of man\n"
     ]
    }
   ],
   "source": [
    "print(lines[0])\n",
    "print(lines[3220])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-malpractice",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The following tokenize function takes a list (lines) as the input, where each element is a text\n",
    "sequence (e.g., a text line). Each text sequence is split into a list of tokens. A token is the basic unit\n",
    "in text. In the end, a list of token lists are returned, where each token is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "central-faculty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'): #@save\n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)\n",
    "        \n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-spencer",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "The string type of the token is inconvenient to be used by models, which take numerical inputs.\n",
    "Now let us build a dictionary, often called vocabulary as well, to map string tokens into numerical\n",
    "indices starting from 0. To do so, we first count the unique tokens in all the documents from the\n",
    "training set, namely a corpus, and then assign a numerical index to each unique token according to\n",
    "its frequency. Rarely appeared tokens are often removed to reduce the complexity. Any token that\n",
    "does not exist in the corpus or has been removed is mapped into a special unknown token “<unk>”.\n",
    "We optionally add a list of reserved tokens, such as “<pad>” for padding, “<bos>” to present the\n",
    "beginning for a sequence, and “<eos>” for the end of a sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adopted-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corpus(tokens):\n",
    "    if len(tokens)==0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "recreational-chorus",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2261), ('i', 1267), ('and', 1245), ('of', 1155), ('a', 816)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_collect = count_corpus(tokens)\n",
    "\n",
    "token_freq = sorted(new_collect.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "token_freq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "exciting-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens=None, min_freq=0,reserved_token=None ):\n",
    "        if tokens==None:\n",
    "            tokens = []\n",
    "        if reserved_token==None:\n",
    "            reserved_token = []\n",
    "        \n",
    "        counter = count_corpus(tokens)\n",
    "        # this line gives the freq list\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "        # keeping the unknown token to 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_token\n",
    "        self.token_to_idx = {token : idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    # called whenever indexed reference is made\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "    def unk(self):\n",
    "        return 0\n",
    "    \n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "quiet-modern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "young-photographer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words :  ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "indices :  [1, 19, 50, 40, 2183, 2184, 400]\n",
      "words :  ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "indices :  [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 10]:\n",
    "    print('words : ', tokens[i])\n",
    "    print('indices : ', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cheap-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    \n",
    "    corpus = [vocab[token] for line in tokens for token in line] \n",
    "    \n",
    "    if max_tokens>0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    \n",
    "    return corpus, vocab\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unlikely-allocation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus, vocab = load_corpus_time_machine()\n",
    "\n",
    "len(corpus),len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "complicated-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 2, 1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-complexity",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "1. Tokenization is a key preprocessing step. It varies for different languages. Try to find another\n",
    "three commonly used methods to tokenize text.\n",
    "\n",
    "    Different Methods to Perform Tokenization in Python\n",
    "    Tokenization using Python split() Function\n",
    "    Tokenization using Regular Expressions\n",
    "    Tokenization using NLTK\n",
    "    Tokenization using Spacy\n",
    "    Tokenization using Keras\n",
    "    Tokenization using Gensim\n",
    "\n",
    "2. In the experiment of this section, tokenize text into words and vary the min_freq arguments\n",
    "of the Vocab instance. How does this affect the vocabulary size?\n",
    "\n",
    "    as min_freq increases vocabulary size decreases apparently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
