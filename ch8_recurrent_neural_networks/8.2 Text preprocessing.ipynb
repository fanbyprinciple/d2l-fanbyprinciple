{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prescribed-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-holder",
   "metadata": {},
   "source": [
    "We will load HG Well time machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integral-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "'090b5e7e70c295757f55df93cb0a180b9691891a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coated-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_time_machine():\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outdoor-reservoir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ..\\data\\timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n"
     ]
    }
   ],
   "source": [
    "lines = read_time_machine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modified-auckland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3221\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "colonial-mandate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time machine by h g wells\n",
      "of man\n"
     ]
    }
   ],
   "source": [
    "print(lines[0])\n",
    "print(lines[3220])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-malpractice",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The following tokenize function takes a list (lines) as the input, where each element is a text\n",
    "sequence (e.g., a text line). Each text sequence is split into a list of tokens. A token is the basic unit\n",
    "in text. In the end, a list of token lists are returned, where each token is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "central-faculty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'): #@save\n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)\n",
    "        \n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-spencer",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "The string type of the token is inconvenient to be used by models, which take numerical inputs.\n",
    "Now let us build a dictionary, often called vocabulary as well, to map string tokens into numerical\n",
    "indices starting from 0. To do so, we first count the unique tokens in all the documents from the\n",
    "training set, namely a corpus, and then assign a numerical index to each unique token according to\n",
    "its frequency. Rarely appeared tokens are often removed to reduce the complexity. Any token that\n",
    "does not exist in the corpus or has been removed is mapped into a special unknown token “<unk>”.\n",
    "We optionally add a list of reserved tokens, such as “<pad>” for padding, “<bos>” to present the\n",
    "beginning for a sequence, and “<eos>” for the end of a sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens=None, min_freq=0,reserved_tokens=None ):\n",
    "        if tokens==None:\n",
    "            tokens = []\n",
    "        if reserved_token==None:\n",
    "            reserved_token = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
