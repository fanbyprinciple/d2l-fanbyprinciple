{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "academic-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smart-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "soviet-formula",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!curl --output pap.txt https://www.gutenberg.org/files/1342/1342-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "paperback-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "young-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt','090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine(): #@save\n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lonely-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_text(text_name='pap.txt'):\n",
    "    with open(text_name, 'r', encoding='utf-8') as text_input:\n",
    "        lines = text_input.readlines()\n",
    "    \n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines] # only alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "failing-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "danish-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = read_time_machine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "contemporary-serial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the time machine by h g wells', '', '', '', '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confidential-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token_type='word'):\n",
    "    if token_type == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    elif token_type == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    else:\n",
    "        'Wrong token type.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "intermediate-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "grand-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md\n",
    "class Vocab:\n",
    "    def __init__(self, tokens=None,min_freq=0):\n",
    "        if tokens == None:\n",
    "            tokens = []\n",
    "        \n",
    "        tokens = [token for line in tokens for token in line]\n",
    "        counter = collections.Counter(tokens)\n",
    "        \n",
    "        self._token_freqs = sorted(counter.items(),key=lambda x:x[1], reverse=True)\n",
    "        self.idx_to_token = ['<unk>']\n",
    "        self.token_to_idx = { token:idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                #print(f\"for {freq} : {token} breaking\")\n",
    "                continue\n",
    "            elif token not in self.idx_to_token:\n",
    "                #print(token + \"adding\")\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token)-1\n",
    "            else:\n",
    "                print(token + \" found already\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if isinstance(tokens, list):\n",
    "            return_list = []\n",
    "            for t in tokens:\n",
    "                for token in t:\n",
    "                    if token in self.token_to_idx.keys():\n",
    "                        return_list.append(self.token_to_idx[token])\n",
    "                    else:\n",
    "                        return_list.append(self.unk)\n",
    "            return return_list\n",
    "        else:\n",
    "            if tokens in self.token_to_idx.keys():\n",
    "                return self.token_to_idx[tokens]\n",
    "            else:\n",
    "                return self.unk\n",
    "            \n",
    "    def to_tokens(self, indices):\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):  # Index for the unknown token\n",
    "        return self._token_freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "occupied-johns",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'time', 'machine', 'by', 'h', 'g', 'wells']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize(text_input)\n",
    "tokens[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "photographic-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = [token for line in tokens for token in line]\n",
    "# tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "outer-bracket",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab = Vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "crucial-stuart",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 2261)\n"
     ]
    }
   ],
   "source": [
    "for i in vocab.token_freqs:\n",
    "    if i[0] == 'the':\n",
    "        print(i)\n",
    "# print(vocab.token_freqs)\n",
    "# vocab.token_to_idx\n",
    "\n",
    "# the error is that we are using the entire sentece for creating token\n",
    "# problem in tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "moving-antigua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "visible-messenger",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2261), ('i', 1267), ('and', 1245), ('of', 1155), ('a', 816)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_freqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "grand-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(max_tokens=-1, text_input=text_input):\n",
    "    tokens = tokenize(text_input)\n",
    "    vocab = Vocab(tokens)\n",
    "    \n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "behind-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocab = load_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "negative-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader:\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else :\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        \n",
    "        self.corpus, self.vocab = load_corpus(max_tokens=max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "annoying-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    print(\"Dont use random seq data iter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "indonesian-casino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_steps = 5\n",
    "batch_size=5\n",
    "offset = random.randint(0, num_steps)\n",
    "    \n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "greek-equity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32773"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)-offset-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "czech-median",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32770"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = (len(corpus) -offset -1)//batch_size * batch_size # ensuring num_tokens is perfectly divisible\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "flying-pavilion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32770, tensor([  19,   50,   40, 2183, 2184]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = torch.tensor(corpus[offset:offset+num_tokens])\n",
    "len(Xs),Xs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "american-success",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32770, tensor([  50,   40, 2183, 2184,  400]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ys = torch.tensor(corpus[offset+1:offset+1+num_tokens])\n",
    "len(Ys),Ys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "changed-region",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " tensor([[  19,   50,   40,  ...,    7, 2658,   10],\n",
       "         [ 745,   72,   42,  ...,  670,    3,   87],\n",
       "         [  29,  246,  160,  ...,   23,    1,  818],\n",
       "         [  16,    1, 1378,  ...,  148,   72,   33],\n",
       "         [ 504, 4128,   16,  ...,  635,   23,    8]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = Xs.reshape(batch_size,-1)\n",
    "len(Xs), Xs[:5]\n",
    "\n",
    "# okay Xs devided into 5 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "starting-institution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1310"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = Xs.shape[1]//num_steps\n",
    "num_batches\n",
    "\n",
    "# these many times we will go through the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "laden-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus)-offset-1)//batch_size)*batch_size\n",
    "    \n",
    "    Xs = torch.tensor(corpus[offset:offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1:offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1]//num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        Xs = Xs[i:i+num_steps]\n",
    "        Ys = Ys[i:i+num_steps]\n",
    "        yield X, Y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "political-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, num_steps, use_random_iter=False, max_tokens=10000):\n",
    "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)\n",
    "    \n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "intermediate-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "executive-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying the book way\n",
    "\n",
    "# batch_size, num_steps = 32, 35\n",
    "# train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-great",
   "metadata": {},
   "source": [
    "book way is working fine showing that there is some error in tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "saving-joining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding\n",
    "\n",
    "F.one_hot(torch.tensor([0,2]), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "thirty-camel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).reshape((2,5))\n",
    "F.one_hot(X.T, 28,).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "first-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size , hidden_size, device):\n",
    "    input_size = output_size = vocab_size\n",
    "    \n",
    "    Wxh = torch.randn(size=(input_size, hidden_size), device=device) * 0.01\n",
    "    Whh = torch.randn(size=(hidden_size, hidden_size), device=device) * 0.01\n",
    "    \n",
    "    bh = torch.zeros((hidden_size), device=device)\n",
    "    \n",
    "    Whq = torch.randn(size=(hidden_size, output_size),device=device)\n",
    "    bq = torch.zeros((output_size), device=device)\n",
    "    \n",
    "    params = [Wxh, Whh, bh, Whq, bq]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-photograph",
   "metadata": {},
   "source": [
    "### RNN Model\n",
    "To define an RNN model, we first need an init_rnn_state function to return the hidden state at\n",
    "initialization. It returns a tensor filled with 0 and with a shape of (batch size, number of hidden\n",
    "units). Using tuples makes it easier to handle situations where the hidden state contains multiple\n",
    "variables, which we will encounter in later sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "handy-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a torch of zeros\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "timely-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, = init_rnn_state(2, 512, torch.device('cuda'))\n",
    "\n",
    "## init state is suppposed to return a tuple so handle accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "national-oklahoma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "diagnostic-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = init_rnn_state(2,512, torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "broad-character",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-meditation",
   "metadata": {},
   "source": [
    "The following rnn function defines how to compute the hidden state and output at a time step.\n",
    "Note that the RNN model loops through the outermost dimension of inputs so that it updates\n",
    "hidden states H of a minibatch, time step by time step. Besides, the activation function here uses\n",
    "the tanh function. As described in Section 4.1, the mean value of the tanh function is 0, when the\n",
    "elements are uniformly distributed over the real numbers.\n",
    "## to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "spoken-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    W_xh, W_hh, b_h, W_qh, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    #print(inputs.shape)\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.mm(X , W_xh)) + (torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_qh) + b_q\n",
    "        outputs.append(Y)\n",
    "    \n",
    "    return torch.cat(outputs, dim=0),(H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-lightweight",
   "metadata": {},
   "source": [
    "With all the needed functions being defined, next we create a class to wrap these functions and\n",
    "store parameters for an RNN model implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "subtle-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch:\n",
    "    def __init__(self, vocab_size, hidden_size,device, get_params, init_state,forward_fn):\n",
    "        self.vocab_size, self.hidden_size = vocab_size, hidden_size\n",
    "        self.forward_fn = forward_fn\n",
    "        self.init_state = init_state\n",
    "        self.params = get_params(vocab_size, hidden_size, device)\n",
    "    \n",
    "    def __call__(self,X, state ):\n",
    "        #print(X.shape)\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        #print(X.shape)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "    \n",
    "    def begin_state(self,batch_size, device):\n",
    "        return self.init_state(batch_size, self.hidden_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-moisture",
   "metadata": {},
   "source": [
    "Let us check whether the outputs have the correct shapes, e.g., to ensure that the dimensionality\n",
    "of the hidden state remains unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "forty-imperial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 4],\n",
       "         [5, 6, 7, 8, 9]]),\n",
       " 2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-burke",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aggregate-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if output gave fixd dusze\n",
    "num_hiddens = 512\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, torch.device('cuda'), get_params, init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0], torch.device('cuda'))\n",
    "Y,new_state = net(X.to(torch.device('cuda')), state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "continent-layout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape, len(new_state), new_state[0].shape\n",
    "\n",
    "#We can see that the output shape is (number of time steps × batch size, vocabulary size), \n",
    "#while the hidden state shape remains the same, i.e., (batch size, number of hidden units).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-lover",
   "metadata": {},
   "source": [
    "We can see that the output shape is (number of time steps × batch size, vocabulary size), while the\n",
    "hidden state shape remains the same, i.e., (batch size, number of hidden units).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-whale",
   "metadata": {},
   "source": [
    "Let us first define the prediction function to generate new characters following the user-provided\n",
    "prefix, which is a string containing several characters. When looping through these beginning\n",
    "characters in prefix, we keep passing the hidden state to the next time step without generating\n",
    "any output. This is called the warm-up period, during which the model updates itself (e.g., update\n",
    "the hidden state) but does not make predictions. After the warm-up period, the hidden state is\n",
    "generally better than its initialized value at the beginning. So we generate the predicted characters\n",
    "and emit them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "interesting-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix,num_preds, net, vocab,device):\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1,1))\n",
    "    for y in prefix[1:]:\n",
    "        #print(get_input())\n",
    "        _,state = net(get_input(),state)\n",
    "        outputs.append(vocab[y])\n",
    "    \n",
    "    for _ in range(num_preds):\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "acceptable-episode",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timess traveller cbojrfe oj'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ch8('timess traveller ', 10, net, vocab, d2l.try_gpu())\n",
    "# generating the same output everytime "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
