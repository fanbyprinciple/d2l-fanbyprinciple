{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "academic-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smart-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "soviet-formula",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!curl --output pap.txt https://www.gutenberg.org/files/1342/1342-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lonely-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_text(text_name='pap.txt'):\n",
    "    with open(text_name, 'r', encoding='utf-8') as text_input:\n",
    "        lines = text_input.readlines()\n",
    "    \n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines] # only alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "failing-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contemporary-serial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of pride and prejudice by jane austen',\n",
       " '',\n",
       " 'this ebook is for the use of anyone anywhere in the united states and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever you may copy it give it away or re use it under the terms']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "confidential-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token_type='word'):\n",
    "    if token_type == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    elif token_type == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    else:\n",
    "        'Wrong token type.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "intermediate-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "grand-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined in file: ./chapter_recurrent-neural-networks/text-preprocessing.md\n",
    "class Vocab:\n",
    "    def __init__(self, tokens=None,min_freq=0):\n",
    "        if tokens == None:\n",
    "            tokens = []\n",
    "        \n",
    "        tokens = [token for line in tokens for token in line]\n",
    "        counter = collections.Counter(tokens)\n",
    "        \n",
    "        self._token_freqs = sorted(counter.items(),key=lambda x:x[1], reverse=True)\n",
    "        self.idx_to_token = ['<unk>']\n",
    "        self.token_to_idx = { token:idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                #print(f\"for {freq} : {token} breaking\")\n",
    "                continue\n",
    "            elif token not in self.idx_to_token:\n",
    "                #print(token + \"adding\")\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token)-1\n",
    "            else:\n",
    "                print(token + \" found already\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if isinstance(tokens, list):\n",
    "            return_list = []\n",
    "            for t in tokens:\n",
    "                for token in t:\n",
    "                    if token in self.token_to_idx.keys():\n",
    "                        return_list.append(self.token_to_idx[token])\n",
    "                    else:\n",
    "                        return_list.append(self.unk)\n",
    "            return return_list\n",
    "        else:\n",
    "            if tokens in self.token_to_idx.keys():\n",
    "                return self.token_to_idx[tokens]\n",
    "            else:\n",
    "                return self.unk\n",
    "            \n",
    "    def to_tokens(self, indices):\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):  # Index for the unknown token\n",
    "        return self._token_freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "occupied-johns",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'ebook',\n",
       "  'of',\n",
       "  'pride',\n",
       "  'and',\n",
       "  'prejudice',\n",
       "  'by',\n",
       "  'jane',\n",
       "  'austen']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize(text_input)\n",
    "tokens[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rapid-idaho",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'project', 'gutenberg', 'ebook', 'of']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = [token for line in tokens for token in line]\n",
    "# tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "blessed-ottawa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab = Vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "crucial-stuart",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 4521)\n"
     ]
    }
   ],
   "source": [
    "for i in vocab.token_freqs:\n",
    "    if i[0] == 'the':\n",
    "        print(i)\n",
    "# print(vocab.token_freqs)\n",
    "# vocab.token_to_idx\n",
    "\n",
    "# the error is that we are using the entire sentece for creating token\n",
    "# problem in tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "behind-paper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "introductory-mobility",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4521), ('to', 4246), ('of', 3735), ('and', 3657), ('her', 2226)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_freqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "grand-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(max_tokens=-1, text_input=text_input):\n",
    "    tokens = tokenize(text_input)\n",
    "    vocab = Vocab(tokens)\n",
    "    \n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "behind-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocab = load_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "popular-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader:\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else :\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        \n",
    "        self.corpus, self.vocab = load_corpus(max_tokens=max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "authorized-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    print(\"Dont use random seq data iter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "disabled-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_steps = 5\n",
    "batch_size=5\n",
    "offset = random.randint(0, num_steps)\n",
    "    \n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "coastal-mistake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126018"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)-offset-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "controlling-destination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126015"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = (len(corpus) -offset -1)//batch_size * batch_size # ensuring num_tokens is perfectly divisible\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "therapeutic-performance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126015, tensor([179, 160, 910,   3, 317]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = torch.tensor(corpus[offset:offset+num_tokens])\n",
    "len(Xs),Xs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "provincial-affair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126015, tensor([160, 910,   3, 317,   4]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ys = torch.tensor(corpus[offset+1:offset+1+num_tokens])\n",
    "len(Ys),Ys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fiscal-commerce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " tensor([[ 179,  160,  910,  ...,  372,    3, 3438],\n",
       "         [  44,  415,    3,  ...,   36,    2,  438],\n",
       "         [  11,    1,  800,  ...,   19,   41,  173],\n",
       "         [   8,  470, 5558,  ...,   33,    9,  417],\n",
       "         [   3,   12,  170,  ..., 4057, 6522,    2]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = Xs.reshape(batch_size,-1)\n",
    "len(Xs), Xs[:5]\n",
    "\n",
    "# okay Xs devided into 5 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "herbal-species",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5040"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches = Xs.shape[1]//num_steps\n",
    "num_batches\n",
    "\n",
    "# these many times we will go through the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "olympic-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus)-offset-1)//batch_size)*batch_size\n",
    "    \n",
    "    Xs = torch.tensor(corpus[offset:offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1:offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1]//num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        Xs = Xs[i:i+num_steps]\n",
    "        Ys = Ys[i:i+num_steps]\n",
    "        yield X, Y\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
