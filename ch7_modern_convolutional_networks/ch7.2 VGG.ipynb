{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "polished-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a vgg block\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels=out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ethical-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing VGG11\n",
    "\n",
    "conv_arch = ((1,64), (1,128), (2,256), (2,512), (2,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surrounded-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    \n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels=out_channels\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        *conv_blks, nn.Flatten(), \n",
    "        nn.Linear(in_channels *7 *7, 4096), nn.ReLU(), nn.Dropout(p=0.5), \n",
    "        nn.Linear(4096, 4096),nn.ReLU(), nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "conservative-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "powerful-veteran",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Sequential: torch.Size([1, 64, 112, 112])\n",
      "For Sequential: torch.Size([1, 128, 56, 56])\n",
      "For Sequential: torch.Size([1, 256, 28, 28])\n",
      "For Sequential: torch.Size([1, 512, 14, 14])\n",
      "For Sequential: torch.Size([1, 512, 7, 7])\n",
      "For Flatten: torch.Size([1, 25088])\n",
      "For Linear: torch.Size([1, 4096])\n",
      "For ReLU: torch.Size([1, 4096])\n",
      "For Dropout: torch.Size([1, 4096])\n",
      "For Linear: torch.Size([1, 4096])\n",
      "For ReLU: torch.Size([1, 4096])\n",
      "For Dropout: torch.Size([1, 4096])\n",
      "For Linear: torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\installs\\anconda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1,1,224,224)\n",
    "\n",
    "def look_at_net(net, X):\n",
    "    out = X\n",
    "    for layer in net:\n",
    "        out = layer(out)\n",
    "        print(f'For {layer.__class__.__name__}: {out.shape}')\n",
    "        \n",
    "look_at_net(vgg_net, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chief-western",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Sequential: torch.Size([1, 16, 112, 112])\n",
      "For Sequential: torch.Size([1, 32, 56, 56])\n",
      "For Sequential: torch.Size([1, 64, 28, 28])\n",
      "For Sequential: torch.Size([1, 128, 14, 14])\n",
      "For Sequential: torch.Size([1, 128, 7, 7])\n",
      "For Flatten: torch.Size([1, 6272])\n",
      "For Linear: torch.Size([1, 4096])\n",
      "For ReLU: torch.Size([1, 4096])\n",
      "For Dropout: torch.Size([1, 4096])\n",
      "For Linear: torch.Size([1, 4096])\n",
      "For ReLU: torch.Size([1, 4096])\n",
      "For Dropout: torch.Size([1, 4096])\n",
      "For Linear: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# we reduce the ratio as it is very computationaly expensive to traina VGG model\n",
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1]//ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)\n",
    "\n",
    "look_at_net(net, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aerial-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.01, 20, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "executive-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "my_transforms = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(224),\n",
    "                    transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(download=False,root=\"../data\", train=True, transform=my_transforms)\n",
    "test_dataset = datasets.FashionMNIST(download=False, root=\"../data\", train=False, transform=my_transforms)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "protecting-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "conscious-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat,y):\n",
    "    return (y_hat.argmax(1)==y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "talented-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affiliated-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_accuracy(net, data_iter):\n",
    "    net.eval()\n",
    "#     device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    net = net.to(device)\n",
    "    \n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = net(X)\n",
    "\n",
    "            total_acc += accuracy(y_hat, y)\n",
    "            total_num += y.numel()\n",
    "    \n",
    "    return total_acc/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "superior-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net): \n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "\n",
    "    net= net.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        acc_value = 0\n",
    "        total_number = 0\n",
    "        total_loss= 0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            acc_value += accuracy(outputs, labels)\n",
    "            total_number += labels.numel()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            print(f\"\\tEpoch {epoch} : Statistics: \")\n",
    "            print(f'\\tcurrent train loss : {total_loss} / {total_number} : {float(total_loss/total_number)}')\n",
    "            print(f'\\tcurrent train acc : {acc_value}/{total_number} : {float(acc_value/total_number)}')\n",
    "            print(f'\\tcurrent test acc : {float(full_accuracy(net, test_dataloader))}')\n",
    "\n",
    "\n",
    "            train_loss.append(float(total_loss/total_number))\n",
    "            test_acc.append(float(full_accuracy(net, test_dataloader)))\n",
    "            train_acc.append(float(acc_value/total_number))\n",
    "    \n",
    "    return train_loss, test_acc, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "harmful-armenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 224, 224]) 128\n",
      "For Sequential: torch.Size([128, 16, 112, 112])\n",
      "For Sequential: torch.Size([128, 32, 56, 56])\n",
      "For Sequential: torch.Size([128, 64, 28, 28])\n",
      "For Sequential: torch.Size([128, 128, 14, 14])\n",
      "For Sequential: torch.Size([128, 128, 7, 7])\n",
      "For Flatten: torch.Size([128, 6272])\n",
      "For Linear: torch.Size([128, 4096])\n",
      "For ReLU: torch.Size([128, 4096])\n",
      "For Dropout: torch.Size([128, 4096])\n",
      "For Linear: torch.Size([128, 4096])\n",
      "For ReLU: torch.Size([128, 4096])\n",
      "For Dropout: torch.Size([128, 4096])\n",
      "For Linear: torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(X.shape, len(y))\n",
    "    look_at_net(net.to(device), X.to(device).float())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "settled-answer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 4.00 GiB total capacity; 2.69 GiB already allocated; 15.23 MiB free; 2.70 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_loss, test_acc, train_acc = train_net(net)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "determined-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# its difficult to train so\n",
    "# 1. lets reduce the image size\n",
    "# 2. and also create a smaller architecture\n",
    "my_transforms = transforms.Compose(\n",
    "                [\n",
    "#                     transforms.Resize(224),\n",
    "                    transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(download=False,root=\"../data\", train=True, transform=my_transforms)\n",
    "test_dataset = datasets.FashionMNIST(download=False, root=\"../data\", train=False, transform=my_transforms)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "practical-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So it is difficult to run this lets reduce the model even further\n",
    "\n",
    "new_conv_arch = ((1,64), (1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reserved-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_reduced(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    \n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels=out_channels\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        *conv_blks, nn.Flatten(), \n",
    "        nn.Linear(in_channels *7 *7, 4096), nn.ReLU(), nn.Dropout(p=0.5), \n",
    "        nn.Linear(4096, 10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "verbal-natural",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Sequential: torch.Size([1, 64, 14, 14])\n",
      "For Sequential: torch.Size([1, 128, 7, 7])\n",
      "For Flatten: torch.Size([1, 6272])\n",
      "For Linear: torch.Size([1, 4096])\n",
      "For ReLU: torch.Size([1, 4096])\n",
      "For Dropout: torch.Size([1, 4096])\n",
      "For Linear: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1,1,28,28)\n",
    "\n",
    "net = vgg_reduced(new_conv_arch)\n",
    "\n",
    "look_at_net(net, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "organic-australia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 : Statistics: \n",
      "\tcurrent train loss : 1080.0545313358307 / 60000 : 0.01800090885559718\n",
      "\tcurrent train acc : 5439/60000 : 0.09064999967813492\n",
      "\tcurrent test acc : 0.09959999471902847\n",
      "\tEpoch 1 : Statistics: \n",
      "\tcurrent train loss : 1079.7904839515686 / 60000 : 0.017996508065859475\n",
      "\tcurrent train acc : 5976/60000 : 0.09960000216960907\n",
      "\tcurrent test acc : 0.09959999471902847\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lr = 0.1\n",
    "    num_epochs=2\n",
    "    train_loss, test_acc, train_acc = train_net(net)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pregnant-cooper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"finished\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "num_epochs = num_epochs\n",
    "plt.plot(range(num_epochs), train_loss, label='train loss')\n",
    "plt.plot(range(num_epochs), train_acc, label = 'train acc')\n",
    "plt.plot(range(num_epochs), test_acc, label = 'test acc')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-closure",
   "metadata": {},
   "source": [
    "## Training predefined VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "devoted-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transforms = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(224),\n",
    "                    transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(download=False,root=\"../data\", train=True, transform=my_transforms)\n",
    "test_dataset = datasets.FashionMNIST(download=False, root=\"../data\", train=False, transform=my_transforms)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bulgarian-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "\n",
    "# Suppose you are trying to load pre-trained resnet model in directory- models\\resnet\n",
    "\n",
    "os.environ['TORCH_HOME'] = '../models' #setting the environment variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "heavy-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_predefined = torchvision.models.vgg16(pretrained=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "upper-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg_predefined.features.parameters(): # only freezing convolutional weights\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "honest-stomach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_predefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "opposite-section",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_predefined.features[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "imposed-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_predefined.features[0] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "accomplished-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_predefined.classifier[6] = nn.Linear(in_features=4096, out_features=10, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "reported-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(vgg_predefined.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "assigned-consumer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 4.00 GiB total capacity; 2.25 GiB already allocated; 423.23 MiB free; 2.30 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lr = 0.05\n",
    "    num_epochs=4\n",
    "    train_loss, test_acc, train_acc = train_net(vgg_predefined)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-explanation",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. When printing out the dimensions of the layers we only saw 8 results rather than 11. Where\n",
    "did the remaining 3 layer information go?\n",
    "\n",
    "* in maxpool\n",
    "\n",
    "2. Compared with AlexNet, VGG is much slower in terms of computation, and it also needs\n",
    "more GPU memory. Analyze the reasons for this.\n",
    "\n",
    "* it has more conv layers\n",
    "\n",
    "3. Try changing the height and width of the images in Fashion-MNIST from 224 to 96. What\n",
    "influence does this have on the experiments?\n",
    "\n",
    "* it willrun faster\n",
    "\n",
    "4. Refer to Table 1 in the VGG paper (Simonyan & Zisserman, 2014) to construct other common\n",
    "models, such as VGG-16 or VGG-19.\n",
    "\n",
    "* it can be done but my GPU says hi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
