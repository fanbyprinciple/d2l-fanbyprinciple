{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_channels, num_channels):\n",
    "    return nn.Sequential(nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "                         nn.Conv2d(input_channels, num_channels,kernel_size=3, padding=1))\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_block(nn.Module):\n",
    "    def __init__(self, num_convs, input_channels, num_channels):\n",
    "        super(dense_block, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(num_convs):\n",
    "            layers.append(conv_block(input_channels + num_channels* i, num_channels))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.net:\n",
    "            y = layer(X)\n",
    "            X = torch.cat((X,y), dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = dense_block(2, 3, 10)\n",
    "\n",
    "X = torch.randn(4,3,8,8)\n",
    "\n",
    "y = blk(X)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(23,10)\n",
    "blk(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Densenet\n",
    "\n",
    "b1 = nn.Sequential(nn.Conv2d(1,64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 64\n",
    "growth_rate = 32\n",
    "\n",
    "num_of_convs_in_dense_blocks = [4,4,4,4]\n",
    "\n",
    "blks = []\n",
    "\n",
    "for i, num_convs in enumerate(num_of_convs_in_dense_blocks):\n",
    "    blks.append(dense_block(num_convs, num_channels, growth_rate))\n",
    "    num_channels += num_convs * growth_rate\n",
    "    if i != len(num_of_convs_in_dense_blocks) - 1:\n",
    "        blks.append(transition_block(num_channels, num_channels//2))\n",
    "        num_channels = num_channels // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net= nn.Sequential(b1, *blks, nn.BatchNorm2d(num_channels),nn.ReLU(),\n",
    "                   nn.AdaptiveMaxPool2d((1,1)), nn.Flatten(), nn.Linear(num_channels, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): dense_block(\n",
       "    (net): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (3): dense_block(\n",
       "    (net): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(224, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (5): dense_block(\n",
       "    (net): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(112, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(144, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(176, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(208, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(240, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (7): dense_block(\n",
       "    (net): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(120, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(152, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(184, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(216, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (8): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): ReLU()\n",
       "  (10): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "  (11): Flatten(start_dim=1, end_dim=-1)\n",
       "  (12): Linear(in_features=248, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "#d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\installs\\anconda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 48, 48]           3,200\n",
      "       BatchNorm2d-2           [-1, 64, 48, 48]             128\n",
      "              ReLU-3           [-1, 64, 48, 48]               0\n",
      "         MaxPool2d-4           [-1, 64, 24, 24]               0\n",
      "       BatchNorm2d-5           [-1, 64, 24, 24]             128\n",
      "              ReLU-6           [-1, 64, 24, 24]               0\n",
      "            Conv2d-7           [-1, 32, 24, 24]          18,464\n",
      "       BatchNorm2d-8           [-1, 96, 24, 24]             192\n",
      "              ReLU-9           [-1, 96, 24, 24]               0\n",
      "           Conv2d-10           [-1, 32, 24, 24]          27,680\n",
      "      BatchNorm2d-11          [-1, 128, 24, 24]             256\n",
      "             ReLU-12          [-1, 128, 24, 24]               0\n",
      "           Conv2d-13           [-1, 32, 24, 24]          36,896\n",
      "      BatchNorm2d-14          [-1, 160, 24, 24]             320\n",
      "             ReLU-15          [-1, 160, 24, 24]               0\n",
      "           Conv2d-16           [-1, 32, 24, 24]          46,112\n",
      "      dense_block-17          [-1, 192, 24, 24]               0\n",
      "      BatchNorm2d-18          [-1, 192, 24, 24]             384\n",
      "             ReLU-19          [-1, 192, 24, 24]               0\n",
      "           Conv2d-20           [-1, 96, 24, 24]          18,528\n",
      "        AvgPool2d-21           [-1, 96, 12, 12]               0\n",
      "      BatchNorm2d-22           [-1, 96, 12, 12]             192\n",
      "             ReLU-23           [-1, 96, 12, 12]               0\n",
      "           Conv2d-24           [-1, 32, 12, 12]          27,680\n",
      "      BatchNorm2d-25          [-1, 128, 12, 12]             256\n",
      "             ReLU-26          [-1, 128, 12, 12]               0\n",
      "           Conv2d-27           [-1, 32, 12, 12]          36,896\n",
      "      BatchNorm2d-28          [-1, 160, 12, 12]             320\n",
      "             ReLU-29          [-1, 160, 12, 12]               0\n",
      "           Conv2d-30           [-1, 32, 12, 12]          46,112\n",
      "      BatchNorm2d-31          [-1, 192, 12, 12]             384\n",
      "             ReLU-32          [-1, 192, 12, 12]               0\n",
      "           Conv2d-33           [-1, 32, 12, 12]          55,328\n",
      "      dense_block-34          [-1, 224, 12, 12]               0\n",
      "      BatchNorm2d-35          [-1, 224, 12, 12]             448\n",
      "             ReLU-36          [-1, 224, 12, 12]               0\n",
      "           Conv2d-37          [-1, 112, 12, 12]          25,200\n",
      "        AvgPool2d-38            [-1, 112, 6, 6]               0\n",
      "      BatchNorm2d-39            [-1, 112, 6, 6]             224\n",
      "             ReLU-40            [-1, 112, 6, 6]               0\n",
      "           Conv2d-41             [-1, 32, 6, 6]          32,288\n",
      "      BatchNorm2d-42            [-1, 144, 6, 6]             288\n",
      "             ReLU-43            [-1, 144, 6, 6]               0\n",
      "           Conv2d-44             [-1, 32, 6, 6]          41,504\n",
      "      BatchNorm2d-45            [-1, 176, 6, 6]             352\n",
      "             ReLU-46            [-1, 176, 6, 6]               0\n",
      "           Conv2d-47             [-1, 32, 6, 6]          50,720\n",
      "      BatchNorm2d-48            [-1, 208, 6, 6]             416\n",
      "             ReLU-49            [-1, 208, 6, 6]               0\n",
      "           Conv2d-50             [-1, 32, 6, 6]          59,936\n",
      "      dense_block-51            [-1, 240, 6, 6]               0\n",
      "      BatchNorm2d-52            [-1, 240, 6, 6]             480\n",
      "             ReLU-53            [-1, 240, 6, 6]               0\n",
      "           Conv2d-54            [-1, 120, 6, 6]          28,920\n",
      "        AvgPool2d-55            [-1, 120, 3, 3]               0\n",
      "      BatchNorm2d-56            [-1, 120, 3, 3]             240\n",
      "             ReLU-57            [-1, 120, 3, 3]               0\n",
      "           Conv2d-58             [-1, 32, 3, 3]          34,592\n",
      "      BatchNorm2d-59            [-1, 152, 3, 3]             304\n",
      "             ReLU-60            [-1, 152, 3, 3]               0\n",
      "           Conv2d-61             [-1, 32, 3, 3]          43,808\n",
      "      BatchNorm2d-62            [-1, 184, 3, 3]             368\n",
      "             ReLU-63            [-1, 184, 3, 3]               0\n",
      "           Conv2d-64             [-1, 32, 3, 3]          53,024\n",
      "      BatchNorm2d-65            [-1, 216, 3, 3]             432\n",
      "             ReLU-66            [-1, 216, 3, 3]               0\n",
      "           Conv2d-67             [-1, 32, 3, 3]          62,240\n",
      "      dense_block-68            [-1, 248, 3, 3]               0\n",
      "      BatchNorm2d-69            [-1, 248, 3, 3]             496\n",
      "             ReLU-70            [-1, 248, 3, 3]               0\n",
      "AdaptiveMaxPool2d-71            [-1, 248, 1, 1]               0\n",
      "          Flatten-72                  [-1, 248]               0\n",
      "           Linear-73                   [-1, 10]           2,490\n",
      "================================================================\n",
      "Total params: 758,226\n",
      "Trainable params: 758,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 14.29\n",
      "Params size (MB): 2.89\n",
      "Estimated Total Size (MB): 17.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(net.to(torch.device('cuda')), (1,96,96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Why do we use average pooling rather than maximum pooling in the transition layer?\n",
    "- so that theother cells may not be eliminated and have some value addition in the model\n",
    "\n",
    "2. One of the advantages mentioned in the DenseNet paper is that its model parameters are\n",
    "smaller than those of ResNet. Why is this the case?\n",
    "- because of less number of linear layers, more memory is utilised instead disk space( I am ssuming because of the follow up question given below)\n",
    "\n",
    "```\n",
    "Total params: 758,226\n",
    "Trainable params: 758,226\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.04\n",
    "Forward/backward pass size (MB): 14.29\n",
    "Params size (MB): 2.89\n",
    "Estimated Total Size (MB): 17.22\n",
    "```\n",
    "\n",
    "This is the disk space, while the memory used in 3.1 GB.\n",
    "\n",
    "3. One problem for which DenseNet has been criticized is its high memory consumption.\n",
    "    1. Is this really the case? Try to change the input shape to 224 × 224 to see the actual GPU\n",
    "    memory consumption.\n",
    "    ```\n",
    "    Total params: 758,226\n",
    "    Trainable params: 758,226\n",
    "    Non-trainable params: 0\n",
    "    ----------------------------------------------------------------\n",
    "    Input size (MB): 0.19\n",
    "    Forward/backward pass size (MB): 77.81\n",
    "    Params size (MB): 2.89\n",
    "    Estimated Total Size (MB): 80.89\n",
    "    ```\n",
    "    \n",
    "    2. Can you think of an alternative means of reducing the memory consumption? How\n",
    "    would you need to change the framework?\n",
    "    - no Idea. Maybe focus on storing varibale rather than using it in the memory.\n",
    "    \n",
    "4. Implement the various DenseNet versions presented in Table 1 of the DenseNet paper\n",
    "(Huang et al., 2017).\n",
    "\n",
    "- https://arxiv.org/pdf/1608.06993.pdf\n",
    "\n",
    "5. Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price\n",
    "prediction task in Section 4.10.\n",
    "\n",
    "-  This is a task! But how? I am low on motivation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for densenet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    return (torch.argmax(y_hat, dim=1) == y).sum().float().mean()\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m)==nn.Linear or type(m)==nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "def train_net(net, train_dataloader, test_dataloader, n_epochs=10, lr=0.1, batch_size=256,device=torch.device('cuda')):\n",
    "        \n",
    "    net.apply(init_weights)\n",
    "    print('training on: ', device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr )\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_acc_arr = []\n",
    "    train_loss_arr = []\n",
    "    test_acc_arr = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        net.train()\n",
    "        train_acc = 0\n",
    "        train_loss = 0\n",
    "        number = 0\n",
    "        for X, y in train_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "        \n",
    "            y_hat = net(X)\n",
    "            train_acc += accuracy(y_hat, y).item()\n",
    "            #print(train_acc * len(y))\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_criterion(y_hat, y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            #print(train_loss*10, train_acc/10)\n",
    "            \n",
    "            number += len(y)\n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "        \n",
    "        train_acc_arr.append(train_acc/number)\n",
    "        train_loss_arr.append((train_loss/number) *100)\n",
    "        \n",
    "        net.eval()\n",
    "        test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            number_test = 0 \n",
    "            for X, y in test_dataloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "            \n",
    "                y_hat = net(X)\n",
    "                test_acc += accuracy(y_hat, y).item()\n",
    "                number_test += len(y)\n",
    "        \n",
    "            test_acc_arr.append(test_acc/number_test)\n",
    "                \n",
    "        print(f'train_acc, train_loss, test_acc : {train_acc_arr[-1]}, {train_loss_arr[-1]}, {test_acc_arr[-1]}')\n",
    "    \n",
    "    \n",
    "    plt.plot(range(n_epochs),train_acc_arr )\n",
    "    plt.plot(range(n_epochs),test_acc_arr )\n",
    "    plt.plot(range(n_epochs),train_loss_arr)\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on:  cuda\n",
      "train_acc, train_loss, test_acc : 0.7872166666666667, 0.2578543589512507, 0.8427\n",
      "train_acc, train_loss, test_acc : 0.8799, 0.12611655476192635, 0.8329\n",
      "train_acc, train_loss, test_acc : 0.9009333333333334, 0.10417473054180543, 0.8771\n",
      "train_acc, train_loss, test_acc : 0.9116333333333333, 0.09339966493348281, 0.7978\n",
      "train_acc, train_loss, test_acc : 0.9205166666666666, 0.08460599350432556, 0.8863\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfmElEQVR4nO3daXAc553f8e9/LtwEQBIQD4A3AfqSLIsmZcoSScmWZFsbObJEyk7Wa1dSilzlrdh5sfZuVRKnUilvqlIp72Z3S6Uoyia18ZqgJcuyVpa9Fkkd1EoiJesgJQG8T4kALwAECcz15EXPAIPBABgQRw+av08Vama6n575own++pmn+5kx5xwiIjL7hfwuQEREpoYCXUQkIBToIiIBoUAXEQkIBbqISEBE/Hrh+fPnu2XLlvn18iIis9Ibb7xx1jnXUGidb4G+bNky9u7d69fLi4jMSmZ2bLR1GnIREQkIBbqISEAo0EVEAkKBLiISEAp0EZGAUKCLiASEAl1EJCB8uw5dRGQ2Sacd8VSagWSaeDJNPJW5HXycGlqXsz5/2UAyzdql9dzWUnBu0KQo0EWkJKXSbjAIB1IpBhJ5IZpzf2BEwKZGrB/ZJu85UoW3zW6fTE/dd0d8Z9NKBbqITL902mUCMEV/orjb0cJ2eG+2cA+2YCCn0qSmMEBjkRBl4RCxiPdTFhm6H8ssr41FiYVz1uW0z31cNmL7cME2hZ6jLLPMzKbsd8ulQBcpUdlg7U+krup2oMDy0bcZHsyTETJywixcONzCIarKIsOCsWxEm/DwMMzbviw6vH1ZgW1i4RDRsE1bgJYaBbrIOFJpNxh2/fm3RYRqf/a2qN7uUBBPNlizPcnyaJjyaHjwflkkRGUsQn3l0OOyvPVj3kZDlEeG3+aGdiSsay38okCXQIsn03RfSdB9JUFPf+b2yvDbwfVXkoPt+hNDwZ1ITe6tf34glkdDlEW828pYhLlV3uOynOWTuc0G7LXSK5UhCnQpac45riRSI0K3+0rhcM6GdrbtlURqzOcvj4aorYhSWxFlTnmUhbXltC6ooSI2gR5rgdtsj1fBKjNJgS7TLp129A4kC/aMcwO4+8rINj39iXF7yDVlEeZkQrm2Isry+VXDQrq2MnM/+zi7riJCWSQ8Q3tBZPop0KUoiVQ6J2RH9pJHC+meK0l6+hO4MTI5HLJM+EYGg3dxfcVg8NbmBXE2jGsrotSURwmH1AOWWaL3Izi0E+avhqa1U/70CvRrSP+woYv8YYvkiEDO7S33xcceuiiLhAbDuLYiSmNNOasba4aFdG4vOrdtVSysYQkJpkQ/HH8FDu3wgvzMPm/5+ocV6FK8831xXuzoYmd7J68fOc+5vjjx5NhXTVSX5YRveYQlcyuHBW9+7zh3GKM8GoChC+fg0hnoOQXzVkF5rd8VyWzjHHR94AX4wefh2G5I9kM4Bktuhi/8CFbeAdd9clpeXoEeEOm0Y9/pbnZ+4IX42ycv4hzMq4rx+dXzWVhbMTyI84Ywasoj18blZqkkdB+H80fgwpHM7VE4f9i7TVz22lkIFlwPyz4PS2+BpZ+Dino/K5dS1XcODu/0euCHdkDvaW/5/Ba46duw8nZYdgvEqqa9FAX6LNZ9OcGLB7rY1d7FCx2dnL0UxwxuaKrje3e0sHlNA59cVEvoWhtjjvdlQjo3tI94oX3xBLic4aNIOdQvg7krYMVmmLscqq/z3hoffRle/5/wT38FmNerWnZLJuBvgap5Pv2C4qtkHE7uyQyjPA+n3wIclNfBik1egK+8HeqaZ7w0c2OdrZpGa9eudfqS6IlxzvHehz3sau9iV3snbxy7QNpBXWWUjS0NbG5t5NbV85lXXeZ3qdPLObhywQvoEaF9BC59NLx9eZ0X1PXLveAevL8cqhdAaIx3Jol+OLUXju6GYy/DiT2QvOKta/iYF/DZXnx147T9yuIj57y/tUM7vJ8jL0L8ElgYmtcNBfiiGyE0/UOPZvaGc67gALwCvcT19CfYfeCsF+IdnZzpGQDgU4tr2dzawMbWRj7dXBe8Kz3Sae+ta8HQPgoD3cPb1yzKCeplXnBnQ3sqh0qScTj9ptd7P7Ybjr8GiT5v3fwWL9izAT9n4dS9rsys/m4vuA8+74X4xWPe8rqlsOoObxx8+a2+nGdRoM8izjk6zlxiZ3snu9o72Xv0Asm0o6Y8wm0tDWxqaWBjawONNeV+lzp5yQG4mBnPPn94eGhfOAapgaG2oYj3nym3d53tcdcvhWiFP79DKgEfvg1HX/J68cdfhXivt27uiuEB78NbcClSOgWn3hzqhZ/c4w3NxWpg+W2wcrPXC5+30u9KFeilrm8gySuHznkh/kEnp7v7AfjYwjlsavWGUj6zpG52nrTs7xk5jp09Edl9Esj5+4tWeUE9IrSXw5wmCM+CUz6pJHz0jtd7P7rbu2StP/Nuom7pULgvu8V7rMs1/XPxxNA4+OFdmX8n84ZOVt7u9cSbPgvhqN+VDqNALzHOOQ519bGrvZNd7V28fuQ88VSa6rIIt6yax+bWRja2NrCw1qde50Q4B5c6R45jZ3vcl88Nb185PxPaK0aGdlVD8AIunYIz+zMB/zIcewWunPfWzWkaOsm67PPePgna719K4n3ev0G2F362w1teswhWZcbBV2yGyrn+1jmOYAV6vM87kkbKvWs7I+Wzoud2JZ7i1cNeL3xneycnznsn1lquq2ZTayObWhtYu3QusUgJ9sJTSeg+USC0Mz3t7BgyeJf7zWnyxrHzT0LWL4PyOT79EiUinYau94dOsh7dDZfPeutqFsLSDZle/Oe92YQK+KuXTsOZd4euCT/+KqQTEKnwDqQr7/BCvKF1Vu3nYAX6/l/A9m8NX2ZhL9gjZTk/5UO32eAfXF4OkfxlZRAusG1Rz1dW8Oz20bNeL3xnexevHj7HQDJNRTTMLavmDYZ4U33l1e3AqRa/7IVzfmhfOOKNc6eTQ23DZZlL/QqMZ9ct8fatFMc5r6eYPcl6dPfQVTpVjTkBfws0rBn7ihwZmlp/aId3bXhfl7f8uk8NjYMv+RxEZ+85qGAF+vkjcOQF74Rasj9zm3s/c5sqtDxeuF0qPvlfKBTBRcpJWJQr6Si9yTB9qTADRLFIOVVVVdTW1FBXU0U4VjH84FLoAFHwQFLogBMbWjfef/bL5wtf5nfhCPR+OLxtWe0o49krvJ6kgmV6ZC+RO/ryUMj3nPLWVcwdHvDXfVL/Dol+OP5P3jh47tT6qgZv+GTVHd614TULfC1zKgUr0KdDOp05AOQeIPIOBqMcIC729nL0zHlOdV2g60IPoXScylCCxdUhFlUbDZVGpSXGP9jk9oCv1uAQVN4BwkLe7Mjsybms6gWjj2dX1M+qt6GB5Zz3zinbez/2sveOCbzr65duGDrJuuD6GbkO2lfjTa1fefvQ1PqAHuwU6FMonkyz9+j5zGWFXRzovARA89wKbm9tZFNrIzevmEdFbIL/sdKpAgeRAu8oJvxupN977jmLRo5nx0pkuEcm5uKJnJOsu70ePUDZHC/UsidZF95QcldoXJXL573hk4M7Rk6tzwb4DE2tLwVjBXrpn00sAR92X2FXexc7P+hk98Gz9MVTxMIh1q+Yy9bPNrN5TSMr5ldN7hMDQ2EvYBWyMp66Zqh7EG540Hvcc9q7eiZ7LfyB33rLo1WwZP3QSdZFN86O8xupBJx4vSSn1pc69dALSKTSvHnsAjszU+w/+MibKLKotpxNaxrZ3NrIhpXzqCrT8VBKUO8Zr+eeHabpet9bHqnwpqpnx+AX31QaJwfHmlrf9NnMzMyZm1pf6jTkUoTOnn52dXTxQnsXLx7oorc/SSRkrF1Wz+bWRjavaWR1Y7U+t1tmn76zXg8+G/Bn9gHOO5He9Nmha+Gb183cjNvs1PrsWPiIqfW3ezM09RHGIyjQC0ilHW+duOANpbR3su9UDwDXzSljU0sjm9c0cMuq+dSUB2AMUiTX5fPelSHZk6wfvQsu7Z1YXHzT0EnW5vVTNy6dTsHp3w99Nsrg1PrqzNT620tman2pm3Sgm9ndwF8AYeAx59yf562vBf4OWII3Lv/fnHP/e6zn9CPQz10a4IUO7+NmXzzQxcXLCUIGNy2tZ1OrN5TysYU16oXLtaW/25t0kz3JevotL2xDEW+YI3uStXn9xCaGDU6t35GZWn+R2TC1vtRNKtDNLAx0AF8ETgJ7gK87597LafNnQK1z7gdm1gC0Awucc6Ne4D0TgZ5OO9451T04ueedzJc+zK+OsTHTC791VQO1lfqDEhk00AsnXvN68Edf9j5dMp30Ln9deEMm4G/1rqipqBvaLt7nbXPo+dGn1i/fpM+Rn6TJXuWyDjjonDucebKfAfcC7+W0cUCNeV3bauA8MAUXVk/cxctxXjxwll0fdPJCRxfn+rwvffh0cx3f/0ILm1sb+cSiOdfelz6IFKusBlZ9wfsBL6hPvD40Bv/6o0Nf+rHgU14v+2yHdxBIxYem1t/0Le+Swlk2tX42KybQFwMnch6fBNbntfkr4GngNFADbHXOjfgCSzN7CHgIYMmSJVdT7wjOOfaf7hnshf/+uPelD/XZL31Y08itqxuYWzULLtcSKUWxqsy0+c3e48QVOLl36Fr4t//em+Ow/uFATK2fzYoJ9EKH1vxxmruAt4DbgZXAP5rZS865nmEbOfco8Ch4Qy4Trjajpz/BywfOsvODTnZ1dNHV631u9vVNtXx38yo2rWnkhqYAfumDSCmIVnhf7rD8Vr8rkTzFBPpJIPcK/ia8nniubwN/7rwB+YNmdgRYA7w+JVXm+NXbp/netrdIpR1zsl/60NrIxpYGGmoC/tVrIiJjKCbQ9wCrzWw5cAp4EPhGXpvjwB3AS2Z2HdAKHJ7KQrOub6rl39y2gs1rGrmxeZZ+6YOIyDQYN9Cdc0kz+y7wG7zLFh93zu03s4cz6x8B/jPwt2b2Lt4QzQ+cc2eno+Cl86r4k7vXTMdTi4jMakXNXXfOPQs8m7fskZz7p4E7p7Y0ERGZCI1XiIgEhAJdRCQgFOgiIgGhQBcRCQgFuohIQCjQRUQCQoEuIhIQCnQRkYBQoIuIBIQCXUQkIBToIiIBoUAXEQkIBbqISEAo0EVEAkKBLiISEAp0EZGAUKCLiASEAl1EJCAU6CIiAaFAFxEJCAW6iEhAKNBFRAJCgS4iEhAKdBGRgFCgi4gEhAJdRCQgFOgiIgGhQBcRCQgFuohIQCjQRUQCQoEuIhIQCnQRkYBQoIuIBIQCXUQkIBToIiIBoUAXEQkIBbqISEAo0EVEAqKoQDezu82s3cwOmtkPR2mzyczeMrP9ZvbC1JYpIiLjiYzXwMzCwF8DXwROAnvM7Gnn3Hs5beqAvwHuds4dN7PGaapXRERGUUwPfR1w0Dl32DkXB34G3JvX5hvAk8654wDOuc6pLVNERMZTTKAvBk7kPD6ZWZarBag3s11m9oaZfbPQE5nZQ2a218z2dnV1XV3FIiJSUDGBbgWWubzHEeAm4CvAXcC/N7OWERs596hzbq1zbm1DQ8OEixURkdGNO4aO1yNvznncBJwu0Oasc64P6DOzF4EbgI4pqVJERMZVTA99D7DazJabWQx4EHg6r80vgVvNLGJmlcB64P2pLVVERMYybg/dOZc0s+8CvwHCwOPOuf1m9nBm/SPOuffN7DngHSANPOac2zedhYuIyHDmXP5w+MxYu3at27t3ry+vLSIyW5nZG865tYXWaaaoiEhAKNBFRAJCgS4iEhAKdBGRgFCgi4gEhAJdRCQgFOgiIgGhQBcRCQgFuohIQCjQRUQCQoEuIhIQCnQRkYBQoIuIBIQCXUQkIBToIiIBoUAXEQkIBbqISEAo0EVEAkKBLiISEAp0EZGAUKCLiASEAl1EJCAU6CIiAaFAFxEJCAW6iEhAKNBFRAJCgS4iEhAKdBGRgFCgi4gEhAJdRCQgFOgiIgGhQBcRCQgFuohIQCjQRUQCQoEuIhIQCnQRkYBQoIuIBIQCXUQkIIoKdDO728zazeygmf1wjHafNbOUmd0/dSWKiEgxxg10MwsDfw18Cfg48HUz+/go7f4r8JupLlJERMZXTA99HXDQOXfYORcHfgbcW6DdHwNPAJ1TWJ+IiBSpmEBfDJzIeXwys2yQmS0G/jnwyFhPZGYPmdleM9vb1dU10VpFRGQMxQS6FVjm8h7/BPiBcy411hM55x51zq11zq1taGgoskQRESlGpIg2J4HmnMdNwOm8NmuBn5kZwHzgy2aWdM49NRVFiojI+IoJ9D3AajNbDpwCHgS+kdvAObc8e9/M/hZ4RmEuIjKzxg1051zSzL6Ld/VKGHjcObffzB7OrB9z3FxERGZGMT10nHPPAs/mLSsY5M65b02+LBERmSjNFBURCQgFuohIQCjQRUQCQoEuIhIQCnQRkYBQoIuIBIQCXUQkIBToIiIBoUAXEQkIBbqISEAo0EVEAkKBLiISEAp0EZGAUKCLiASEAl1EJCAU6CIiAaFAFxEJCAW6iEhAKNBFRAJCgS4iEhAKdBGRgFCgi4gEhAJdRCQgFOgiIgGhQBcRCQgFuohIQMy6QHfO4ZzzuwwRkZIz6wJ939l93Pf0ffz0/Z/SG+/1uxwRkZIx6wI9no5THi7nx6//mDu238GPXvkR7517z++yRER8Z34NX6xdu9bt3bv3qrfff24/be1tPHv4WfpT/Xxq/qfY0rqFu5bdRUWkYgorFREpHWb2hnNubcF1szXQs3riPfzq0K9oa2/jcPdhamI13LvyXra0bmF57fIpqFREpHQEOtCznHPsPbOXtvY2fnf8dyTTSdYvWM+W1i1sXrKZaCg6Za8lIuKXayLQc529cpanDj7F9vbtnO47TUNFA/etvo/7W+5nQdWCaXlNEZGZcM0FelYqnWL36d1sa9/GSydfwsy4rek2trZuZcOiDYRs1p0TFpFr3DUb6LlOXTrFEx1P8MSBJzjff56m6iYeaH2Ar676KnPL585YHSIik6FAz5FIJXj++PNsa9/G3jN7iYaifHHpF9naupUbG2/EzGa8JhGRYinQR3Ho4iG2d2zn6YNP05voZVXdKra0buEPVvwB1bFqX2sTESlEgT6Oy4nLPHf0Oba1b+O9c+9REangKyu+wtbWrayZu8bv8kREBk060M3sbuAvgDDwmHPuz/PW/wvgB5mHl4DvOOfeHus5SynQc+07u49t7dt47shz9Kf6ub7hera2buXOpXdSHin3uzwRucZNKtDNLAx0AF8ETgJ7gK87597LabMBeN85d8HMvgT8yDm3fqznLdVAz+oe6PYmLHW0caT7CLVltYMTlpbOWep3eSJyjZpsoH8OL6Dvyjz+UwDn3I9HaV8P7HPOLR7reUs90LOcc+z5aA/b2rex4/gOki7JzQtvZmvrVjY2b9SEJRGZUWMFeqSI7RcDJ3IenwTG6n3/K+DXxZdX2syMdQvXsW7hOs5eOcuTB57k5x0/5/u7vk9jRSP3tdzH11Z/TROWRMR3xfTQHwDucs7968zjPwTWOef+uEDbzcDfAJ93zp0rsP4h4CGAJUuW3HTs2LHJ/wY+SKVTvHTqJba1b2P3qd2ELMTGpo1sbd3KzYtu1oQlEZk2k+2hnwSacx43AacLvMj1wGPAlwqFOYBz7lHgUfCGXIp47ZIUDoXZ1LyJTc2bONl7kp93/JxfHPwFO07soLmmmQdavAlL9eX1fpcqIteQYnroEbyToncAp/BOin7DObc/p80SYAfwTefcK8W88GwZQy9WPBXnd8d+x7b2bbzZ+SaxUIw7l93J1tat3NBwgyYsiciUmIrLFr8M/ATvssXHnXP/xcweBnDOPWJmjwFfA7JjKMnRXjAraIGe68CFA2zv2M6vDv2KS4lLtNS3sKVlC/esvIeqaJXf5YnILKaJRT65nLjMs0eepa29jffPv09lpJJ7VtzDltYttM5t9bs8EZmFFOg+c87x7tl3aWtv47mjzzGQGuCGhhu8CUvL7qQsXOZ3iSIySyjQS0j3QDe/PPhLtnds52jPUerK6vjqqq/yQMsDLJmzxO/yRKTEKdBLkHOO1z56jbb2NnYc30HKpdiwaANbWrawsXkjkVAxFyCJyLVGgV7iOi93Dk5YOnP5DI2Vjdy/+n7uW30f11Vd53d5IlJCFOizRDKd5MWTL9LW0cYrp14hZCE2N2/mgdYHuHmhJiyJiAJ9VjrRe4LtHdt56sBTXBi4wNI5S3mg5QHuXXkvdeV1fpcnIj5RoM9i8VSc3x77LW3tbfy+8/fEQjHuXn43W1q3cP386zVhSeQao0APiI4LHbS1t/HM4WfoS/TRWt/KltYt3LPiHiqjlX6XJyIzQIEeMH2JPv7h8D/Q1t5G+4V2qqJV3LPiHra2bmV1/Wq/yxORaaRADyjnHO+cfcebsHTkOeLpODc23siW1i3cufROYuGY3yWKyBRToF8DLvZf5JeHfklbexvHe49TX1bPV1d7E5aaa5rHfwIRmRUU6NeQtEvz6oevsr19OztP7CTlUqxfuJ5lc5YxJzaHObE51JbVevfLhj+uiFToJKtIiZvs56HLLBKyEBsWbWDDog2c6TvDkwee5LfHfkvH+Q564j2kXGrUbaOh6LCAzw3+2ljtiANA7nJ9FZ+I/9RDv4Y45+hL9NET76F7oHvM256BnmGPLyUujfnclZHKEYFf6MAw7HFsDjWxGk2YEpkA9dAF8L4ftTpWTXWsmkXViya0bTKdpDfeW/TB4FjPMXoGeuiOdzOQGhi9JoyaWM2oB4H8g4GGiERGp0CXokRCEerL66/qa/X6k/2DQd8d7x79Nt5D70Avpy6dGnyHcDVDRKO+I9AQkQScAl2mXXmknPJIOY2VjRPaLjtENOZBIBP8PQM9dF7u5MCFAxMaIip4biDT+4+FY5RHyr3b8NBtWaSMslCZdxse/qN3DOInBbqUrNwhosXViye0bXaIaNRhoQJDRN0D3XQPdBNPx6+65rJw2YgDQPbAkB/+uQeE3HYFDyD522a2Lw+XEw1FdSARQIEuATWZIaKB1AD9yX4GUgMMJAfoT/UTT8XpTw0tG0hnblN5PwWW9Se97a8kr9A90D30fNnXSA2QSCeu+nc1bOSBZJwDyFjLCm5b4AAUCUV0ICkxCnSRPNnQmkmpdIp4Oj7sgJAb/NkDyuCBpdDBJO/Akm3fl+zjwsCFYc+TbZdMJ6+6ZsMG31HEQjHvNud+NBQdtiwajo5ol79scJsxlg3bJmdZxHSAUaCLlIBwKExFqIKKSMWMvm4qnRr1HUXuAST/HUX+gSORThBPxb2fdJxEKkE8PfSuZHB92muTXR9PxXFMzaXThg0/UEzg4DKZA0nusvzXnelLchXoItewcChMZajSt0/rdM6RdEkv4HMCf/CgkHsQyDtoFFqWe6AYbVlPsmfUg0s8HZ/Uu5Z8EYsUPLjc33I/f/SJP5qy1xl8vSl/RhGRIpkZUYsSDUVL5iOg0y497EBRzIGkmINL7ruUeRXzpqV2BbqISI6QhXw5jzIVNOdaRCQgFOgiIgGhQBcRCQgFuohIQCjQRUQCQoEuIhIQCnQRkYBQoIuIBIRvX0FnZl3AsavcfD5wdgrLmSqlWheUbm2qa2JU18QEsa6lzrmGQit8C/TJMLO9o32nnp9KtS4o3dpU18Sorom51urSkIuISEAo0EVEAmK2BvqjfhcwilKtC0q3NtU1MaprYq6pumblGLqIiIw0W3voIiKSR4EuIhIQJR3oZna3mbWb2UEz+2GB9WZmf5lZ/46ZfaZE6tpkZt1m9lbm5z/MUF2Pm1mnme0bZb1f+2u8umZ8f5lZs5ntNLP3zWy/mf3bAm1mfH8VWZcf+6vczF43s7czdf2nAm382F/F1OXL/8fMa4fN7Pdm9kyBdVO/v5xzJfkDhIFDwAogBrwNfDyvzZeBXwMG3Ay8ViJ1bQKe8WGf3QZ8Btg3yvoZ319F1jXj+wtYCHwmc78G6CiRv69i6vJjfxlQnbkfBV4Dbi6B/VVMXb78f8y89r8Dflro9adjf5VyD30dcNA5d9g5Fwd+Btyb1+Ze4P86z6tAnZktLIG6fOGcexE4P0YTP/ZXMXXNOOfch865NzP3e4H3gcV5zWZ8fxVZ14zL7INLmYfRzE/+FRV+7K9i6vKFmTUBXwEeG6XJlO+vUg70xcCJnMcnGfmHXUwbP+oC+FzmbeCvzewT01xTsfzYX8XybX+Z2TLgRrzeXS5f99cYdYEP+yszfPAW0An8o3OuJPZXEXWBP39fPwH+BEiPsn7K91cpB7oVWJZ/5C2mzVQr5jXfxPu8hRuA/wE8Nc01FcuP/VUM3/aXmVUDTwDfc8715K8usMmM7K9x6vJlfznnUs65TwNNwDoz+2ReE1/2VxF1zfj+MrN7gE7n3BtjNSuwbFL7q5QD/STQnPO4CTh9FW1mvC7nXE/2baBz7lkgambzp7muYvixv8bl1/4ysyheaP4/59yTBZr4sr/Gq8vvvy/n3EVgF3B33ipf/75Gq8un/XUL8M/M7CjesOztZvZ3eW2mfH+VcqDvAVab2XIziwEPAk/ntXka+GbmbPHNQLdz7kO/6zKzBWZmmfvr8PbzuWmuqxh+7K9x+bG/Mq/3v4D3nXP/fZRmM76/iqnLp/3VYGZ1mfsVwBeAD/Ka+bG/xq3Lj/3lnPtT51yTc24ZXkbscM79y7xmU76/IpPZeDo555Jm9l3gN3hXljzunNtvZg9n1j8CPIt3pvggcBn4donUdT/wHTNLAleAB13mtPZ0MrO/xzujP9/MTgL/Ee8kkW/7q8i6/NhftwB/CLybGX8F+DNgSU5dfuyvYuryY38tBP6PmYXxArHNOfeM3/8fi6zLl/+PhUz3/tLUfxGRgCjlIRcREZkABbqISEAo0EVEAkKBLiISEAp0EZGAUKCLiASEAl1EJCD+P3kfF2pNYBx6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_net(net, train_iter, test_iter, n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           3,200\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
      "              ReLU-6           [-1, 64, 56, 56]               0\n",
      "            Conv2d-7           [-1, 32, 56, 56]          18,464\n",
      "       BatchNorm2d-8           [-1, 96, 56, 56]             192\n",
      "              ReLU-9           [-1, 96, 56, 56]               0\n",
      "           Conv2d-10           [-1, 32, 56, 56]          27,680\n",
      "      BatchNorm2d-11          [-1, 128, 56, 56]             256\n",
      "             ReLU-12          [-1, 128, 56, 56]               0\n",
      "           Conv2d-13           [-1, 32, 56, 56]          36,896\n",
      "      BatchNorm2d-14          [-1, 160, 56, 56]             320\n",
      "             ReLU-15          [-1, 160, 56, 56]               0\n",
      "           Conv2d-16           [-1, 32, 56, 56]          46,112\n",
      "      dense_block-17          [-1, 192, 56, 56]               0\n",
      "      BatchNorm2d-18          [-1, 192, 56, 56]             384\n",
      "             ReLU-19          [-1, 192, 56, 56]               0\n",
      "           Conv2d-20           [-1, 96, 56, 56]          18,528\n",
      "        AvgPool2d-21           [-1, 96, 28, 28]               0\n",
      "      BatchNorm2d-22           [-1, 96, 28, 28]             192\n",
      "             ReLU-23           [-1, 96, 28, 28]               0\n",
      "           Conv2d-24           [-1, 32, 28, 28]          27,680\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "           Conv2d-27           [-1, 32, 28, 28]          36,896\n",
      "      BatchNorm2d-28          [-1, 160, 28, 28]             320\n",
      "             ReLU-29          [-1, 160, 28, 28]               0\n",
      "           Conv2d-30           [-1, 32, 28, 28]          46,112\n",
      "      BatchNorm2d-31          [-1, 192, 28, 28]             384\n",
      "             ReLU-32          [-1, 192, 28, 28]               0\n",
      "           Conv2d-33           [-1, 32, 28, 28]          55,328\n",
      "      dense_block-34          [-1, 224, 28, 28]               0\n",
      "      BatchNorm2d-35          [-1, 224, 28, 28]             448\n",
      "             ReLU-36          [-1, 224, 28, 28]               0\n",
      "           Conv2d-37          [-1, 112, 28, 28]          25,200\n",
      "        AvgPool2d-38          [-1, 112, 14, 14]               0\n",
      "      BatchNorm2d-39          [-1, 112, 14, 14]             224\n",
      "             ReLU-40          [-1, 112, 14, 14]               0\n",
      "           Conv2d-41           [-1, 32, 14, 14]          32,288\n",
      "      BatchNorm2d-42          [-1, 144, 14, 14]             288\n",
      "             ReLU-43          [-1, 144, 14, 14]               0\n",
      "           Conv2d-44           [-1, 32, 14, 14]          41,504\n",
      "      BatchNorm2d-45          [-1, 176, 14, 14]             352\n",
      "             ReLU-46          [-1, 176, 14, 14]               0\n",
      "           Conv2d-47           [-1, 32, 14, 14]          50,720\n",
      "      BatchNorm2d-48          [-1, 208, 14, 14]             416\n",
      "             ReLU-49          [-1, 208, 14, 14]               0\n",
      "           Conv2d-50           [-1, 32, 14, 14]          59,936\n",
      "      dense_block-51          [-1, 240, 14, 14]               0\n",
      "      BatchNorm2d-52          [-1, 240, 14, 14]             480\n",
      "             ReLU-53          [-1, 240, 14, 14]               0\n",
      "           Conv2d-54          [-1, 120, 14, 14]          28,920\n",
      "        AvgPool2d-55            [-1, 120, 7, 7]               0\n",
      "      BatchNorm2d-56            [-1, 120, 7, 7]             240\n",
      "             ReLU-57            [-1, 120, 7, 7]               0\n",
      "           Conv2d-58             [-1, 32, 7, 7]          34,592\n",
      "      BatchNorm2d-59            [-1, 152, 7, 7]             304\n",
      "             ReLU-60            [-1, 152, 7, 7]               0\n",
      "           Conv2d-61             [-1, 32, 7, 7]          43,808\n",
      "      BatchNorm2d-62            [-1, 184, 7, 7]             368\n",
      "             ReLU-63            [-1, 184, 7, 7]               0\n",
      "           Conv2d-64             [-1, 32, 7, 7]          53,024\n",
      "      BatchNorm2d-65            [-1, 216, 7, 7]             432\n",
      "             ReLU-66            [-1, 216, 7, 7]               0\n",
      "           Conv2d-67             [-1, 32, 7, 7]          62,240\n",
      "      dense_block-68            [-1, 248, 7, 7]               0\n",
      "      BatchNorm2d-69            [-1, 248, 7, 7]             496\n",
      "             ReLU-70            [-1, 248, 7, 7]               0\n",
      "AdaptiveMaxPool2d-71            [-1, 248, 1, 1]               0\n",
      "          Flatten-72                  [-1, 248]               0\n",
      "           Linear-73                   [-1, 10]           2,490\n",
      "================================================================\n",
      "Total params: 758,226\n",
      "Trainable params: 758,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 77.81\n",
      "Params size (MB): 2.89\n",
      "Estimated Total Size (MB): 80.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, (1,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 \n",
    "## Designing MLP  with Densenet\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
