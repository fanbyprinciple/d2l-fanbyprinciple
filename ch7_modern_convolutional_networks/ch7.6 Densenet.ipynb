{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_channels, num_channels):\n",
    "    return nn.Sequential(nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "                         nn.Conv2d(input_channels, num_channels,kernel_size=3, padding=1))\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_block(nn.Module):\n",
    "    def __init__(self, num_convs, input_channels, num_channels):\n",
    "        super(dense_block, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(num_convs):\n",
    "            layers.append(conv_block(input_channels + num_channels* i, num_channels))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.net:\n",
    "            y = layer(X)\n",
    "            X = torch.cat((X,y), dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = dense_block(2, 3, 10)\n",
    "\n",
    "X = torch.randn(4,3,8,8)\n",
    "\n",
    "y = blk(X)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(23,10)\n",
    "blk(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Densenet\n",
    "\n",
    "b1 = nn.Sequential(nn.Conv2d(1,64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 64\n",
    "growth_rate = 32\n",
    "\n",
    "num_of_convs_in_dense_blocks = [4,4,4,4]\n",
    "\n",
    "blks = []\n",
    "\n",
    "for i, num_convs in enumerate(num_of_convs_in_dense_blocks):\n",
    "    blks.append(dense_block(num_convs, num_channels, growth_rate))\n",
    "    num_channels += num_convs * growth_rate\n",
    "    if i != len(num_of_convs_in_dense_blocks) - 1:\n",
    "        blks.append(transition_block(num_channels, num_channels//2))\n",
    "        num_channels = num_channels // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net= nn.Sequential(b1, *blks, nn.BatchNorm2d(num_channels),nn.ReLU(),\n",
    "                   nn.AdaptiveMaxPool2d((1,1)), nn.Flatten(), nn.Linear(num_channels, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "#d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\installs\\anconda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 48, 48]           3,200\n",
      "       BatchNorm2d-2           [-1, 64, 48, 48]             128\n",
      "              ReLU-3           [-1, 64, 48, 48]               0\n",
      "         MaxPool2d-4           [-1, 64, 24, 24]               0\n",
      "       BatchNorm2d-5           [-1, 64, 24, 24]             128\n",
      "              ReLU-6           [-1, 64, 24, 24]               0\n",
      "            Conv2d-7           [-1, 32, 24, 24]          18,464\n",
      "       BatchNorm2d-8           [-1, 96, 24, 24]             192\n",
      "              ReLU-9           [-1, 96, 24, 24]               0\n",
      "           Conv2d-10           [-1, 32, 24, 24]          27,680\n",
      "      BatchNorm2d-11          [-1, 128, 24, 24]             256\n",
      "             ReLU-12          [-1, 128, 24, 24]               0\n",
      "           Conv2d-13           [-1, 32, 24, 24]          36,896\n",
      "      BatchNorm2d-14          [-1, 160, 24, 24]             320\n",
      "             ReLU-15          [-1, 160, 24, 24]               0\n",
      "           Conv2d-16           [-1, 32, 24, 24]          46,112\n",
      "      dense_block-17          [-1, 192, 24, 24]               0\n",
      "      BatchNorm2d-18          [-1, 192, 24, 24]             384\n",
      "             ReLU-19          [-1, 192, 24, 24]               0\n",
      "           Conv2d-20           [-1, 96, 24, 24]          18,528\n",
      "        AvgPool2d-21           [-1, 96, 12, 12]               0\n",
      "      BatchNorm2d-22           [-1, 96, 12, 12]             192\n",
      "             ReLU-23           [-1, 96, 12, 12]               0\n",
      "           Conv2d-24           [-1, 32, 12, 12]          27,680\n",
      "      BatchNorm2d-25          [-1, 128, 12, 12]             256\n",
      "             ReLU-26          [-1, 128, 12, 12]               0\n",
      "           Conv2d-27           [-1, 32, 12, 12]          36,896\n",
      "      BatchNorm2d-28          [-1, 160, 12, 12]             320\n",
      "             ReLU-29          [-1, 160, 12, 12]               0\n",
      "           Conv2d-30           [-1, 32, 12, 12]          46,112\n",
      "      BatchNorm2d-31          [-1, 192, 12, 12]             384\n",
      "             ReLU-32          [-1, 192, 12, 12]               0\n",
      "           Conv2d-33           [-1, 32, 12, 12]          55,328\n",
      "      dense_block-34          [-1, 224, 12, 12]               0\n",
      "      BatchNorm2d-35          [-1, 224, 12, 12]             448\n",
      "             ReLU-36          [-1, 224, 12, 12]               0\n",
      "           Conv2d-37          [-1, 112, 12, 12]          25,200\n",
      "        AvgPool2d-38            [-1, 112, 6, 6]               0\n",
      "      BatchNorm2d-39            [-1, 112, 6, 6]             224\n",
      "             ReLU-40            [-1, 112, 6, 6]               0\n",
      "           Conv2d-41             [-1, 32, 6, 6]          32,288\n",
      "      BatchNorm2d-42            [-1, 144, 6, 6]             288\n",
      "             ReLU-43            [-1, 144, 6, 6]               0\n",
      "           Conv2d-44             [-1, 32, 6, 6]          41,504\n",
      "      BatchNorm2d-45            [-1, 176, 6, 6]             352\n",
      "             ReLU-46            [-1, 176, 6, 6]               0\n",
      "           Conv2d-47             [-1, 32, 6, 6]          50,720\n",
      "      BatchNorm2d-48            [-1, 208, 6, 6]             416\n",
      "             ReLU-49            [-1, 208, 6, 6]               0\n",
      "           Conv2d-50             [-1, 32, 6, 6]          59,936\n",
      "      dense_block-51            [-1, 240, 6, 6]               0\n",
      "      BatchNorm2d-52            [-1, 240, 6, 6]             480\n",
      "             ReLU-53            [-1, 240, 6, 6]               0\n",
      "           Conv2d-54            [-1, 120, 6, 6]          28,920\n",
      "        AvgPool2d-55            [-1, 120, 3, 3]               0\n",
      "      BatchNorm2d-56            [-1, 120, 3, 3]             240\n",
      "             ReLU-57            [-1, 120, 3, 3]               0\n",
      "           Conv2d-58             [-1, 32, 3, 3]          34,592\n",
      "      BatchNorm2d-59            [-1, 152, 3, 3]             304\n",
      "             ReLU-60            [-1, 152, 3, 3]               0\n",
      "           Conv2d-61             [-1, 32, 3, 3]          43,808\n",
      "      BatchNorm2d-62            [-1, 184, 3, 3]             368\n",
      "             ReLU-63            [-1, 184, 3, 3]               0\n",
      "           Conv2d-64             [-1, 32, 3, 3]          53,024\n",
      "      BatchNorm2d-65            [-1, 216, 3, 3]             432\n",
      "             ReLU-66            [-1, 216, 3, 3]               0\n",
      "           Conv2d-67             [-1, 32, 3, 3]          62,240\n",
      "      dense_block-68            [-1, 248, 3, 3]               0\n",
      "      BatchNorm2d-69            [-1, 248, 3, 3]             496\n",
      "             ReLU-70            [-1, 248, 3, 3]               0\n",
      "AdaptiveMaxPool2d-71            [-1, 248, 1, 1]               0\n",
      "          Flatten-72                  [-1, 248]               0\n",
      "           Linear-73                   [-1, 10]           2,490\n",
      "================================================================\n",
      "Total params: 758,226\n",
      "Trainable params: 758,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 14.29\n",
      "Params size (MB): 2.89\n",
      "Estimated Total Size (MB): 17.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(net.to(torch.device('cuda')), (1,96,96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Why do we use average pooling rather than maximum pooling in the transition layer?\n",
    "- so that theother cells may not be eliminated and have some value addition in the model\n",
    "\n",
    "2. One of the advantages mentioned in the DenseNet paper is that its model parameters are\n",
    "smaller than those of ResNet. Why is this the case?\n",
    "- because of less number of linear layers, more memory is utilised instead disk space( I am ssuming because of the follow up question given below)\n",
    "\n",
    "```\n",
    "Total params: 758,226\n",
    "Trainable params: 758,226\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.04\n",
    "Forward/backward pass size (MB): 14.29\n",
    "Params size (MB): 2.89\n",
    "Estimated Total Size (MB): 17.22\n",
    "```\n",
    "\n",
    "This is the disk space, while the memory used in 3.1 GB.\n",
    "\n",
    "3. One problem for which DenseNet has been criticized is its high memory consumption.\n",
    "    1. Is this really the case? Try to change the input shape to 224 Ã— 224 to see the actual GPU\n",
    "    memory consumption.\n",
    "    - all right weiil have to try\n",
    "    \n",
    "    2. Can you think of an alternative means of reducing the memory consumption? How\n",
    "    would you need to change the framework?\n",
    "    - no Idea\n",
    "4. Implement the various DenseNet versions presented in Table 1 of the DenseNet paper\n",
    "(Huang et al., 2017).\n",
    "\n",
    "- https://arxiv.org/pdf/1608.06993.pdf\n",
    "\n",
    "5. Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price\n",
    "prediction task in Section 4.10.\n",
    "\n",
    "-  This is a task! But how? I am low on motivation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for densenet\n",
    "\n",
    "def train_net(net, train_dataloader, test_dataloader, n_epochs=10, lr=0.1, batch_size=256,device=torch.device('cuda')):\n",
    "    def init_weights(m):\n",
    "        if type(m)==nn.Linear or type(m)==nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "    net.apply(init_weights)\n",
    "    print('training on: ', device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr )\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def accuracy(y_hat, y):\n",
    "        return (torch.argmax(y_hat, dim=1) == y).sum().float().mean()\n",
    "    \n",
    "    train_acc_arr = []\n",
    "    train_loss_arr = []\n",
    "    test_acc_arr = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        net.train()\n",
    "        train_acc = 0\n",
    "        train_loss = 0\n",
    "        for X, y in train_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "        \n",
    "            y_hat = net(X)\n",
    "            train_acc += accuracy(y_hat, y).item()\n",
    "            #print(train_acc * len(y))\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_criterion(y_hat, y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            #print(train_loss*10, train_acc/10)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        net.eval()\n",
    "        test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for X, y in test_dataloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "            \n",
    "                y_hat = net(X)\n",
    "                test_acc += accuracy(y_hat, y).item()\n",
    "        \n",
    "        test_acc_arr.append(test_acc/len(y))\n",
    "                \n",
    "        train_acc_arr.append(train_acc/len(y))\n",
    "        train_loss_arr.append(train_loss/len(y))\n",
    "        \n",
    "        print(f'train_acc, train_loss, test_acc : {}, {train_acc}, {train_loss}, {train }')\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.plot(train_acc_arr, range(n_epochs))\n",
    "    plt.plot(test_acc_arr, range(n_epochs))\n",
    "    plt.plot(train_loss_arr, range(n_epochs))\n",
    "    plt.show()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on:  cuda\n",
      "train_acc, train_loss, test_acc : 7275.0, 46244.0, 169.77538293600082\n",
      "train_acc, train_loss, test_acc : 7978.0, 52591.0, 79.3158895522356\n",
      "train_acc, train_loss, test_acc : 7976.0, 53687.0, 65.82091580331326\n",
      "train_acc, train_loss, test_acc : 8044.0, 54463.0, 58.812214106321335\n",
      "train_acc, train_loss, test_acc : 8838.0, 55063.0, 51.77932734787464\n",
      "train_acc, train_loss, test_acc : 8896.0, 55516.0, 47.68976314365864\n",
      "train_acc, train_loss, test_acc : 9050.0, 55853.0, 43.618239901959896\n",
      "train_acc, train_loss, test_acc : 7752.0, 56192.0, 39.982881247997284\n",
      "train_acc, train_loss, test_acc : 8348.0, 56430.0, 37.777349799871445\n",
      "train_acc, train_loss, test_acc : 9111.0, 56824.0, 33.4674229323864\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_net(net, train_iter, test_iter, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
