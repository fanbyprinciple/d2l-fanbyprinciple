{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c2490667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, optim\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5d642047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualLR:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def compute_weight(self, module):\n",
    "        weight = getattr(module, self.name + '_orig')\n",
    "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
    "\n",
    "        return weight * sqrt(2 / fan_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(module, name):\n",
    "        fn = EqualLR(name)\n",
    "\n",
    "        weight = getattr(module, name)\n",
    "        del module._parameters[name]\n",
    "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
    "        module.register_forward_pre_hook(fn)\n",
    "\n",
    "        return fn\n",
    "\n",
    "    def __call__(self, module, input):\n",
    "        weight = self.compute_weight(module)\n",
    "        setattr(module, self.name, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e78a7ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_lr(module, name='weight'):\n",
    "    EqualLR.apply(module, name)\n",
    "\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e231ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True)\n",
    "                                  + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "169fca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualConv2d(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        conv = nn.Conv2d(*args, **kwargs)\n",
    "        conv.weight.data.normal_()\n",
    "        conv.bias.data.zero_()\n",
    "        self.conv = equal_lr(conv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2afb363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualConvTranspose2d(nn.Module):\n",
    "    ### additional module for OOGAN usage\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        conv = nn.ConvTranspose2d(*args, **kwargs)\n",
    "        conv.weight.data.normal_()\n",
    "        conv.bias.data.zero_()\n",
    "        self.conv = equal_lr(conv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3703a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        linear = nn.Linear(in_dim, out_dim)\n",
    "        linear.weight.data.normal_()\n",
    "        linear.bias.data.zero_()\n",
    "\n",
    "        self.linear = equal_lr(linear)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7675add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, padding, kernel_size2=None, padding2=None, pixel_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        pad1 = padding\n",
    "        pad2 = padding\n",
    "        if padding2 is not None:\n",
    "            pad2 = padding2\n",
    "\n",
    "        kernel1 = kernel_size\n",
    "        kernel2 = kernel_size\n",
    "        if kernel_size2 is not None:\n",
    "            kernel2 = kernel_size2\n",
    "\n",
    "        convs = [EqualConv2d(in_channel, out_channel, kernel1, padding=pad1)]\n",
    "        if pixel_norm:\n",
    "            convs.append(PixelNorm())\n",
    "        convs.append(nn.LeakyReLU(0.1))\n",
    "        convs.append(EqualConv2d(out_channel, out_channel, kernel2, padding=pad2))\n",
    "        if pixel_norm:\n",
    "            convs.append(PixelNorm())\n",
    "        convs.append(nn.LeakyReLU(0.1))\n",
    "\n",
    "        self.conv = nn.Sequential(*convs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv(input)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3dba7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale(feat):\n",
    "    return F.interpolate(feat, scale_factor=2, mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7ce97f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_code_dim=128, in_channel=128, pixel_norm=True, tanh=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_code_dim\n",
    "        self.tanh = tanh\n",
    "        self.input_layer = nn.Sequential(\n",
    "            EqualConvTranspose2d(input_code_dim, in_channel, 4, 1, 0),\n",
    "            PixelNorm(),\n",
    "            nn.LeakyReLU(0.1))\n",
    "\n",
    "        self.progression_4 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\n",
    "        self.progression_8 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\n",
    "        self.progression_16 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\n",
    "        self.progression_32 = ConvBlock(in_channel, in_channel, 3, 1, pixel_norm=pixel_norm)\n",
    "        self.progression_64 = ConvBlock(in_channel, in_channel//2, 3, 1, pixel_norm=pixel_norm)\n",
    "        self.progression_128 = ConvBlock(in_channel//2, in_channel//4, 3, 1, pixel_norm=pixel_norm)\n",
    "        self.progression_256 = ConvBlock(in_channel//4, in_channel//4, 3, 1, pixel_norm=pixel_norm)\n",
    "\n",
    "        self.to_rgb_8 = EqualConv2d(in_channel, 3, 1)\n",
    "        self.to_rgb_16 = EqualConv2d(in_channel, 3, 1)\n",
    "        self.to_rgb_32 = EqualConv2d(in_channel, 3, 1)\n",
    "        self.to_rgb_64 = EqualConv2d(in_channel//2, 3, 1)\n",
    "        self.to_rgb_128 = EqualConv2d(in_channel//4, 3, 1)\n",
    "        self.to_rgb_256 = EqualConv2d(in_channel//4, 3, 1)\n",
    "        \n",
    "        self.max_step = 6\n",
    "\n",
    "    def progress(self, feat, module):\n",
    "        out = F.interpolate(feat, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        out = module(out)\n",
    "        return out\n",
    "\n",
    "    def output(self, feat1, feat2, module1, module2, alpha):\n",
    "        if 0 <= alpha < 1:\n",
    "            skip_rgb = upscale(module1(feat1))\n",
    "            out = (1-alpha)*skip_rgb + alpha*module2(feat2)\n",
    "        else:\n",
    "            out = module2(feat2)\n",
    "        if self.tanh:\n",
    "            return torch.tanh(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, input, step=0, alpha=-1):\n",
    "        if step > self.max_step:\n",
    "            step = self.max_step\n",
    "\n",
    "        out_4 = self.input_layer(input.view(-1, self.input_dim, 1, 1))\n",
    "        out_4 = self.progression_4(out_4)\n",
    "        out_8 = self.progress(out_4, self.progression_8)\n",
    "        if step==1:\n",
    "            if self.tanh:\n",
    "                return torch.tanh(self.to_rgb_8(out_8))\n",
    "            return self.to_rgb_8(out_8)\n",
    "        \n",
    "        out_16 = self.progress(out_8, self.progression_16)\n",
    "        if step==2:\n",
    "            return self.output( out_8, out_16, self.to_rgb_8, self.to_rgb_16, alpha )\n",
    "        \n",
    "        out_32 = self.progress(out_16, self.progression_32)\n",
    "        if step==3:\n",
    "            return self.output( out_16, out_32, self.to_rgb_16, self.to_rgb_32, alpha )\n",
    "\n",
    "        out_64 = self.progress(out_32, self.progression_64)\n",
    "        if step==4:\n",
    "            return self.output( out_32, out_64, self.to_rgb_32, self.to_rgb_64, alpha )\n",
    "        \n",
    "        out_128 = self.progress(out_64, self.progression_128)\n",
    "        if step==5:\n",
    "            return self.output( out_64, out_128, self.to_rgb_64, self.to_rgb_128, alpha )\n",
    "\n",
    "        out_256 = self.progress(out_128, self.progression_256)\n",
    "        if step==6:\n",
    "            return self.output( out_128, out_256, self.to_rgb_128, self.to_rgb_256, alpha )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "bb2c18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, feat_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.progression = nn.ModuleList([ConvBlock(feat_dim//4, feat_dim//4, 3, 1),\n",
    "                                          ConvBlock(feat_dim//4, feat_dim//2, 3, 1),\n",
    "                                          ConvBlock(feat_dim//2, feat_dim, 3, 1),\n",
    "                                          ConvBlock(feat_dim, feat_dim, 3, 1),\n",
    "                                          ConvBlock(feat_dim, feat_dim, 3, 1),\n",
    "                                          ConvBlock(feat_dim, feat_dim, 3, 1),\n",
    "                                          ConvBlock(feat_dim+1, feat_dim, 3, 1, 4, 0)])\n",
    "\n",
    "        self.from_rgb = nn.ModuleList([EqualConv2d(3, feat_dim//4, 1),\n",
    "                                       EqualConv2d(3, feat_dim//4, 1),\n",
    "                                       EqualConv2d(3, feat_dim//2, 1),\n",
    "                                       EqualConv2d(3, feat_dim, 1),\n",
    "                                       EqualConv2d(3, feat_dim, 1),\n",
    "                                       EqualConv2d(3, feat_dim, 1),\n",
    "                                       EqualConv2d(3, feat_dim, 1)])\n",
    "\n",
    "        self.n_layer = len(self.progression)\n",
    "\n",
    "        self.linear = EqualLinear(feat_dim, 1)\n",
    "\n",
    "    def forward(self, input, step=0, alpha=-1):\n",
    "        for i in range(step, -1, -1):\n",
    "            index = self.n_layer - i - 1\n",
    "\n",
    "            if i == step:\n",
    "                out = self.from_rgb[index](input)\n",
    "\n",
    "            if i == 0:\n",
    "                out_std = torch.sqrt(out.var(0, unbiased=False) + 1e-8)\n",
    "                mean_std = out_std.mean()\n",
    "                mean_std = mean_std.expand(out.size(0), 1, 4, 4)\n",
    "                out = torch.cat([out, mean_std], 1)\n",
    "\n",
    "            out = self.progression[index](out)\n",
    "\n",
    "            if i > 0:\n",
    "                # out = F.avg_pool2d(out, 2)\n",
    "                out = F.interpolate(out, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "\n",
    "                if i == step and 0 <= alpha < 1:\n",
    "                    # skip_rgb = F.avg_pool2d(input, 2)\n",
    "                    skip_rgb = F.interpolate(input, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "                    skip_rgb = self.from_rgb[index + 1](skip_rgb)\n",
    "                    out = (1 - alpha) * skip_rgb + alpha * out\n",
    "\n",
    "        out = out.squeeze(2).squeeze(2)\n",
    "        # print(input.size(), out.size(), step)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7b22f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "    \n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8a705fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_wrapper:\n",
    "    def __init__(self):\n",
    "        self.path = \"../data/celebb\"\n",
    "        self.trial_name = \"progressive_gans\"\n",
    "        self.z_dim = 100\n",
    "        self.channel = 512\n",
    "        self.batch_size = 4\n",
    "        self.init_step = 2\n",
    "        self.total_iter = 100000\n",
    "        self.pixel_norm=True\n",
    "        self.tanh=True\n",
    "        self.gpu_id=2\n",
    "        self.lr=0.001\n",
    "        self.n_critic=1\n",
    "        self.init_step=1\n",
    "               \n",
    "args = Args_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6f698a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args_wrapper object at 0x7f2bd36f2c70>\n"
     ]
    }
   ],
   "source": [
    "print(str(args))\n",
    "\n",
    "trial_name = args.trial_name\n",
    "\n",
    "device = torch.device(\"cuda:%d\"%(args.gpu_id))\n",
    "\n",
    "input_code_size = args.z_dim\n",
    "batch_size = args.batch_size\n",
    "n_critic = args.n_critic\n",
    "\n",
    "generator = Generator(in_channel=args.channel, input_code_dim=input_code_size, pixel_norm=args.pixel_norm, tanh=args.tanh)\n",
    "#generator = nn.DataParallel(generator, list(range(ngpu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bacfefc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(feat_dim=args.channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d89020d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_running = Generator(in_channel=args.channel, input_code_dim=input_code_size, pixel_norm=args.pixel_norm, tanh=args.tanh).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "fba3b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optimizer = optim.Adam(generator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c57f10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_PATH = \"/home/misthios/Documents/d2l-fanbyprinciple/testing/trial_progressive_gans_2022-04-20_15_8/checkpoint/150000_g.model\"\n",
    "D_PATH = \"/home/misthios/Documents/d2l-fanbyprinciple/testing/trial_progressive_gans_2022-04-20_15_8/checkpoint/150000_d.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8d5992d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_checkpoint = torch.load(G_PATH)\n",
    "D_checkpoint = torch.load(D_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b071bbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['input_layer.0.conv.bias', 'input_layer.0.conv.weight_orig', 'progression_4.conv.0.conv.bias', 'progression_4.conv.0.conv.weight_orig', 'progression_4.conv.3.conv.bias', 'progression_4.conv.3.conv.weight_orig', 'progression_8.conv.0.conv.bias', 'progression_8.conv.0.conv.weight_orig', 'progression_8.conv.3.conv.bias', 'progression_8.conv.3.conv.weight_orig', 'progression_16.conv.0.conv.bias', 'progression_16.conv.0.conv.weight_orig', 'progression_16.conv.3.conv.bias', 'progression_16.conv.3.conv.weight_orig', 'progression_32.conv.0.conv.bias', 'progression_32.conv.0.conv.weight_orig', 'progression_32.conv.3.conv.bias', 'progression_32.conv.3.conv.weight_orig', 'progression_64.conv.0.conv.bias', 'progression_64.conv.0.conv.weight_orig', 'progression_64.conv.3.conv.bias', 'progression_64.conv.3.conv.weight_orig', 'progression_128.conv.0.conv.bias', 'progression_128.conv.0.conv.weight_orig', 'progression_128.conv.3.conv.bias', 'progression_128.conv.3.conv.weight_orig', 'progression_256.conv.0.conv.bias', 'progression_256.conv.0.conv.weight_orig', 'progression_256.conv.3.conv.bias', 'progression_256.conv.3.conv.weight_orig', 'to_rgb_8.conv.bias', 'to_rgb_8.conv.weight_orig', 'to_rgb_16.conv.bias', 'to_rgb_16.conv.weight_orig', 'to_rgb_32.conv.bias', 'to_rgb_32.conv.weight_orig', 'to_rgb_64.conv.bias', 'to_rgb_64.conv.weight_orig', 'to_rgb_128.conv.bias', 'to_rgb_128.conv.weight_orig', 'to_rgb_256.conv.bias', 'to_rgb_256.conv.weight_orig'])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "853506a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_running.load_state_dict(G_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fc975393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.load_state_dict(D_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "380790fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8784, -2.1372, -0.5575,  ...,  0.5112,  0.4631, -0.6862],\n",
       "        [ 0.2292, -1.1222,  0.2686,  ...,  1.8446, -0.1479, -0.2290],\n",
       "        [-0.5751, -1.1901,  0.3560,  ...,  1.0936,  0.0292, -0.2377],\n",
       "        ...,\n",
       "        [ 1.1955,  0.8432, -0.2938,  ..., -1.7986,  0.1316,  0.7277],\n",
       "        [-0.6199, -0.9528,  1.0585,  ...,  1.1389,  0.4161,  0.6267],\n",
       "        [-0.5158, -0.7683, -0.3314,  ...,  0.3381,  0.3096,  1.3006]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(50, input_code_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8a179d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "72855414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating images from GAN\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in range(1,20):\n",
    "#         for j in range(1,10):\n",
    "#             well = g_running(torch.randn(5*10, input_code_size).to(device),step=i, alpha=j)\n",
    "#             images = well.data.cpu()\n",
    "   \n",
    "#             utils.save_image(images,f'sample/output_{j}_{i}.png',nrow=10,normalize=True,range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "5ea8d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step value 3 gives the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "05b2d128",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/PIL/ImageFile.py:495\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[1;32m    496\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [244]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m well \u001b[38;5;241m=\u001b[39m g_running(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m, input_code_size)\u001b[38;5;241m.\u001b[39mto(device),step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, alpha\u001b[38;5;241m=\u001b[39mi)\n\u001b[1;32m      4\u001b[0m images \u001b[38;5;241m=\u001b[39m well\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msample/output_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torchvision/utils.py:156\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    155\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(ndarr)\n\u001b[0;32m--> 156\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/PIL/Image.py:2212\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2209\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2212\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# do what we can to clean up\u001b[39;00m\n\u001b[1;32m   2215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/PIL/PngImagePlugin.py:1348\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode)\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/PIL/ImageFile.py:509\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 509\u001b[0m         l, s, d \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(d)\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m s:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#     for i in range(1,10):\n",
    "#         well = g_running(torch.randn(5*10, input_code_size).to(device),step=3, alpha=i)\n",
    "#         images = well.data.cpu()\n",
    "   \n",
    "#         utils.save_image(images,f'sample/output_{i}.png',nrow=10,normalize=True,range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "337342ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "#     for i in range(1,10):\n",
    "    well = g_running(torch.randn(5*10, input_code_size).to(device),step=3, alpha=i)\n",
    "    images = well.data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a96b0058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 32, 32])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a94df7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torchvision \n",
    "import torchvision.transforms as T \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e9757617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 3])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = images[random.choice(range(len(images)))].permute(1,2,0)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1e84c1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0156, -0.2112, -0.3759],\n",
       "         [ 0.0895, -0.1691, -0.3914],\n",
       "         [-0.5702, -0.6419, -0.7592],\n",
       "         ...,\n",
       "         [-0.3253, -0.4405, -0.5099],\n",
       "         [-0.2742, -0.3728, -0.4510],\n",
       "         [-0.1318, -0.2351, -0.2989]],\n",
       "\n",
       "        [[-0.0330, -0.3007, -0.4236],\n",
       "         [-0.2483, -0.4801, -0.6211],\n",
       "         [-0.5294, -0.6058, -0.7089],\n",
       "         ...,\n",
       "         [-0.5434, -0.6339, -0.6943],\n",
       "         [-0.3003, -0.3604, -0.4234],\n",
       "         [-0.1359, -0.2753, -0.3250]],\n",
       "\n",
       "        [[-0.1035, -0.3875, -0.5449],\n",
       "         [-0.1254, -0.4039, -0.5814],\n",
       "         [-0.2942, -0.4267, -0.5911],\n",
       "         ...,\n",
       "         [-0.6393, -0.7245, -0.7837],\n",
       "         [-0.4455, -0.5094, -0.5754],\n",
       "         [-0.1433, -0.2728, -0.3267]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6864,  0.5594,  0.2264],\n",
       "         [ 0.6495,  0.5185,  0.1407],\n",
       "         [ 0.6300,  0.4927,  0.1050],\n",
       "         ...,\n",
       "         [ 0.3558,  0.2598,  0.2063],\n",
       "         [ 0.4241,  0.3721,  0.3538],\n",
       "         [ 0.4851,  0.4202,  0.4064]],\n",
       "\n",
       "        [[ 0.6892,  0.5698,  0.1859],\n",
       "         [ 0.7157,  0.5681,  0.1625],\n",
       "         [ 0.6660,  0.5172,  0.1194],\n",
       "         ...,\n",
       "         [ 0.4028,  0.3512,  0.3411],\n",
       "         [ 0.4019,  0.4124,  0.4391],\n",
       "         [ 0.4256,  0.4094,  0.4426]],\n",
       "\n",
       "        [[ 0.6679,  0.5099,  0.1492],\n",
       "         [ 0.7171,  0.5353,  0.1358],\n",
       "         [ 0.6849,  0.5086,  0.1281],\n",
       "         ...,\n",
       "         [ 0.4049,  0.3856,  0.3477],\n",
       "         [ 0.2970,  0.3695,  0.3808],\n",
       "         [ 0.4136,  0.4727,  0.5447]]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "90a16bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = images[random.choice(range(len(images)))].permute(1,2,0)\n",
    "plt.matshow(image * 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "20ebbdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5e489785",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transform(images[random.choice(range(len(images)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "45fb3f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b3fcfe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## openning a normal image\n",
    "im = Image.open(r\"gan.png\") \n",
    "  \n",
    "# This method will show image in any image viewer \n",
    "#im.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "86b70761",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.PILToTensor()])\n",
    "\n",
    "tensor = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ed59e48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[184,  91,  81,  ..., 170, 158, 181],\n",
       "         [107,  52,  36,  ...,  58,  72,  70],\n",
       "         [113,  51,  25,  ...,  24,  36,  33],\n",
       "         ...,\n",
       "         [ 61,  35,  59,  ..., 145,   7, 234],\n",
       "         [ 41,  39,  47,  ..., 108, 228, 206],\n",
       "         [ 30,  22,  33,  ..., 100, 203, 232]],\n",
       "\n",
       "        [[108, 115,  78,  ..., 223, 227, 198],\n",
       "         [103, 117, 105,  ..., 193, 166, 155],\n",
       "         [101, 108, 102,  ..., 141, 114, 116],\n",
       "         ...,\n",
       "         [ 66,  47,  44,  ...,  62,  78,  58],\n",
       "         [ 52,  41,  46,  ...,  35,  69,  47],\n",
       "         [ 37,  44,  88,  ...,  30,  42,  49]],\n",
       "\n",
       "        [[ 41,  23,  21,  ...,  59, 122, 235],\n",
       "         [ 26,  24,  25,  ...,  69, 136, 177],\n",
       "         [ 48,  63,  86,  ...,  63, 112, 145],\n",
       "         ...,\n",
       "         [ 55,   4,  43,  ..., 246, 191, 117],\n",
       "         [ 40, 226, 252,  ..., 184, 181,  92],\n",
       "         [ 85,  39,  18,  ..., 119,  79,  55]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1ce2ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this for training models\n",
    "\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "# checkpoint = torch.load(PATH)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# model.eval()\n",
    "# # - or -\n",
    "# model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
