{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510a900e",
   "metadata": {},
   "source": [
    "from : https://github.com/clovaai/stargan-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b3c155c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: munch in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (2.5.0)\r\n",
      "Requirement already satisfied: six in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (from munch) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install munch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c6f69515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (1.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "348c5fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (0.19.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-image) (1.21.5)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-image) (2.8)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-image) (1.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-image) (2022.4.8)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-image) (2.17.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-image) (1.3.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-image) (9.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/misthios/anaconda3/envs/myenv/lib/python3.9/site-packages (from packaging>=20.0->scikit-image) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "15e87e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "\n",
    "from munch import Munch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from os.path import join as ospj\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "import json\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ffmpeg\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from skimage.filters import gaussian\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "\n",
    "import argparse\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "from torch.backends import cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dcdee36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.py\n",
    "\n",
    "class CheckpointIO(object):\n",
    "    def __init__(self, fname_template, data_parallel=False, **kwargs):\n",
    "        os.makedirs(os.path.dirname(fname_template), exist_ok=True)\n",
    "        self.fname_template = fname_template\n",
    "        self.module_dict = kwargs\n",
    "        self.data_parallel = data_parallel\n",
    "\n",
    "    def register(self, **kwargs):\n",
    "        self.module_dict.update(kwargs)\n",
    "\n",
    "    def save(self, step):\n",
    "        fname = self.fname_template.format(step)\n",
    "        print('Saving checkpoint into %s...' % fname)\n",
    "        outdict = {}\n",
    "        for name, module in self.module_dict.items():\n",
    "            if self.data_parallel:\n",
    "                outdict[name] = module.module.state_dict()\n",
    "            else:\n",
    "                outdict[name] = module.state_dict()\n",
    "                        \n",
    "        torch.save(outdict, fname)\n",
    "\n",
    "    def load(self, step):\n",
    "        fname = self.fname_template.format(step)\n",
    "        assert os.path.exists(fname), fname + ' does not exist!'\n",
    "        print('Loading checkpoint from %s...' % fname)\n",
    "        if torch.cuda.is_available():\n",
    "            module_dict = torch.load(fname)\n",
    "        else:\n",
    "            module_dict = torch.load(fname, map_location=torch.device('cpu'))\n",
    "            \n",
    "        for name, module in self.module_dict.items():\n",
    "            if self.data_parallel:\n",
    "                module.module.load_state_dict(module_dict[name])\n",
    "            else:\n",
    "                module.load_state_dict(module_dict[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1513e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader.py\n",
    "\n",
    "def listdir(dname):\n",
    "    fnames = list(chain(*[list(Path(dname).rglob('*.' + ext))\n",
    "                          for ext in ['png', 'jpg', 'jpeg', 'JPG']]))\n",
    "    return fnames\n",
    "\n",
    "\n",
    "class DefaultDataset(data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.samples = listdir(root)\n",
    "        self.samples.sort()\n",
    "        self.transform = transform\n",
    "        self.targets = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fname = self.samples[index]\n",
    "        img = Image.open(fname).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "class ReferenceDataset(data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.samples, self.targets = self._make_dataset(root)\n",
    "        self.transform = transform\n",
    "\n",
    "    def _make_dataset(self, root):\n",
    "        domains = os.listdir(root)\n",
    "        fnames, fnames2, labels = [], [], []\n",
    "        for idx, domain in enumerate(sorted(domains)):\n",
    "            class_dir = os.path.join(root, domain)\n",
    "            cls_fnames = listdir(class_dir)\n",
    "            fnames += cls_fnames\n",
    "            fnames2 += random.sample(cls_fnames, len(cls_fnames))\n",
    "            labels += [idx] * len(cls_fnames)\n",
    "        return list(zip(fnames, fnames2)), labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fname, fname2 = self.samples[index]\n",
    "        label = self.targets[index]\n",
    "        img = Image.open(fname).convert('RGB')\n",
    "        img2 = Image.open(fname2).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            img2 = self.transform(img2)\n",
    "        return img, img2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "\n",
    "def _make_balanced_sampler(labels):\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1. / class_counts\n",
    "    weights = class_weights[labels]\n",
    "    return WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "\n",
    "def get_train_loader(root, which='source', img_size=256,\n",
    "                     batch_size=8, prob=0.5, num_workers=4):\n",
    "    print('Preparing DataLoader to fetch %s images '\n",
    "          'during the training phase...' % which)\n",
    "\n",
    "    crop = transforms.RandomResizedCrop(\n",
    "        img_size, scale=[0.8, 1.0], ratio=[0.9, 1.1])\n",
    "    rand_crop = transforms.Lambda(\n",
    "        lambda x: crop(x) if random.random() < prob else x)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        rand_crop,\n",
    "        transforms.Resize([img_size, img_size]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                             std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    if which == 'source':\n",
    "        dataset = ImageFolder(root, transform)\n",
    "    elif which == 'reference':\n",
    "        dataset = ReferenceDataset(root, transform)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    sampler = _make_balanced_sampler(dataset.targets)\n",
    "    return data.DataLoader(dataset=dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           sampler=sampler,\n",
    "                           num_workers=num_workers,\n",
    "                           pin_memory=True,\n",
    "                           drop_last=True)\n",
    "\n",
    "\n",
    "def get_eval_loader(root, img_size=256, batch_size=32,\n",
    "                    imagenet_normalize=True, shuffle=True,\n",
    "                    num_workers=4, drop_last=False):\n",
    "    print('Preparing DataLoader for the evaluation phase...')\n",
    "    if imagenet_normalize:\n",
    "        height, width = 299, 299\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    else:\n",
    "        height, width = img_size, img_size\n",
    "        mean = [0.5, 0.5, 0.5]\n",
    "        std = [0.5, 0.5, 0.5]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize([img_size, img_size]),\n",
    "        transforms.Resize([height, width]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    dataset = DefaultDataset(root, transform=transform)\n",
    "    return data.DataLoader(dataset=dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           num_workers=num_workers,\n",
    "                           pin_memory=True,\n",
    "                           drop_last=drop_last)\n",
    "\n",
    "\n",
    "def get_test_loader(root, img_size=256, batch_size=32,\n",
    "                    shuffle=True, num_workers=4):\n",
    "    print('Preparing DataLoader for the generation phase...')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize([img_size, img_size]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                             std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    dataset = ImageFolder(root, transform)\n",
    "    return data.DataLoader(dataset=dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           num_workers=num_workers,\n",
    "                           pin_memory=True)\n",
    "\n",
    "\n",
    "class InputFetcher:\n",
    "    def __init__(self, loader, loader_ref=None, latent_dim=16, mode=''):\n",
    "        self.loader = loader\n",
    "        self.loader_ref = loader_ref\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mode = mode\n",
    "\n",
    "    def _fetch_inputs(self):\n",
    "        try:\n",
    "            x, y = next(self.iter)\n",
    "        except (AttributeError, StopIteration):\n",
    "            self.iter = iter(self.loader)\n",
    "            x, y = next(self.iter)\n",
    "        return x, y\n",
    "\n",
    "    def _fetch_refs(self):\n",
    "        try:\n",
    "            x, x2, y = next(self.iter_ref)\n",
    "        except (AttributeError, StopIteration):\n",
    "            self.iter_ref = iter(self.loader_ref)\n",
    "            x, x2, y = next(self.iter_ref)\n",
    "        return x, x2, y\n",
    "\n",
    "    def __next__(self):\n",
    "        x, y = self._fetch_inputs()\n",
    "        if self.mode == 'train':\n",
    "            x_ref, x_ref2, y_ref = self._fetch_refs()\n",
    "            z_trg = torch.randn(x.size(0), self.latent_dim)\n",
    "            z_trg2 = torch.randn(x.size(0), self.latent_dim)\n",
    "            inputs = Munch(x_src=x, y_src=y, y_ref=y_ref,\n",
    "                           x_ref=x_ref, x_ref2=x_ref2,\n",
    "                           z_trg=z_trg, z_trg2=z_trg2)\n",
    "        elif self.mode == 'val':\n",
    "            x_ref, y_ref = self._fetch_inputs()\n",
    "            inputs = Munch(x_src=x, y_src=y,\n",
    "                           x_ref=x_ref, y_ref=y_ref)\n",
    "        elif self.mode == 'test':\n",
    "            inputs = Munch(x=x, y=y)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return Munch({k: v.to(self.device)\n",
    "                      for k, v in inputs.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "80df7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "\n",
    "class ResBlk(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2),\n",
    "                 normalize=False, downsample=False):\n",
    "        super().__init__()\n",
    "        self.actv = actv\n",
    "        self.normalize = normalize\n",
    "        self.downsample = downsample\n",
    "        self.learned_sc = dim_in != dim_out\n",
    "        self._build_weights(dim_in, dim_out)\n",
    "\n",
    "    def _build_weights(self, dim_in, dim_out):\n",
    "        self.conv1 = nn.Conv2d(dim_in, dim_in, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n",
    "        if self.normalize:\n",
    "            self.norm1 = nn.InstanceNorm2d(dim_in, affine=True)\n",
    "            self.norm2 = nn.InstanceNorm2d(dim_in, affine=True)\n",
    "        if self.learned_sc:\n",
    "            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n",
    "\n",
    "    def _shortcut(self, x):\n",
    "        if self.learned_sc:\n",
    "            x = self.conv1x1(x)\n",
    "        if self.downsample:\n",
    "            x = F.avg_pool2d(x, 2)\n",
    "        return x\n",
    "\n",
    "    def _residual(self, x):\n",
    "        if self.normalize:\n",
    "            x = self.norm1(x)\n",
    "        x = self.actv(x)\n",
    "        x = self.conv1(x)\n",
    "        if self.downsample:\n",
    "            x = F.avg_pool2d(x, 2)\n",
    "        if self.normalize:\n",
    "            x = self.norm2(x)\n",
    "        x = self.actv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._shortcut(x) + self._residual(x)\n",
    "        return x / math.sqrt(2)  # unit variance\n",
    "\n",
    "\n",
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, style_dim, num_features):\n",
    "        super().__init__()\n",
    "        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n",
    "        self.fc = nn.Linear(style_dim, num_features*2)\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        h = self.fc(s)\n",
    "        h = h.view(h.size(0), h.size(1), 1, 1)\n",
    "        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n",
    "        return (1 + gamma) * self.norm(x) + beta\n",
    "\n",
    "\n",
    "class AdainResBlk(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, style_dim=64, w_hpf=0,\n",
    "                 actv=nn.LeakyReLU(0.2), upsample=False):\n",
    "        super().__init__()\n",
    "        self.w_hpf = w_hpf\n",
    "        self.actv = actv\n",
    "        self.upsample = upsample\n",
    "        self.learned_sc = dim_in != dim_out\n",
    "        self._build_weights(dim_in, dim_out, style_dim)\n",
    "\n",
    "    def _build_weights(self, dim_in, dim_out, style_dim=64):\n",
    "        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n",
    "        self.norm1 = AdaIN(style_dim, dim_in)\n",
    "        self.norm2 = AdaIN(style_dim, dim_out)\n",
    "        if self.learned_sc:\n",
    "            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n",
    "\n",
    "    def _shortcut(self, x):\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        if self.learned_sc:\n",
    "            x = self.conv1x1(x)\n",
    "        return x\n",
    "\n",
    "    def _residual(self, x, s):\n",
    "        x = self.norm1(x, s)\n",
    "        x = self.actv(x)\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm2(x, s)\n",
    "        x = self.actv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        out = self._residual(x, s)\n",
    "        if self.w_hpf == 0:\n",
    "            out = (out + self._shortcut(x)) / math.sqrt(2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class HighPass(nn.Module):\n",
    "    def __init__(self, w_hpf, device):\n",
    "        super(HighPass, self).__init__()\n",
    "        self.register_buffer('filter',\n",
    "                             torch.tensor([[-1, -1, -1],\n",
    "                                           [-1, 8., -1],\n",
    "                                           [-1, -1, -1]]) / w_hpf)\n",
    "\n",
    "    def forward(self, x):\n",
    "        filter = self.filter.unsqueeze(0).unsqueeze(1).repeat(x.size(1), 1, 1, 1)\n",
    "        return F.conv2d(x, filter, padding=1, groups=x.size(1))\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_size=256, style_dim=64, max_conv_dim=512, w_hpf=1):\n",
    "        super().__init__()\n",
    "        dim_in = 2**14 // img_size\n",
    "        self.img_size = img_size\n",
    "        self.from_rgb = nn.Conv2d(3, dim_in, 3, 1, 1)\n",
    "        self.encode = nn.ModuleList()\n",
    "        self.decode = nn.ModuleList()\n",
    "        self.to_rgb = nn.Sequential(\n",
    "            nn.InstanceNorm2d(dim_in, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(dim_in, 3, 1, 1, 0))\n",
    "\n",
    "        # down/up-sampling blocks\n",
    "        repeat_num = int(np.log2(img_size)) - 4\n",
    "        if w_hpf > 0:\n",
    "            repeat_num += 1\n",
    "        for _ in range(repeat_num):\n",
    "            dim_out = min(dim_in*2, max_conv_dim)\n",
    "            self.encode.append(\n",
    "                ResBlk(dim_in, dim_out, normalize=True, downsample=True))\n",
    "            self.decode.insert(\n",
    "                0, AdainResBlk(dim_out, dim_in, style_dim,\n",
    "                               w_hpf=w_hpf, upsample=True))  # stack-like\n",
    "            dim_in = dim_out\n",
    "\n",
    "        # bottleneck blocks\n",
    "        for _ in range(2):\n",
    "            self.encode.append(\n",
    "                ResBlk(dim_out, dim_out, normalize=True))\n",
    "            self.decode.insert(\n",
    "                0, AdainResBlk(dim_out, dim_out, style_dim, w_hpf=w_hpf))\n",
    "\n",
    "        if w_hpf > 0:\n",
    "            device = torch.device(\n",
    "                'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            self.hpf = HighPass(w_hpf, device)\n",
    "\n",
    "    def forward(self, x, s, masks=None):\n",
    "        x = self.from_rgb(x)\n",
    "        cache = {}\n",
    "        for block in self.encode:\n",
    "            if (masks is not None) and (x.size(2) in [32, 64, 128]):\n",
    "                cache[x.size(2)] = x\n",
    "            x = block(x)\n",
    "        for block in self.decode:\n",
    "            x = block(x, s)\n",
    "            if (masks is not None) and (x.size(2) in [32, 64, 128]):\n",
    "                mask = masks[0] if x.size(2) in [32] else masks[1]\n",
    "                mask = F.interpolate(mask, size=x.size(2), mode='bilinear')\n",
    "                x = x + self.hpf(mask * cache[x.size(2)])\n",
    "        return self.to_rgb(x)\n",
    "\n",
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim=16, style_dim=64, num_domains=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers += [nn.Linear(latent_dim, 512)]\n",
    "        layers += [nn.ReLU()]\n",
    "        for _ in range(3):\n",
    "            layers += [nn.Linear(512, 512)]\n",
    "            layers += [nn.ReLU()]\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "\n",
    "        self.unshared = nn.ModuleList()\n",
    "        for _ in range(num_domains):\n",
    "            self.unshared += [nn.Sequential(nn.Linear(512, 512),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(512, 512),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(512, 512),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(512, style_dim))]\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        h = self.shared(z)\n",
    "        out = []\n",
    "        for layer in self.unshared:\n",
    "            out += [layer(h)]\n",
    "        out = torch.stack(out, dim=1)  # (batch, num_domains, style_dim)\n",
    "        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n",
    "        s = out[idx, y]  # (batch, style_dim)\n",
    "        return s\n",
    "\n",
    "\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, img_size=256, style_dim=64, num_domains=2, max_conv_dim=512):\n",
    "        super().__init__()\n",
    "        dim_in = 2**14 // img_size\n",
    "        blocks = []\n",
    "        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n",
    "\n",
    "        repeat_num = int(np.log2(img_size)) - 2\n",
    "        for _ in range(repeat_num):\n",
    "            dim_out = min(dim_in*2, max_conv_dim)\n",
    "            blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n",
    "            dim_in = dim_out\n",
    "\n",
    "        blocks += [nn.LeakyReLU(0.2)]\n",
    "        blocks += [nn.Conv2d(dim_out, dim_out, 4, 1, 0)]\n",
    "        blocks += [nn.LeakyReLU(0.2)]\n",
    "        self.shared = nn.Sequential(*blocks)\n",
    "\n",
    "        self.unshared = nn.ModuleList()\n",
    "        for _ in range(num_domains):\n",
    "            self.unshared += [nn.Linear(dim_out, style_dim)]\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        h = self.shared(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        out = []\n",
    "        for layer in self.unshared:\n",
    "            out += [layer(h)]\n",
    "        out = torch.stack(out, dim=1)  # (batch, num_domains, style_dim)\n",
    "        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n",
    "        s = out[idx, y]  # (batch, style_dim)\n",
    "        return s\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size=256, num_domains=2, max_conv_dim=512):\n",
    "        super().__init__()\n",
    "        dim_in = 2**14 // img_size\n",
    "        blocks = []\n",
    "        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n",
    "\n",
    "        repeat_num = int(np.log2(img_size)) - 2\n",
    "        for _ in range(repeat_num):\n",
    "            dim_out = min(dim_in*2, max_conv_dim)\n",
    "            blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n",
    "            dim_in = dim_out\n",
    "\n",
    "        blocks += [nn.LeakyReLU(0.2)]\n",
    "        blocks += [nn.Conv2d(dim_out, dim_out, 4, 1, 0)]\n",
    "        blocks += [nn.LeakyReLU(0.2)]\n",
    "        blocks += [nn.Conv2d(dim_out, num_domains, 1, 1, 0)]\n",
    "        self.main = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.main(x)\n",
    "        out = out.view(out.size(0), -1)  # (batch, num_domains)\n",
    "        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n",
    "        out = out[idx, y]  # (batch)\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_model(args):\n",
    "    generator = nn.DataParallel(Generator(args.img_size, args.style_dim, w_hpf=args.w_hpf))\n",
    "    mapping_network = nn.DataParallel(MappingNetwork(args.latent_dim, args.style_dim, args.num_domains))\n",
    "    style_encoder = nn.DataParallel(StyleEncoder(args.img_size, args.style_dim, args.num_domains))\n",
    "    discriminator = nn.DataParallel(Discriminator(args.img_size, args.num_domains))\n",
    "    generator_ema = copy.deepcopy(generator)\n",
    "    mapping_network_ema = copy.deepcopy(mapping_network)\n",
    "    style_encoder_ema = copy.deepcopy(style_encoder)\n",
    "\n",
    "    nets = Munch(generator=generator,\n",
    "                 mapping_network=mapping_network,\n",
    "                 style_encoder=style_encoder,\n",
    "                 discriminator=discriminator)\n",
    "    nets_ema = Munch(generator=generator_ema,\n",
    "                     mapping_network=mapping_network_ema,\n",
    "                     style_encoder=style_encoder_ema)\n",
    "\n",
    "    if args.w_hpf > 0:\n",
    "        fan = nn.DataParallel(FAN(fname_pretrained=args.wing_path).eval())\n",
    "        fan.get_heatmap = fan.module.get_heatmap\n",
    "        nets.fan = fan\n",
    "        nets_ema.fan = fan\n",
    "\n",
    "    return nets, nets_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2bd93eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver.py\n",
    "\n",
    "class Solver(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.nets, self.nets_ema = build_model(args)\n",
    "        # below setattrs are to make networks be children of Solver, e.g., for self.to(self.device)\n",
    "        for name, module in self.nets.items():\n",
    "            utils.print_network(module, name)\n",
    "            setattr(self, name, module)\n",
    "        for name, module in self.nets_ema.items():\n",
    "            setattr(self, name + '_ema', module)\n",
    "\n",
    "        if args.mode == 'train':\n",
    "            self.optims = Munch()\n",
    "            for net in self.nets.keys():\n",
    "                if net == 'fan':\n",
    "                    continue\n",
    "                self.optims[net] = torch.optim.Adam(\n",
    "                    params=self.nets[net].parameters(),\n",
    "                    lr=args.f_lr if net == 'mapping_network' else args.lr,\n",
    "                    betas=[args.beta1, args.beta2],\n",
    "                    weight_decay=args.weight_decay)\n",
    "\n",
    "            self.ckptios = [\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets.ckpt'), data_parallel=True, **self.nets),\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets_ema.ckpt'), data_parallel=True, **self.nets_ema),\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_optims.ckpt'), **self.optims)]\n",
    "        else:\n",
    "            self.ckptios = [CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets_ema.ckpt'), data_parallel=True, **self.nets_ema)]\n",
    "\n",
    "        self.to(self.device)\n",
    "        for name, network in self.named_children():\n",
    "            # Do not initialize the FAN parameters\n",
    "            if ('ema' not in name) and ('fan' not in name):\n",
    "                print('Initializing %s...' % name)\n",
    "                network.apply(utils.he_init)\n",
    "\n",
    "    def _save_checkpoint(self, step):\n",
    "        for ckptio in self.ckptios:\n",
    "            ckptio.save(step)\n",
    "\n",
    "    def _load_checkpoint(self, step):\n",
    "        for ckptio in self.ckptios:\n",
    "            ckptio.load(step)\n",
    "\n",
    "    def _reset_grad(self):\n",
    "        for optim in self.optims.values():\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def train(self, loaders):\n",
    "        args = self.args\n",
    "        nets = self.nets\n",
    "        nets_ema = self.nets_ema\n",
    "        optims = self.optims\n",
    "\n",
    "        # fetch random validation images for debugging\n",
    "        fetcher = InputFetcher(loaders.src, loaders.ref, args.latent_dim, 'train')\n",
    "        fetcher_val = InputFetcher(loaders.val, None, args.latent_dim, 'val')\n",
    "        inputs_val = next(fetcher_val)\n",
    "\n",
    "        # resume training if necessary\n",
    "        if args.resume_iter > 0:\n",
    "            self._load_checkpoint(args.resume_iter)\n",
    "\n",
    "        # remember the initial value of ds weight\n",
    "        initial_lambda_ds = args.lambda_ds\n",
    "\n",
    "        print('Start training...')\n",
    "        start_time = time.time()\n",
    "        for i in range(args.resume_iter, args.total_iters):\n",
    "            # fetch images and labels\n",
    "            inputs = next(fetcher)\n",
    "            x_real, y_org = inputs.x_src, inputs.y_src\n",
    "            x_ref, x_ref2, y_trg = inputs.x_ref, inputs.x_ref2, inputs.y_ref\n",
    "            z_trg, z_trg2 = inputs.z_trg, inputs.z_trg2\n",
    "\n",
    "            masks = nets.fan.get_heatmap(x_real) if args.w_hpf > 0 else None\n",
    "\n",
    "            # train the discriminator\n",
    "            d_loss, d_losses_latent = compute_d_loss(\n",
    "                nets, args, x_real, y_org, y_trg, z_trg=z_trg, masks=masks)\n",
    "            self._reset_grad()\n",
    "            d_loss.backward()\n",
    "            optims.discriminator.step()\n",
    "\n",
    "            d_loss, d_losses_ref = compute_d_loss(\n",
    "                nets, args, x_real, y_org, y_trg, x_ref=x_ref, masks=masks)\n",
    "            self._reset_grad()\n",
    "            d_loss.backward()\n",
    "            optims.discriminator.step()\n",
    "\n",
    "            # train the generator\n",
    "            g_loss, g_losses_latent = compute_g_loss(\n",
    "                nets, args, x_real, y_org, y_trg, z_trgs=[z_trg, z_trg2], masks=masks)\n",
    "            self._reset_grad()\n",
    "            g_loss.backward()\n",
    "            optims.generator.step()\n",
    "            optims.mapping_network.step()\n",
    "            optims.style_encoder.step()\n",
    "\n",
    "            g_loss, g_losses_ref = compute_g_loss(\n",
    "                nets, args, x_real, y_org, y_trg, x_refs=[x_ref, x_ref2], masks=masks)\n",
    "            self._reset_grad()\n",
    "            g_loss.backward()\n",
    "            optims.generator.step()\n",
    "\n",
    "            # compute moving average of network parameters\n",
    "            moving_average(nets.generator, nets_ema.generator, beta=0.999)\n",
    "            moving_average(nets.mapping_network, nets_ema.mapping_network, beta=0.999)\n",
    "            moving_average(nets.style_encoder, nets_ema.style_encoder, beta=0.999)\n",
    "\n",
    "            # decay weight for diversity sensitive loss\n",
    "            if args.lambda_ds > 0:\n",
    "                args.lambda_ds -= (initial_lambda_ds / args.ds_iter)\n",
    "\n",
    "            # print out log info\n",
    "            if (i+1) % args.print_every == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                elapsed = str(datetime.timedelta(seconds=elapsed))[:-7]\n",
    "                log = \"Elapsed time [%s], Iteration [%i/%i], \" % (elapsed, i+1, args.total_iters)\n",
    "                all_losses = dict()\n",
    "                for loss, prefix in zip([d_losses_latent, d_losses_ref, g_losses_latent, g_losses_ref],\n",
    "                                        ['D/latent_', 'D/ref_', 'G/latent_', 'G/ref_']):\n",
    "                    for key, value in loss.items():\n",
    "                        all_losses[prefix + key] = value\n",
    "                all_losses['G/lambda_ds'] = args.lambda_ds\n",
    "                log += ' '.join(['%s: [%.4f]' % (key, value) for key, value in all_losses.items()])\n",
    "                print(log)\n",
    "\n",
    "            # generate images for debugging\n",
    "            if (i+1) % args.sample_every == 0:\n",
    "                os.makedirs(args.sample_dir, exist_ok=True)\n",
    "                utils.debug_image(nets_ema, args, inputs=inputs_val, step=i+1)\n",
    "\n",
    "            # save model checkpoints\n",
    "            if (i+1) % args.save_every == 0:\n",
    "                self._save_checkpoint(step=i+1)\n",
    "\n",
    "            # compute FID and LPIPS if necessary\n",
    "            if (i+1) % args.eval_every == 0:\n",
    "                calculate_metrics(nets_ema, args, i+1, mode='latent')\n",
    "                calculate_metrics(nets_ema, args, i+1, mode='reference')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, loaders):\n",
    "        args = self.args\n",
    "        nets_ema = self.nets_ema\n",
    "        os.makedirs(args.result_dir, exist_ok=True)\n",
    "        self._load_checkpoint(args.resume_iter)\n",
    "\n",
    "        src = next(InputFetcher(loaders.src, None, args.latent_dim, 'test'))\n",
    "        ref = next(InputFetcher(loaders.ref, None, args.latent_dim, 'test'))\n",
    "\n",
    "        fname = ospj(args.result_dir, 'reference.jpg')\n",
    "        print('Working on {}...'.format(fname))\n",
    "        utils.translate_using_reference(nets_ema, args, src.x, ref.x, ref.y, fname)\n",
    "\n",
    "        fname = ospj(args.result_dir, 'video_ref.mp4')\n",
    "        print('Working on {}...'.format(fname))\n",
    "        utils.video_ref(nets_ema, args, src.x, ref.x, ref.y, fname)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        args = self.args\n",
    "        nets_ema = self.nets_ema\n",
    "        resume_iter = args.resume_iter\n",
    "        self._load_checkpoint(args.resume_iter)\n",
    "        calculate_metrics(nets_ema, args, step=resume_iter, mode='latent')\n",
    "        calculate_metrics(nets_ema, args, step=resume_iter, mode='reference')\n",
    "\n",
    "\n",
    "def compute_d_loss(nets, args, x_real, y_org, y_trg, z_trg=None, x_ref=None, masks=None):\n",
    "    assert (z_trg is None) != (x_ref is None)\n",
    "    # with real images\n",
    "    x_real.requires_grad_()\n",
    "    out = nets.discriminator(x_real, y_org)\n",
    "    loss_real = adv_loss(out, 1)\n",
    "    loss_reg = r1_reg(out, x_real)\n",
    "\n",
    "    # with fake images\n",
    "    with torch.no_grad():\n",
    "        if z_trg is not None:\n",
    "            s_trg = nets.mapping_network(z_trg, y_trg)\n",
    "        else:  # x_ref is not None\n",
    "            s_trg = nets.style_encoder(x_ref, y_trg)\n",
    "\n",
    "        x_fake = nets.generator(x_real, s_trg, masks=masks)\n",
    "    out = nets.discriminator(x_fake, y_trg)\n",
    "    loss_fake = adv_loss(out, 0)\n",
    "\n",
    "    loss = loss_real + loss_fake + args.lambda_reg * loss_reg\n",
    "    return loss, Munch(real=loss_real.item(),\n",
    "                       fake=loss_fake.item(),\n",
    "                       reg=loss_reg.item())\n",
    "\n",
    "\n",
    "def compute_g_loss(nets, args, x_real, y_org, y_trg, z_trgs=None, x_refs=None, masks=None):\n",
    "    assert (z_trgs is None) != (x_refs is None)\n",
    "    if z_trgs is not None:\n",
    "        z_trg, z_trg2 = z_trgs\n",
    "    if x_refs is not None:\n",
    "        x_ref, x_ref2 = x_refs\n",
    "\n",
    "    # adversarial loss\n",
    "    if z_trgs is not None:\n",
    "        s_trg = nets.mapping_network(z_trg, y_trg)\n",
    "    else:\n",
    "        s_trg = nets.style_encoder(x_ref, y_trg)\n",
    "\n",
    "    x_fake = nets.generator(x_real, s_trg, masks=masks)\n",
    "    out = nets.discriminator(x_fake, y_trg)\n",
    "    loss_adv = adv_loss(out, 1)\n",
    "\n",
    "    # style reconstruction loss\n",
    "    s_pred = nets.style_encoder(x_fake, y_trg)\n",
    "    loss_sty = torch.mean(torch.abs(s_pred - s_trg))\n",
    "\n",
    "    # diversity sensitive loss\n",
    "    if z_trgs is not None:\n",
    "        s_trg2 = nets.mapping_network(z_trg2, y_trg)\n",
    "    else:\n",
    "        s_trg2 = nets.style_encoder(x_ref2, y_trg)\n",
    "    x_fake2 = nets.generator(x_real, s_trg2, masks=masks)\n",
    "    x_fake2 = x_fake2.detach()\n",
    "    loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n",
    "\n",
    "    # cycle-consistency loss\n",
    "    masks = nets.fan.get_heatmap(x_fake) if args.w_hpf > 0 else None\n",
    "    s_org = nets.style_encoder(x_real, y_org)\n",
    "    x_rec = nets.generator(x_fake, s_org, masks=masks)\n",
    "    loss_cyc = torch.mean(torch.abs(x_rec - x_real))\n",
    "\n",
    "    loss = loss_adv + args.lambda_sty * loss_sty \\\n",
    "        - args.lambda_ds * loss_ds + args.lambda_cyc * loss_cyc\n",
    "    return loss, Munch(adv=loss_adv.item(),\n",
    "                       sty=loss_sty.item(),\n",
    "                       ds=loss_ds.item(),\n",
    "                       cyc=loss_cyc.item())\n",
    "\n",
    "\n",
    "def moving_average(model, model_test, beta=0.999):\n",
    "    for param, param_test in zip(model.parameters(), model_test.parameters()):\n",
    "        param_test.data = torch.lerp(param.data, param_test.data, beta)\n",
    "\n",
    "\n",
    "def adv_loss(logits, target):\n",
    "    assert target in [1, 0]\n",
    "    targets = torch.full_like(logits, fill_value=target)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def r1_reg(d_out, x_in):\n",
    "    # zero-centered gradient penalty for real images\n",
    "    batch_size = x_in.size(0)\n",
    "    grad_dout = torch.autograd.grad(\n",
    "        outputs=d_out.sum(), inputs=x_in,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    grad_dout2 = grad_dout.pow(2)\n",
    "    assert(grad_dout2.size() == x_in.size())\n",
    "    reg = 0.5 * grad_dout2.view(batch_size, -1).sum(1).mean(0)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3bf9f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "def save_json(json_file, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(json_file, f, indent=4, sort_keys=False)\n",
    "\n",
    "\n",
    "def print_network(network, name):\n",
    "    num_params = 0\n",
    "    for p in network.parameters():\n",
    "        num_params += p.numel()\n",
    "    # print(network)\n",
    "    print(\"Number of parameters of %s: %i\" % (name, num_params))\n",
    "\n",
    "\n",
    "def he_init(module):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if module.bias is not None:\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if module.bias is not None:\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "\n",
    "\n",
    "def denormalize(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp_(0, 1)\n",
    "\n",
    "\n",
    "def save_image(x, ncol, filename):\n",
    "    x = denormalize(x)\n",
    "    vutils.save_image(x.cpu(), filename, nrow=ncol, padding=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_and_reconstruct(nets, args, x_src, y_src, x_ref, y_ref, filename):\n",
    "    N, C, H, W = x_src.size()\n",
    "    s_ref = nets.style_encoder(x_ref, y_ref)\n",
    "    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n",
    "    x_fake = nets.generator(x_src, s_ref, masks=masks)\n",
    "    s_src = nets.style_encoder(x_src, y_src)\n",
    "    masks = nets.fan.get_heatmap(x_fake) if args.w_hpf > 0 else None\n",
    "    x_rec = nets.generator(x_fake, s_src, masks=masks)\n",
    "    x_concat = [x_src, x_ref, x_fake, x_rec]\n",
    "    x_concat = torch.cat(x_concat, dim=0)\n",
    "    save_image(x_concat, N, filename)\n",
    "    del x_concat\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_using_latent(nets, args, x_src, y_trg_list, z_trg_list, psi, filename):\n",
    "    N, C, H, W = x_src.size()\n",
    "    latent_dim = z_trg_list[0].size(1)\n",
    "    x_concat = [x_src]\n",
    "    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n",
    "\n",
    "    for i, y_trg in enumerate(y_trg_list):\n",
    "        z_many = torch.randn(10000, latent_dim).to(x_src.device)\n",
    "        y_many = torch.LongTensor(10000).to(x_src.device).fill_(y_trg[0])\n",
    "        s_many = nets.mapping_network(z_many, y_many)\n",
    "        s_avg = torch.mean(s_many, dim=0, keepdim=True)\n",
    "        s_avg = s_avg.repeat(N, 1)\n",
    "\n",
    "        for z_trg in z_trg_list:\n",
    "            s_trg = nets.mapping_network(z_trg, y_trg)\n",
    "            s_trg = torch.lerp(s_avg, s_trg, psi)\n",
    "            x_fake = nets.generator(x_src, s_trg, masks=masks)\n",
    "            x_concat += [x_fake]\n",
    "\n",
    "    x_concat = torch.cat(x_concat, dim=0)\n",
    "    save_image(x_concat, N, filename)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_using_reference(nets, args, x_src, x_ref, y_ref, filename):\n",
    "    N, C, H, W = x_src.size()\n",
    "    wb = torch.ones(1, C, H, W).to(x_src.device)\n",
    "    x_src_with_wb = torch.cat([wb, x_src], dim=0)\n",
    "\n",
    "    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n",
    "    s_ref = nets.style_encoder(x_ref, y_ref)\n",
    "    s_ref_list = s_ref.unsqueeze(1).repeat(1, N, 1)\n",
    "    x_concat = [x_src_with_wb]\n",
    "    for i, s_ref in enumerate(s_ref_list):\n",
    "        x_fake = nets.generator(x_src, s_ref, masks=masks)\n",
    "        x_fake_with_ref = torch.cat([x_ref[i:i+1], x_fake], dim=0)\n",
    "        x_concat += [x_fake_with_ref]\n",
    "\n",
    "    x_concat = torch.cat(x_concat, dim=0)\n",
    "    save_image(x_concat, N+1, filename)\n",
    "    del x_concat\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def debug_image(nets, args, inputs, step):\n",
    "    x_src, y_src = inputs.x_src, inputs.y_src\n",
    "    x_ref, y_ref = inputs.x_ref, inputs.y_ref\n",
    "\n",
    "    device = inputs.x_src.device\n",
    "    N = inputs.x_src.size(0)\n",
    "\n",
    "    # translate and reconstruct (reference-guided)\n",
    "    filename = ospj(args.sample_dir, '%06d_cycle_consistency.jpg' % (step))\n",
    "    translate_and_reconstruct(nets, args, x_src, y_src, x_ref, y_ref, filename)\n",
    "\n",
    "    # latent-guided image synthesis\n",
    "    y_trg_list = [torch.tensor(y).repeat(N).to(device)\n",
    "                  for y in range(min(args.num_domains, 5))]\n",
    "    z_trg_list = torch.randn(args.num_outs_per_domain, 1, args.latent_dim).repeat(1, N, 1).to(device)\n",
    "    for psi in [0.5, 0.7, 1.0]:\n",
    "        filename = ospj(args.sample_dir, '%06d_latent_psi_%.1f.jpg' % (step, psi))\n",
    "        translate_using_latent(nets, args, x_src, y_trg_list, z_trg_list, psi, filename)\n",
    "\n",
    "    # reference-guided image synthesis\n",
    "    filename = ospj(args.sample_dir, '%06d_reference.jpg' % (step))\n",
    "    translate_using_reference(nets, args, x_src, x_ref, y_ref, filename)\n",
    "\n",
    "\n",
    "# ======================= #\n",
    "# Video-related functions #\n",
    "# ======================= #\n",
    "\n",
    "\n",
    "def sigmoid(x, w=1):\n",
    "    return 1. / (1 + np.exp(-w * x))\n",
    "\n",
    "\n",
    "def get_alphas(start=-5, end=5, step=0.5, len_tail=10):\n",
    "    return [0] + [sigmoid(alpha) for alpha in np.arange(start, end, step)] + [1] * len_tail\n",
    "\n",
    "\n",
    "def interpolate(nets, args, x_src, s_prev, s_next):\n",
    "    ''' returns T x C x H x W '''\n",
    "    B = x_src.size(0)\n",
    "    frames = []\n",
    "    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n",
    "    alphas = get_alphas()\n",
    "\n",
    "    for alpha in alphas:\n",
    "        s_ref = torch.lerp(s_prev, s_next, alpha)\n",
    "        x_fake = nets.generator(x_src, s_ref, masks=masks)\n",
    "        entries = torch.cat([x_src.cpu(), x_fake.cpu()], dim=2)\n",
    "        frame = torchvision.utils.make_grid(entries, nrow=B, padding=0, pad_value=-1).unsqueeze(0)\n",
    "        frames.append(frame)\n",
    "    frames = torch.cat(frames)\n",
    "    return frames\n",
    "\n",
    "\n",
    "def slide(entries, margin=32):\n",
    "    \"\"\"Returns a sliding reference window.\n",
    "    Args:\n",
    "        entries: a list containing two reference images, x_prev and x_next, \n",
    "                 both of which has a shape (1, 3, 256, 256)\n",
    "    Returns:\n",
    "        canvas: output slide of shape (num_frames, 3, 256*2, 256+margin)\n",
    "    \"\"\"\n",
    "    _, C, H, W = entries[0].shape\n",
    "    alphas = get_alphas()\n",
    "    T = len(alphas) # number of frames\n",
    "\n",
    "    canvas = - torch.ones((T, C, H*2, W + margin))\n",
    "    merged = torch.cat(entries, dim=2)  # (1, 3, 512, 256)\n",
    "    for t, alpha in enumerate(alphas):\n",
    "        top = int(H * (1 - alpha))  # top, bottom for canvas\n",
    "        bottom = H * 2\n",
    "        m_top = 0  # top, bottom for merged\n",
    "        m_bottom = 2 * H - top\n",
    "        canvas[t, :, top:bottom, :W] = merged[:, :, m_top:m_bottom, :]\n",
    "    return canvas\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def video_ref(nets, args, x_src, x_ref, y_ref, fname):\n",
    "    video = []\n",
    "    s_ref = nets.style_encoder(x_ref, y_ref)\n",
    "    s_prev = None\n",
    "    for data_next in tqdm(zip(x_ref, y_ref, s_ref), 'video_ref', len(x_ref)):\n",
    "        x_next, y_next, s_next = [d.unsqueeze(0) for d in data_next]\n",
    "        if s_prev is None:\n",
    "            x_prev, y_prev, s_prev = x_next, y_next, s_next\n",
    "            continue\n",
    "        if y_prev != y_next:\n",
    "            x_prev, y_prev, s_prev = x_next, y_next, s_next\n",
    "            continue\n",
    "\n",
    "        interpolated = interpolate(nets, args, x_src, s_prev, s_next)\n",
    "        entries = [x_prev, x_next]\n",
    "        slided = slide(entries)  # (T, C, 256*2, 256)\n",
    "        frames = torch.cat([slided, interpolated], dim=3).cpu()  # (T, C, 256*2, 256*(batch+1))\n",
    "        video.append(frames)\n",
    "        x_prev, y_prev, s_prev = x_next, y_next, s_next\n",
    "\n",
    "    # append last frame 10 time\n",
    "    for _ in range(10):\n",
    "        video.append(frames[-1:])\n",
    "    video = tensor2ndarray255(torch.cat(video))\n",
    "    save_video(fname, video)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def video_latent(nets, args, x_src, y_list, z_list, psi, fname):\n",
    "    latent_dim = z_list[0].size(1)\n",
    "    s_list = []\n",
    "    for i, y_trg in enumerate(y_list):\n",
    "        z_many = torch.randn(10000, latent_dim).to(x_src.device)\n",
    "        y_many = torch.LongTensor(10000).to(x_src.device).fill_(y_trg[0])\n",
    "        s_many = nets.mapping_network(z_many, y_many)\n",
    "        s_avg = torch.mean(s_many, dim=0, keepdim=True)\n",
    "        s_avg = s_avg.repeat(x_src.size(0), 1)\n",
    "\n",
    "        for z_trg in z_list:\n",
    "            s_trg = nets.mapping_network(z_trg, y_trg)\n",
    "            s_trg = torch.lerp(s_avg, s_trg, psi)\n",
    "            s_list.append(s_trg)\n",
    "\n",
    "    s_prev = None\n",
    "    video = []\n",
    "    # fetch reference images\n",
    "    for idx_ref, s_next in enumerate(tqdm(s_list, 'video_latent', len(s_list))):\n",
    "        if s_prev is None:\n",
    "            s_prev = s_next\n",
    "            continue\n",
    "        if idx_ref % len(z_list) == 0:\n",
    "            s_prev = s_next\n",
    "            continue\n",
    "        frames = interpolate(nets, args, x_src, s_prev, s_next).cpu()\n",
    "        video.append(frames)\n",
    "        s_prev = s_next\n",
    "    for _ in range(10):\n",
    "        video.append(frames[-1:])\n",
    "    video = tensor2ndarray255(torch.cat(video))\n",
    "    save_video(fname, video)\n",
    "\n",
    "\n",
    "def save_video(fname, images, output_fps=30, vcodec='libx264', filters=''):\n",
    "    assert isinstance(images, np.ndarray), \"images should be np.array: NHWC\"\n",
    "    num_frames, height, width, channels = images.shape\n",
    "    stream = ffmpeg.input('pipe:', format='rawvideo', \n",
    "                          pix_fmt='rgb24', s='{}x{}'.format(width, height))\n",
    "    stream = ffmpeg.filter(stream, 'setpts', '2*PTS')  # 2*PTS is for slower playback\n",
    "    stream = ffmpeg.output(stream, fname, pix_fmt='yuv420p', vcodec=vcodec, r=output_fps)\n",
    "    stream = ffmpeg.overwrite_output(stream)\n",
    "    process = ffmpeg.run_async(stream, pipe_stdin=True)\n",
    "    for frame in tqdm(images, desc='writing video to %s' % fname):\n",
    "        process.stdin.write(frame.astype(np.uint8).tobytes())\n",
    "    process.stdin.close()\n",
    "    process.wait()\n",
    "\n",
    "\n",
    "def tensor2ndarray255(images):\n",
    "    images = torch.clamp(images * 0.5 + 0.5, 0, 1)\n",
    "    return images.cpu().numpy().transpose(0, 2, 3, 1) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "04266e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wing.py\n",
    "\n",
    "def get_preds_fromhm(hm):\n",
    "    max, idx = torch.max(\n",
    "        hm.view(hm.size(0), hm.size(1), hm.size(2) * hm.size(3)), 2)\n",
    "    idx += 1\n",
    "    preds = idx.view(idx.size(0), idx.size(1), 1).repeat(1, 1, 2).float()\n",
    "    preds[..., 0].apply_(lambda x: (x - 1) % hm.size(3) + 1)\n",
    "    preds[..., 1].add_(-1).div_(hm.size(2)).floor_().add_(1)\n",
    "\n",
    "    for i in range(preds.size(0)):\n",
    "        for j in range(preds.size(1)):\n",
    "            hm_ = hm[i, j, :]\n",
    "            pX, pY = int(preds[i, j, 0]) - 1, int(preds[i, j, 1]) - 1\n",
    "            if pX > 0 and pX < 63 and pY > 0 and pY < 63:\n",
    "                diff = torch.FloatTensor(\n",
    "                    [hm_[pY, pX + 1] - hm_[pY, pX - 1],\n",
    "                     hm_[pY + 1, pX] - hm_[pY - 1, pX]])\n",
    "                preds[i, j].add_(diff.sign_().mul_(.25))\n",
    "\n",
    "    preds.add_(-0.5)\n",
    "    return preds\n",
    "\n",
    "\n",
    "class HourGlass(nn.Module):\n",
    "    def __init__(self, num_modules, depth, num_features, first_one=False):\n",
    "        super(HourGlass, self).__init__()\n",
    "        self.num_modules = num_modules\n",
    "        self.depth = depth\n",
    "        self.features = num_features\n",
    "        self.coordconv = CoordConvTh(64, 64, True, True, 256, first_one,\n",
    "                                     out_channels=256,\n",
    "                                     kernel_size=1, stride=1, padding=0)\n",
    "        self._generate_network(self.depth)\n",
    "\n",
    "    def _generate_network(self, level):\n",
    "        self.add_module('b1_' + str(level), ConvBlock(256, 256))\n",
    "        self.add_module('b2_' + str(level), ConvBlock(256, 256))\n",
    "        if level > 1:\n",
    "            self._generate_network(level - 1)\n",
    "        else:\n",
    "            self.add_module('b2_plus_' + str(level), ConvBlock(256, 256))\n",
    "        self.add_module('b3_' + str(level), ConvBlock(256, 256))\n",
    "\n",
    "    def _forward(self, level, inp):\n",
    "        up1 = inp\n",
    "        up1 = self._modules['b1_' + str(level)](up1)\n",
    "        low1 = F.avg_pool2d(inp, 2, stride=2)\n",
    "        low1 = self._modules['b2_' + str(level)](low1)\n",
    "\n",
    "        if level > 1:\n",
    "            low2 = self._forward(level - 1, low1)\n",
    "        else:\n",
    "            low2 = low1\n",
    "            low2 = self._modules['b2_plus_' + str(level)](low2)\n",
    "        low3 = low2\n",
    "        low3 = self._modules['b3_' + str(level)](low3)\n",
    "        up2 = F.interpolate(low3, scale_factor=2, mode='nearest')\n",
    "\n",
    "        return up1 + up2\n",
    "\n",
    "    def forward(self, x, heatmap):\n",
    "        x, last_channel = self.coordconv(x, heatmap)\n",
    "        return self._forward(self.depth, x), last_channel\n",
    "\n",
    "\n",
    "class AddCoordsTh(nn.Module):\n",
    "    def __init__(self, height=64, width=64, with_r=False, with_boundary=False):\n",
    "        super(AddCoordsTh, self).__init__()\n",
    "        self.with_r = with_r\n",
    "        self.with_boundary = with_boundary\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_coords = torch.arange(height).unsqueeze(1).expand(height, width).float()\n",
    "            y_coords = torch.arange(width).unsqueeze(0).expand(height, width).float()\n",
    "            x_coords = (x_coords / (height - 1)) * 2 - 1\n",
    "            y_coords = (y_coords / (width - 1)) * 2 - 1\n",
    "            coords = torch.stack([x_coords, y_coords], dim=0)  # (2, height, width)\n",
    "\n",
    "            if self.with_r:\n",
    "                rr = torch.sqrt(torch.pow(x_coords, 2) + torch.pow(y_coords, 2))  # (height, width)\n",
    "                rr = (rr / torch.max(rr)).unsqueeze(0)\n",
    "                coords = torch.cat([coords, rr], dim=0)\n",
    "\n",
    "            self.coords = coords.unsqueeze(0).to(device)  # (1, 2 or 3, height, width)\n",
    "            self.x_coords = x_coords.to(device)\n",
    "            self.y_coords = y_coords.to(device)\n",
    "\n",
    "    def forward(self, x, heatmap=None):\n",
    "        \"\"\"\n",
    "        x: (batch, c, x_dim, y_dim)\n",
    "        \"\"\"\n",
    "        coords = self.coords.repeat(x.size(0), 1, 1, 1)\n",
    "\n",
    "        if self.with_boundary and heatmap is not None:\n",
    "            boundary_channel = torch.clamp(heatmap[:, -1:, :, :], 0.0, 1.0)\n",
    "            zero_tensor = torch.zeros_like(self.x_coords)\n",
    "            xx_boundary_channel = torch.where(boundary_channel > 0.05, self.x_coords, zero_tensor).to(zero_tensor.device)\n",
    "            yy_boundary_channel = torch.where(boundary_channel > 0.05, self.y_coords, zero_tensor).to(zero_tensor.device)\n",
    "            coords = torch.cat([coords, xx_boundary_channel, yy_boundary_channel], dim=1)\n",
    "\n",
    "        x_and_coords = torch.cat([x, coords], dim=1)\n",
    "        return x_and_coords\n",
    "\n",
    "\n",
    "class CoordConvTh(nn.Module):\n",
    "    \"\"\"CoordConv layer as in the paper.\"\"\"\n",
    "    def __init__(self, height, width, with_r, with_boundary,\n",
    "                 in_channels, first_one=False, *args, **kwargs):\n",
    "        super(CoordConvTh, self).__init__()\n",
    "        self.addcoords = AddCoordsTh(height, width, with_r, with_boundary)\n",
    "        in_channels += 2\n",
    "        if with_r:\n",
    "            in_channels += 1\n",
    "        if with_boundary and not first_one:\n",
    "            in_channels += 2\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, *args, **kwargs)\n",
    "\n",
    "    def forward(self, input_tensor, heatmap=None):\n",
    "        ret = self.addcoords(input_tensor, heatmap)\n",
    "        last_channel = ret[:, -2:, :, :]\n",
    "        ret = self.conv(ret)\n",
    "        return ret, last_channel\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        conv3x3 = partial(nn.Conv2d, kernel_size=3, stride=1, padding=1, bias=False, dilation=1)\n",
    "        self.conv1 = conv3x3(in_planes, int(out_planes / 2))\n",
    "        self.bn2 = nn.BatchNorm2d(int(out_planes / 2))\n",
    "        self.conv2 = conv3x3(int(out_planes / 2), int(out_planes / 4))\n",
    "        self.bn3 = nn.BatchNorm2d(int(out_planes / 4))\n",
    "        self.conv3 = conv3x3(int(out_planes / 4), int(out_planes / 4))\n",
    "\n",
    "        self.downsample = None\n",
    "        if in_planes != out_planes:\n",
    "            self.downsample = nn.Sequential(nn.BatchNorm2d(in_planes),\n",
    "                                            nn.ReLU(True),\n",
    "                                            nn.Conv2d(in_planes, out_planes, 1, 1, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out1 = self.bn1(x)\n",
    "        out1 = F.relu(out1, True)\n",
    "        out1 = self.conv1(out1)\n",
    "\n",
    "        out2 = self.bn2(out1)\n",
    "        out2 = F.relu(out2, True)\n",
    "        out2 = self.conv2(out2)\n",
    "\n",
    "        out3 = self.bn3(out2)\n",
    "        out3 = F.relu(out3, True)\n",
    "        out3 = self.conv3(out3)\n",
    "\n",
    "        out3 = torch.cat((out1, out2, out3), 1)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(residual)\n",
    "        out3 += residual\n",
    "        return out3\n",
    "\n",
    "\n",
    "class FAN(nn.Module):\n",
    "    def __init__(self, num_modules=1, end_relu=False, num_landmarks=98, fname_pretrained=None):\n",
    "        super(FAN, self).__init__()\n",
    "        self.num_modules = num_modules\n",
    "        self.end_relu = end_relu\n",
    "\n",
    "        # Base part\n",
    "        self.conv1 = CoordConvTh(256, 256, True, False,\n",
    "                                 in_channels=3, out_channels=64,\n",
    "                                 kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = ConvBlock(64, 128)\n",
    "        self.conv3 = ConvBlock(128, 128)\n",
    "        self.conv4 = ConvBlock(128, 256)\n",
    "\n",
    "        # Stacking part\n",
    "        self.add_module('m0', HourGlass(1, 4, 256, first_one=True))\n",
    "        self.add_module('top_m_0', ConvBlock(256, 256))\n",
    "        self.add_module('conv_last0', nn.Conv2d(256, 256, 1, 1, 0))\n",
    "        self.add_module('bn_end0', nn.BatchNorm2d(256))\n",
    "        self.add_module('l0', nn.Conv2d(256, num_landmarks+1, 1, 1, 0))\n",
    "\n",
    "        if fname_pretrained is not None:\n",
    "            self.load_pretrained_weights(fname_pretrained)\n",
    "\n",
    "    def load_pretrained_weights(self, fname):\n",
    "        if torch.cuda.is_available():\n",
    "            checkpoint = torch.load(fname)\n",
    "        else:\n",
    "            checkpoint = torch.load(fname, map_location=torch.device('cpu'))\n",
    "        model_weights = self.state_dict()\n",
    "        model_weights.update({k: v for k, v in checkpoint['state_dict'].items()\n",
    "                              if k in model_weights})\n",
    "        self.load_state_dict(model_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x), True)\n",
    "        x = F.avg_pool2d(self.conv2(x), 2, stride=2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        outputs = []\n",
    "        boundary_channels = []\n",
    "        tmp_out = None\n",
    "        ll, boundary_channel = self._modules['m0'](x, tmp_out)\n",
    "        ll = self._modules['top_m_0'](ll)\n",
    "        ll = F.relu(self._modules['bn_end0']\n",
    "                    (self._modules['conv_last0'](ll)), True)\n",
    "\n",
    "        # Predict heatmaps\n",
    "        tmp_out = self._modules['l0'](ll)\n",
    "        if self.end_relu:\n",
    "            tmp_out = F.relu(tmp_out)  # HACK: Added relu\n",
    "        outputs.append(tmp_out)\n",
    "        boundary_channels.append(boundary_channel)\n",
    "        return outputs, boundary_channels\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_heatmap(self, x, b_preprocess=True):\n",
    "        ''' outputs 0-1 normalized heatmap '''\n",
    "        x = F.interpolate(x, size=256, mode='bilinear')\n",
    "        x_01 = x*0.5 + 0.5\n",
    "        outputs, _ = self(x_01)\n",
    "        heatmaps = outputs[-1][:, :-1, :, :]\n",
    "        scale_factor = x.size(2) // heatmaps.size(2)\n",
    "        if b_preprocess:\n",
    "            heatmaps = F.interpolate(heatmaps, scale_factor=scale_factor,\n",
    "                                     mode='bilinear', align_corners=True)\n",
    "            heatmaps = preprocess(heatmaps)\n",
    "        return heatmaps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_landmark(self, x):\n",
    "        ''' outputs landmarks of x.shape '''\n",
    "        heatmaps = self.get_heatmap(x, b_preprocess=False)\n",
    "        landmarks = []\n",
    "        for i in range(x.size(0)):\n",
    "            pred_landmarks = get_preds_fromhm(heatmaps[i].cpu().unsqueeze(0))\n",
    "            landmarks.append(pred_landmarks)\n",
    "        scale_factor = x.size(2) // heatmaps.size(2)\n",
    "        landmarks = torch.cat(landmarks) * scale_factor\n",
    "        return landmarks\n",
    "\n",
    "\n",
    "# ========================== #\n",
    "#   Align related functions  #\n",
    "# ========================== #\n",
    "\n",
    "\n",
    "def tensor2numpy255(tensor):\n",
    "    \"\"\"Converts torch tensor to numpy array.\"\"\"\n",
    "    return ((tensor.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5) * 255).astype('uint8')\n",
    "\n",
    "\n",
    "def np2tensor(image):\n",
    "    \"\"\"Converts numpy array to torch tensor.\"\"\"\n",
    "    return torch.FloatTensor(image).permute(2, 0, 1) / 255 * 2 - 1\n",
    "\n",
    "\n",
    "class FaceAligner():\n",
    "    def __init__(self, fname_wing, fname_celeba_mean, output_size):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.fan = FAN(fname_pretrained=fname_wing).to(self.device).eval()\n",
    "        scale = output_size // 256\n",
    "        self.CELEB_REF = np.float32(np.load(fname_celeba_mean)['mean']) * scale\n",
    "        self.xaxis_ref = landmarks2xaxis(self.CELEB_REF)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def align(self, imgs, output_size=256):\n",
    "        ''' imgs = torch.CUDATensor of BCHW '''\n",
    "        imgs = imgs.to(self.device)\n",
    "        landmarkss = self.fan.get_landmark(imgs).cpu().numpy()\n",
    "        for i, (img, landmarks) in enumerate(zip(imgs, landmarkss)):\n",
    "            img_np = tensor2numpy255(img)\n",
    "            img_np, landmarks = pad_mirror(img_np, landmarks)\n",
    "            transform = self.landmarks2mat(landmarks)\n",
    "            rows, cols, _ = img_np.shape\n",
    "            rows = max(rows, self.output_size)\n",
    "            cols = max(cols, self.output_size)\n",
    "            aligned = cv2.warpPerspective(img_np, transform, (cols, rows), flags=cv2.INTER_LANCZOS4)\n",
    "            imgs[i] = np2tensor(aligned[:self.output_size, :self.output_size, :])\n",
    "        return imgs\n",
    "\n",
    "    def landmarks2mat(self, landmarks):\n",
    "        T_origin = points2T(landmarks, 'from')\n",
    "        xaxis_src = landmarks2xaxis(landmarks)\n",
    "        R = vecs2R(xaxis_src, self.xaxis_ref)\n",
    "        S = landmarks2S(landmarks, self.CELEB_REF)\n",
    "        T_ref = points2T(self.CELEB_REF, 'to')\n",
    "        matrix = np.dot(T_ref, np.dot(S, np.dot(R, T_origin)))\n",
    "        return matrix\n",
    "\n",
    "\n",
    "def points2T(point, direction):\n",
    "    point_mean = point.mean(axis=0)\n",
    "    T = np.eye(3)\n",
    "    coef = -1 if direction == 'from' else 1\n",
    "    T[:2, 2] = coef * point_mean\n",
    "    return T\n",
    "\n",
    "\n",
    "def landmarks2eyes(landmarks):\n",
    "    idx_left = np.array(list(range(60, 67+1)) + [96])\n",
    "    idx_right = np.array(list(range(68, 75+1)) + [97])\n",
    "    left = landmarks[idx_left]\n",
    "    right = landmarks[idx_right]\n",
    "    return left.mean(axis=0), right.mean(axis=0)\n",
    "\n",
    "\n",
    "def landmarks2mouthends(landmarks):\n",
    "    left = landmarks[76]\n",
    "    right = landmarks[82]\n",
    "    return left, right\n",
    "\n",
    "\n",
    "def rotate90(vec):\n",
    "    x, y = vec\n",
    "    return np.array([y, -x])\n",
    "\n",
    "\n",
    "def landmarks2xaxis(landmarks):\n",
    "    eye_left, eye_right = landmarks2eyes(landmarks)\n",
    "    mouth_left, mouth_right = landmarks2mouthends(landmarks)\n",
    "    xp = eye_right - eye_left  # x' in pggan\n",
    "    eye_center = (eye_left + eye_right) * 0.5\n",
    "    mouth_center = (mouth_left + mouth_right) * 0.5\n",
    "    yp = eye_center - mouth_center\n",
    "    xaxis = xp - rotate90(yp)\n",
    "    return xaxis / np.linalg.norm(xaxis)\n",
    "\n",
    "\n",
    "def vecs2R(vec_x, vec_y):\n",
    "    vec_x = vec_x / np.linalg.norm(vec_x)\n",
    "    vec_y = vec_y / np.linalg.norm(vec_y)\n",
    "    c = np.dot(vec_x, vec_y)\n",
    "    s = np.sqrt(1 - c * c) * np.sign(np.cross(vec_x, vec_y))\n",
    "    R = np.array(((c, -s, 0), (s, c, 0), (0, 0, 1)))\n",
    "    return R\n",
    "\n",
    "\n",
    "def landmarks2S(x, y):\n",
    "    x_mean = x.mean(axis=0).squeeze()\n",
    "    y_mean = y.mean(axis=0).squeeze()\n",
    "    # vectors = mean -> each point\n",
    "    x_vectors = x - x_mean\n",
    "    y_vectors = y - y_mean\n",
    "\n",
    "    x_norms = np.linalg.norm(x_vectors, axis=1)\n",
    "    y_norms = np.linalg.norm(y_vectors, axis=1)\n",
    "\n",
    "    indices = [96, 97, 76, 82]  # indices for eyes, lips\n",
    "    scale = (y_norms / x_norms)[indices].mean()\n",
    "\n",
    "    S = np.eye(3)\n",
    "    S[0, 0] = S[1, 1] = scale\n",
    "    return S\n",
    "\n",
    "\n",
    "def pad_mirror(img, landmarks):\n",
    "    H, W, _ = img.shape\n",
    "    img = np.pad(img, ((H//2, H//2), (W//2, W//2), (0, 0)), 'reflect')\n",
    "    small_blurred = gaussian(cv2.resize(img, (W, H)), H//100, multichannel=True)\n",
    "    blurred = cv2.resize(small_blurred, (W * 2, H * 2)) * 255\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    coords = np.meshgrid(np.arange(H), np.arange(W), indexing=\"ij\")\n",
    "    weight_y = np.clip(coords[0] / (H//4), 0, 1)\n",
    "    weight_x = np.clip(coords[1] / (H//4), 0, 1)\n",
    "    weight_y = np.minimum(weight_y, np.flip(weight_y, axis=0))\n",
    "    weight_x = np.minimum(weight_x, np.flip(weight_x, axis=1))\n",
    "    weight = np.expand_dims(np.minimum(weight_y, weight_x), 2)**4\n",
    "    img = img * weight + blurred * (1 - weight)\n",
    "    landmarks += np.array([W//4, H//4])\n",
    "    return img, landmarks\n",
    "\n",
    "\n",
    "def align_faces(args, input_dir, output_dir):\n",
    "    import os\n",
    "    from torchvision import transforms\n",
    "    from PIL import Image\n",
    "    from core.utils import save_image\n",
    "\n",
    "    aligner = FaceAligner(args.wing_path, args.lm_path, args.img_size)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((args.img_size, args.img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                             std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    fnames = os.listdir(input_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    fnames.sort()\n",
    "    for fname in fnames:\n",
    "        image = Image.open(os.path.join(input_dir, fname)).convert('RGB')\n",
    "        x = transform(image).unsqueeze(0)\n",
    "        x_aligned = aligner.align(x)\n",
    "        save_image(x_aligned, 1, filename=os.path.join(output_dir, fname))\n",
    "        print('Saved the aligned image to %s...' % fname)\n",
    "\n",
    "\n",
    "# ========================== #\n",
    "#   Mask related functions   #\n",
    "# ========================== #\n",
    "\n",
    "\n",
    "def normalize(x, eps=1e-6):\n",
    "    \"\"\"Apply min-max normalization.\"\"\"\n",
    "    x = x.contiguous()\n",
    "    N, C, H, W = x.size()\n",
    "    x_ = x.view(N*C, -1)\n",
    "    max_val = torch.max(x_, dim=1, keepdim=True)[0]\n",
    "    min_val = torch.min(x_, dim=1, keepdim=True)[0]\n",
    "    x_ = (x_ - min_val) / (max_val - min_val + eps)\n",
    "    out = x_.view(N, C, H, W)\n",
    "    return out\n",
    "\n",
    "\n",
    "def truncate(x, thres=0.1):\n",
    "    \"\"\"Remove small values in heatmaps.\"\"\"\n",
    "    return torch.where(x < thres, torch.zeros_like(x), x)\n",
    "\n",
    "\n",
    "def resize(x, p=2):\n",
    "    \"\"\"Resize heatmaps.\"\"\"\n",
    "    return x**p\n",
    "\n",
    "\n",
    "def shift(x, N):\n",
    "    \"\"\"Shift N pixels up or down.\"\"\"\n",
    "    up = N >= 0\n",
    "    N = abs(N)\n",
    "    _, _, H, W = x.size()\n",
    "    head = torch.arange(N)\n",
    "    tail = torch.arange(H-N)\n",
    "\n",
    "    if up:\n",
    "        head = torch.arange(H-N)+N\n",
    "        tail = torch.arange(N)\n",
    "    else:\n",
    "        head = torch.arange(N) + (H-N)\n",
    "        tail = torch.arange(H-N)\n",
    "\n",
    "    # permutation indices\n",
    "    perm = torch.cat([head, tail]).to(x.device)\n",
    "    out = x[:, :, perm, :]\n",
    "    return out\n",
    "\n",
    "\n",
    "IDXPAIR = namedtuple('IDXPAIR', 'start end')\n",
    "index_map = Munch(chin=IDXPAIR(0 + 8, 33 - 8),\n",
    "                  eyebrows=IDXPAIR(33, 51),\n",
    "                  eyebrowsedges=IDXPAIR(33, 46),\n",
    "                  nose=IDXPAIR(51, 55),\n",
    "                  nostrils=IDXPAIR(55, 60),\n",
    "                  eyes=IDXPAIR(60, 76),\n",
    "                  lipedges=IDXPAIR(76, 82),\n",
    "                  lipupper=IDXPAIR(77, 82),\n",
    "                  liplower=IDXPAIR(83, 88),\n",
    "                  lipinner=IDXPAIR(88, 96))\n",
    "OPPAIR = namedtuple('OPPAIR', 'shift resize')\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    \"\"\"Preprocess 98-dimensional heatmaps.\"\"\"\n",
    "    N, C, H, W = x.size()\n",
    "    x = truncate(x)\n",
    "    x = normalize(x)\n",
    "\n",
    "    sw = H // 256\n",
    "    operations = Munch(chin=OPPAIR(0, 3),\n",
    "                       eyebrows=OPPAIR(-7*sw, 2),\n",
    "                       nostrils=OPPAIR(8*sw, 4),\n",
    "                       lipupper=OPPAIR(-8*sw, 4),\n",
    "                       liplower=OPPAIR(8*sw, 4),\n",
    "                       lipinner=OPPAIR(-2*sw, 3))\n",
    "\n",
    "    for part, ops in operations.items():\n",
    "        start, end = index_map[part]\n",
    "        x[:, start:end] = resize(shift(x[:, start:end], ops.shift), ops.resize)\n",
    "\n",
    "    zero_out = torch.cat([torch.arange(0, index_map.chin.start),\n",
    "                          torch.arange(index_map.chin.end, 33),\n",
    "                          torch.LongTensor([index_map.eyebrowsedges.start,\n",
    "                                            index_map.eyebrowsedges.end,\n",
    "                                            index_map.lipedges.start,\n",
    "                                            index_map.lipedges.end])])\n",
    "    x[:, zero_out] = 0\n",
    "\n",
    "    start, end = index_map.nose\n",
    "    x[:, start+1:end] = shift(x[:, start+1:end], 4*sw)\n",
    "    x[:, start:end] = resize(x[:, start:end], 1)\n",
    "\n",
    "    start, end = index_map.eyes\n",
    "    x[:, start:end] = resize(x[:, start:end], 1)\n",
    "    x[:, start:end] = resize(shift(x[:, start:end], -8), 3) + \\\n",
    "        shift(x[:, start:end], -24)\n",
    "\n",
    "    # Second-level mask\n",
    "    x2 = deepcopy(x)\n",
    "    x2[:, index_map.chin.start:index_map.chin.end] = 0  # start:end was 0:33\n",
    "    x2[:, index_map.lipedges.start:index_map.lipinner.end] = 0  # start:end was 76:96\n",
    "    x2[:, index_map.eyebrows.start:index_map.eyebrows.end] = 0  # start:end was 33:51\n",
    "\n",
    "    x = torch.sum(x, dim=1, keepdim=True)  # (N, 1, H, W)\n",
    "    x2 = torch.sum(x2, dim=1, keepdim=True)  # mask without faceline and mouth\n",
    "\n",
    "    x[x != x] = 0  # set nan to zero\n",
    "    x2[x != x] = 0  # set nan to zero\n",
    "    return x.clamp_(0, 1), x2.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5e4edae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval.py\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_metrics(nets, args, step, mode):\n",
    "    print('Calculating evaluation metrics...')\n",
    "    assert mode in ['latent', 'reference']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    domains = os.listdir(args.val_img_dir)\n",
    "    domains.sort()\n",
    "    num_domains = len(domains)\n",
    "    print('Number of domains: %d' % num_domains)\n",
    "\n",
    "    lpips_dict = OrderedDict()\n",
    "    for trg_idx, trg_domain in enumerate(domains):\n",
    "        src_domains = [x for x in domains if x != trg_domain]\n",
    "\n",
    "        if mode == 'reference':\n",
    "            path_ref = os.path.join(args.val_img_dir, trg_domain)\n",
    "            loader_ref = get_eval_loader(root=path_ref,\n",
    "                                         img_size=args.img_size,\n",
    "                                         batch_size=args.val_batch_size,\n",
    "                                         imagenet_normalize=False,\n",
    "                                         drop_last=True)\n",
    "\n",
    "        for src_idx, src_domain in enumerate(src_domains):\n",
    "            path_src = os.path.join(args.val_img_dir, src_domain)\n",
    "            loader_src = get_eval_loader(root=path_src,\n",
    "                                         img_size=args.img_size,\n",
    "                                         batch_size=args.val_batch_size,\n",
    "                                         imagenet_normalize=False)\n",
    "\n",
    "            task = '%s2%s' % (src_domain, trg_domain)\n",
    "            path_fake = os.path.join(args.eval_dir, task)\n",
    "            shutil.rmtree(path_fake, ignore_errors=True)\n",
    "            os.makedirs(path_fake)\n",
    "\n",
    "            lpips_values = []\n",
    "            print('Generating images and calculating LPIPS for %s...' % task)\n",
    "            for i, x_src in enumerate(tqdm(loader_src, total=len(loader_src))):\n",
    "                N = x_src.size(0)\n",
    "                x_src = x_src.to(device)\n",
    "                y_trg = torch.tensor([trg_idx] * N).to(device)\n",
    "                masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n",
    "\n",
    "                # generate 10 outputs from the same input\n",
    "                group_of_images = []\n",
    "                for j in range(args.num_outs_per_domain):\n",
    "                    if mode == 'latent':\n",
    "                        z_trg = torch.randn(N, args.latent_dim).to(device)\n",
    "                        s_trg = nets.mapping_network(z_trg, y_trg)\n",
    "                    else:\n",
    "                        try:\n",
    "                            x_ref = next(iter_ref).to(device)\n",
    "                        except:\n",
    "                            iter_ref = iter(loader_ref)\n",
    "                            x_ref = next(iter_ref).to(device)\n",
    "\n",
    "                        if x_ref.size(0) > N:\n",
    "                            x_ref = x_ref[:N]\n",
    "                        s_trg = nets.style_encoder(x_ref, y_trg)\n",
    "\n",
    "                    x_fake = nets.generator(x_src, s_trg, masks=masks)\n",
    "                    group_of_images.append(x_fake)\n",
    "\n",
    "                    # save generated images to calculate FID later\n",
    "                    for k in range(N):\n",
    "                        filename = os.path.join(\n",
    "                            path_fake,\n",
    "                            '%.4i_%.2i.png' % (i*args.val_batch_size+(k+1), j+1))\n",
    "                        utils.save_image(x_fake[k], ncol=1, filename=filename)\n",
    "\n",
    "                lpips_value = calculate_lpips_given_images(group_of_images)\n",
    "                lpips_values.append(lpips_value)\n",
    "\n",
    "            # calculate LPIPS for each task (e.g. cat2dog, dog2cat)\n",
    "            lpips_mean = np.array(lpips_values).mean()\n",
    "            lpips_dict['LPIPS_%s/%s' % (mode, task)] = lpips_mean\n",
    "\n",
    "        # delete dataloaders\n",
    "        del loader_src\n",
    "        if mode == 'reference':\n",
    "            del loader_ref\n",
    "            del iter_ref\n",
    "\n",
    "    # calculate the average LPIPS for all tasks\n",
    "    lpips_mean = 0\n",
    "    for _, value in lpips_dict.items():\n",
    "        lpips_mean += value / len(lpips_dict)\n",
    "    lpips_dict['LPIPS_%s/mean' % mode] = lpips_mean\n",
    "\n",
    "    # report LPIPS values\n",
    "    filename = os.path.join(args.eval_dir, 'LPIPS_%.5i_%s.json' % (step, mode))\n",
    "    utils.save_json(lpips_dict, filename)\n",
    "\n",
    "    # calculate and report fid values\n",
    "    calculate_fid_for_all_tasks(args, domains, step=step, mode=mode)\n",
    "\n",
    "\n",
    "def calculate_fid_for_all_tasks(args, domains, step, mode):\n",
    "    print('Calculating FID for all tasks...')\n",
    "    fid_values = OrderedDict()\n",
    "    for trg_domain in domains:\n",
    "        src_domains = [x for x in domains if x != trg_domain]\n",
    "\n",
    "        for src_domain in src_domains:\n",
    "            task = '%s2%s' % (src_domain, trg_domain)\n",
    "            path_real = os.path.join(args.train_img_dir, trg_domain)\n",
    "            path_fake = os.path.join(args.eval_dir, task)\n",
    "            print('Calculating FID for %s...' % task)\n",
    "            fid_value = calculate_fid_given_paths(\n",
    "                paths=[path_real, path_fake],\n",
    "                img_size=args.img_size,\n",
    "                batch_size=args.val_batch_size)\n",
    "            fid_values['FID_%s/%s' % (mode, task)] = fid_value\n",
    "\n",
    "    # calculate the average FID for all tasks\n",
    "    fid_mean = 0\n",
    "    for _, value in fid_values.items():\n",
    "        fid_mean += value / len(fid_values)\n",
    "    fid_values['FID_%s/mean' % mode] = fid_mean\n",
    "\n",
    "    # report FID values\n",
    "    filename = os.path.join(args.eval_dir, 'FID_%.5i_%s.json' % (step, mode))\n",
    "    utils.save_json(fid_values, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a0d712da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fid.py\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(x): return x\n",
    "\n",
    "\n",
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        inception = models.inception_v3(pretrained=True)\n",
    "        self.block1 = nn.Sequential(\n",
    "            inception.Conv2d_1a_3x3, inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.block2 = nn.Sequential(\n",
    "            inception.Conv2d_3b_1x1, inception.Conv2d_4a_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.block3 = nn.Sequential(\n",
    "            inception.Mixed_5b, inception.Mixed_5c,\n",
    "            inception.Mixed_5d, inception.Mixed_6a,\n",
    "            inception.Mixed_6b, inception.Mixed_6c,\n",
    "            inception.Mixed_6d, inception.Mixed_6e)\n",
    "        self.block4 = nn.Sequential(\n",
    "            inception.Mixed_7a, inception.Mixed_7b,\n",
    "            inception.Mixed_7c,\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "def frechet_distance(mu, cov, mu2, cov2):\n",
    "    cc, _ = linalg.sqrtm(np.dot(cov, cov2), disp=False)\n",
    "    dist = np.sum((mu -mu2)**2) + np.trace(cov + cov2 - 2*cc)\n",
    "    return np.real(dist)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_fid_given_paths(paths, img_size=256, batch_size=50):\n",
    "    print('Calculating FID given paths %s and %s...' % (paths[0], paths[1]))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    inception = InceptionV3().eval().to(device)\n",
    "    loaders = [get_eval_loader(path, img_size, batch_size) for path in paths]\n",
    "\n",
    "    mu, cov = [], []\n",
    "    for loader in loaders:\n",
    "        actvs = []\n",
    "        for x in tqdm(loader, total=len(loader)):\n",
    "            actv = inception(x.to(device))\n",
    "            actvs.append(actv)\n",
    "        actvs = torch.cat(actvs, dim=0).cpu().detach().numpy()\n",
    "        mu.append(np.mean(actvs, axis=0))\n",
    "        cov.append(np.cov(actvs, rowvar=False))\n",
    "    fid_value = frechet_distance(mu[0], cov[0], mu[1], cov[1])\n",
    "    return fid_value\n",
    "\n",
    "\n",
    "\n",
    "# python -m metrics.fid --paths PATH_REAL PATH_FAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4075ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--paths', type=str, nargs=2, help='paths to real and fake images')\n",
    "#     parser.add_argument('--img_size', type=int, default=256, help='image resolution')\n",
    "#     parser.add_argument('--batch_size', type=int, default=64, help='batch size to use')\n",
    "#     args = parser.parse_args()\n",
    "#     fid_value = calculate_fid_given_paths(args.paths, args.img_size, args.batch_size)\n",
    "#     print('FID: ', fid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9f180136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lpips.py\n",
    "\n",
    "def normalize(x, eps=1e-10):\n",
    "    return x * torch.rsqrt(torch.sum(x**2, dim=1, keepdim=True) + eps)\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = models.alexnet(pretrained=True).features\n",
    "        self.channels = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                self.channels.append(layer.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmaps = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                fmaps.append(x)\n",
    "        return fmaps\n",
    "\n",
    "\n",
    "class Conv1x1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alexnet = AlexNet()\n",
    "        self.lpips_weights = nn.ModuleList()\n",
    "        for channels in self.alexnet.channels:\n",
    "            self.lpips_weights.append(Conv1x1(channels, 1))\n",
    "        self._load_lpips_weights()\n",
    "        # imagenet normalization for range [-1, 1]\n",
    "        self.mu = torch.tensor([-0.03, -0.088, -0.188]).view(1, 3, 1, 1).cuda()\n",
    "        self.sigma = torch.tensor([0.458, 0.448, 0.450]).view(1, 3, 1, 1).cuda()\n",
    "\n",
    "    def _load_lpips_weights(self):\n",
    "        own_state_dict = self.state_dict()\n",
    "        if torch.cuda.is_available():\n",
    "            state_dict = torch.load('metrics/lpips_weights.ckpt')\n",
    "        else:\n",
    "            state_dict = torch.load('metrics/lpips_weights.ckpt',\n",
    "                                    map_location=torch.device('cpu'))\n",
    "        for name, param in state_dict.items():\n",
    "            if name in own_state_dict:\n",
    "                own_state_dict[name].copy_(param)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = (x - self.mu) / self.sigma\n",
    "        y = (y - self.mu) / self.sigma\n",
    "        x_fmaps = self.alexnet(x)\n",
    "        y_fmaps = self.alexnet(y)\n",
    "        lpips_value = 0\n",
    "        for x_fmap, y_fmap, conv1x1 in zip(x_fmaps, y_fmaps, self.lpips_weights):\n",
    "            x_fmap = normalize(x_fmap)\n",
    "            y_fmap = normalize(y_fmap)\n",
    "            lpips_value += torch.mean(conv1x1((x_fmap - y_fmap)**2))\n",
    "        return lpips_value\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_lpips_given_images(group_of_images):\n",
    "    # group_of_images = [torch.randn(N, C, H, W) for _ in range(10)]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    lpips = LPIPS().eval().to(device)\n",
    "    lpips_values = []\n",
    "    num_rand_outputs = len(group_of_images)\n",
    "\n",
    "    # calculate the average of pairwise distances among all random outputs\n",
    "    for i in range(num_rand_outputs-1):\n",
    "        for j in range(i+1, num_rand_outputs):\n",
    "            lpips_values.append(lpips(group_of_images[i], group_of_images[j]))\n",
    "    lpips_value = torch.mean(torch.stack(lpips_values, dim=0))\n",
    "    return lpips_value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "247e9581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--  1 misthios misthios     5628 Apr 21 18:08 lpips_weights.ckpt\r\n"
     ]
    }
   ],
   "source": [
    "#!wget https://github.com/clovaai/stargan-v2/raw/master/metrics/lpips_weights.ckpt\n",
    "!ls -al | grep lpips # needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "562670ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "\n",
    "\n",
    "def subdirs(dname):\n",
    "    return [d for d in os.listdir(dname)\n",
    "            if os.path.isdir(os.path.join(dname, d))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f26fe410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stargan_main(args):\n",
    "    print(args)\n",
    "    cudnn.benchmark = True\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    solver = Solver(args)\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        print(\"args mode train selected.\")\n",
    "        assert len(subdirs(args.train_img_dir)) == args.num_domains\n",
    "        assert len(subdirs(args.val_img_dir)) == args.num_domains\n",
    "        loaders = Munch(src=get_train_loader(root=args.train_img_dir,\n",
    "                                             which='source',\n",
    "                                             img_size=args.img_size,\n",
    "                                             batch_size=args.batch_size,\n",
    "                                             prob=args.randcrop_prob,\n",
    "                                             num_workers=args.num_workers),\n",
    "                        ref=get_train_loader(root=args.train_img_dir,\n",
    "                                             which='reference',\n",
    "                                             img_size=args.img_size,\n",
    "                                             batch_size=args.batch_size,\n",
    "                                             prob=args.randcrop_prob,\n",
    "                                             num_workers=args.num_workers),\n",
    "                        val=get_test_loader(root=args.val_img_dir,\n",
    "                                            img_size=args.img_size,\n",
    "                                            batch_size=args.val_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.num_workers))\n",
    "        solver.train(loaders)\n",
    "    elif args.mode == 'sample':\n",
    "        print(\"args mode sample selected.\")\n",
    "        assert len(subdirs(args.src_dir)) == args.num_domains\n",
    "        assert len(subdirs(args.ref_dir)) == args.num_domains\n",
    "        loaders = Munch(src=get_test_loader(root=args.src_dir,\n",
    "                                            img_size=args.img_size,\n",
    "                                            batch_size=args.val_batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=args.num_workers),\n",
    "                        ref=get_test_loader(root=args.ref_dir,\n",
    "                                            img_size=args.img_size,\n",
    "                                            batch_size=args.val_batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=args.num_workers))\n",
    "        solver.sample(loaders)\n",
    "    elif args.mode == 'eval':\n",
    "        print(\"args mode eval selected.\")\n",
    "        solver.evaluate()\n",
    "    elif args.mode == 'align':\n",
    "        print(\"args mode align selected.\")\n",
    "        from core.wing import align_faces\n",
    "        align_faces(args, args.inp_dir, args.out_dir)\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eed3b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument_parser:\n",
    "    def __init__(self):\n",
    "        self.img_size=256\n",
    "        self.num_domains=2\n",
    "        self.latent_dim=16\n",
    "        self.hidden_dim=512\n",
    "        self.style_dim=64\n",
    "        self.lambda_reg=1\n",
    "        self.lambda_cyc=1\n",
    "        self.lambda_sty=1\n",
    "        self.lambda_ds=1\n",
    "        self.ds_iter=100000\n",
    "        self.w_hpf=1\n",
    "        self.randcrop_prob=0.5\n",
    "        self.total_iters=100000\n",
    "        self.resume_iter=0\n",
    "        self.batch_size=8\n",
    "        self.val_batch_size=32\n",
    "        self.lr=1e-4\n",
    "        self.f_lr=1e-6\n",
    "        self.beta1=0.0\n",
    "        self.beta2=0.99\n",
    "        self.weight_decay=1e-4\n",
    "        self.num_outs_per_domain=10\n",
    "        self.mode = 'sample'\n",
    "        self.num_workers=4\n",
    "        self.seed=777\n",
    "        self.train_img_dir='data/celeba_hq/train'\n",
    "        self.val_img_dir='data/celeba_hq/val'\n",
    "        self.sample_dir='expr/samples'\n",
    "        self.checkpoint_dir='expr/checkpoints'\n",
    "        self.eval_dir='expr/eval'\n",
    "        self.result_dir='expr/results'\n",
    "        self.src_dir='assets/representative/celeba_hq/src'\n",
    "        self.ref_dir='assets/representative/celeba_hq/ref'\n",
    "        self.inp_dir='assets/representative/custom/female'\n",
    "        self.out_dir='assets/representative/celeba_hq/src/female'\n",
    "        self.wing_path='expr/checkpoints/wing.ckpt'\n",
    "        self.lm_path='expr/checkpoints/celeba_lm_mean.npz'\n",
    "        self.print_every=10\n",
    "        self.sample_every=5000\n",
    "        self.save_every=10000\n",
    "        self.eval_every=50000\n",
    "        \n",
    "\n",
    "args = Argument_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7ecfca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # getting the pretrained model for celeba-hq\n",
    "\n",
    "# # adapted from  https://github.com/clovaai/stargan-v2/blob/master/download.sh\n",
    "\n",
    "# URL=\"https://www.dropbox.com/s/96fmei6c93o8b8t/100000_nets_ema.ckpt?dl=0\"\n",
    "# !mkdir -p ./expr/checkpoints/celeba_hq\n",
    "# OUT_FILE=\"./expr/checkpoints/celeba_hq/100000_nets_ema.ckpt\"\n",
    "# !wget -N {URL} -O {OUT_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b040b5",
   "metadata": {},
   "source": [
    "gave error FileNotFoundError: [Errno 2] No such file or directory: 'expr/checkpoints/wing.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the wing checkpoint\n",
    "\n",
    "# adapted from  https://github.com/clovaai/stargan-v2/blob/master/download.sh\n",
    "\n",
    "URL=\"https://www.dropbox.com/s/tjxpypwpt38926e/wing.ckpt?dl=0\"\n",
    "!mkdir -p ./expr/checkpoints/\n",
    "    OUT_FILE=./expr/checkpoints/wing.ckpt\n",
    "    wget -N $URL -O $OUT_FILE\n",
    "    URL=https://www.dropbox.com/s/91fth49gyb7xksk/celeba_lm_mean.npz?dl=0\n",
    "    OUT_FILE=./expr/checkpoints/celeba_lm_mean.npz\n",
    "    wget -N $URL -O $OUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291f05c",
   "metadata": {},
   "source": [
    "### Generating the interpolation videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e341cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python main.py --mode sample --num_domains 2 --resume_iter 100000 --w_hpf 1 \\\n",
    "#                --checkpoint_dir expr/checkpoints/celeba_hq \\\n",
    "#                --result_dir expr/results/celeba_hq \\\n",
    "#                --src_dir assets/representative/celeba_hq/src \\\n",
    "#                --ref_dir assets/representative/celeba_hq/ref\n",
    "\n",
    "args.mode = 'sample'\n",
    "args.num_domains = 2\n",
    "args.resume_iter = 100000\n",
    "args.w_hpf = 1\n",
    "args.checkpoint_dir = 'expr/checkpoints/celeba_hq'\n",
    "args.result_dir='expr/results/celeba_hq'\n",
    "args.src_dir='assets/representative/celeba_hq/src'\n",
    "args.ref_dir='assets/representative/celeba_hq/ref'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bb0a88be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Argument_parser object at 0x7f0f2a099cd0>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'expr/checkpoints/wing.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [133]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstargan_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [129]\u001b[0m, in \u001b[0;36mstargan_main\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      3\u001b[0m cudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m----> 6\u001b[0m solver \u001b[38;5;241m=\u001b[39m \u001b[43mSolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs mode train selected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [120]\u001b[0m, in \u001b[0;36mSolver.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnets, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnets_ema \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# below setattrs are to make networks be children of Solver, e.g., for self.to(self.device)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnets\u001b[38;5;241m.\u001b[39mitems():\n",
      "Input \u001b[0;32mIn [119]\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    276\u001b[0m nets_ema \u001b[38;5;241m=\u001b[39m Munch(generator\u001b[38;5;241m=\u001b[39mgenerator_ema,\n\u001b[1;32m    277\u001b[0m                  mapping_network\u001b[38;5;241m=\u001b[39mmapping_network_ema,\n\u001b[1;32m    278\u001b[0m                  style_encoder\u001b[38;5;241m=\u001b[39mstyle_encoder_ema)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mw_hpf \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 281\u001b[0m     fan \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(\u001b[43mFAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname_pretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwing_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval())\n\u001b[1;32m    282\u001b[0m     fan\u001b[38;5;241m.\u001b[39mget_heatmap \u001b[38;5;241m=\u001b[39m fan\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mget_heatmap\n\u001b[1;32m    283\u001b[0m     nets\u001b[38;5;241m.\u001b[39mfan \u001b[38;5;241m=\u001b[39m fan\n",
      "Input \u001b[0;32mIn [122]\u001b[0m, in \u001b[0;36mFAN.__init__\u001b[0;34m(self, num_modules, end_relu, num_landmarks, fname_pretrained)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml0\u001b[39m\u001b[38;5;124m'\u001b[39m, nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m256\u001b[39m, num_landmarks\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fname_pretrained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_pretrained_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname_pretrained\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [122]\u001b[0m, in \u001b[0;36mFAN.load_pretrained_weights\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_pretrained_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, fname):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 193\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(fname, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'expr/checkpoints/wing.ckpt'"
     ]
    }
   ],
   "source": [
    "stargan_main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
