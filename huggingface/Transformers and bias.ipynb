{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569894e2",
   "metadata": {},
   "source": [
    "## Transformer history\n",
    "\n",
    "une 2018: GPT, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results\n",
    "\n",
    "October 2018: BERT, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)\n",
    "\n",
    "February 2019: GPT-2, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns\n",
    "\n",
    "October 2019: DistilBERT, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERTâ€™s performance\n",
    "\n",
    "October 2019: BART and T5, two large pretrained models using the same architecture as the original Transformer model (the first to do so)\n",
    "\n",
    "May 2020, GPT-3, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called zero-shot learning)\n",
    "\n",
    "Distilbert is really small compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300787c",
   "metadata": {},
   "source": [
    "### Types of transformer models\n",
    "\n",
    "Each of these parts can be used independently, depending on the task:\n",
    "\n",
    "Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
    "\n",
    "Decoder-only models: Good for generative tasks such as text generation.\n",
    "\n",
    "\n",
    "Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e435b1",
   "metadata": {},
   "source": [
    "### Transformer was introduced in \"Attention is all you need\"\n",
    "\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter1/transformers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366849c",
   "metadata": {},
   "source": [
    "### Looking at transformer bias\n",
    "\n",
    "If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4afaa2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3ee346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x000002B784BAE3C0>\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model=\"bert-base-uncased\")\n",
    "\n",
    "result = unmasker(\"This man works as a [MASK]\")\n",
    "\n",
    "print(r[\"token_str\"] for r in result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b50245b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x000002B784BAE580>\n"
     ]
    }
   ],
   "source": [
    "result = unmasker(\"This woman works as a [MASK]\")\n",
    "print(r[\"token_str\"] for r in result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae86cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profession for men :  ['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "Profession for women :  ['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print(\"Profession for men : \", [r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print(\"Profession for women : \", [r[\"token_str\"] for r in result])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
