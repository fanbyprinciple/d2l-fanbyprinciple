{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f68b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9982609152793884},\n",
       " {'label': 'NEGATIVE', 'score': 0.9984269142150879}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"Waiting for hugging face course my whole life\",\n",
    "        \"I hate achu so much\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e49608",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/course/static/chapter2/full_nlp_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0f492",
   "metadata": {},
   "source": [
    "Like other neural networks, Transformer models can‚Äôt process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a tokenizer, which will be responsible for:\n",
    "\n",
    "Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "Mapping each token to an integer\n",
    "\n",
    "Adding additional inputs that may be useful to the model\n",
    "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the Model Hub. To do this, we use the AutoTokenizer class and its from_pretrained() method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model‚Äôs tokenizer and cache it (so it‚Äôs only downloaded the first time you run the code below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f197b",
   "metadata": {},
   "source": [
    "## Trying different parts of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f779a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d95d32",
   "metadata": {},
   "source": [
    "Once we have the tokenizer, we can directly pass our sentences to it and we‚Äôll get back a dictionary that‚Äôs ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.\n",
    "\n",
    "You can use ü§ó Transformers without having to worry about which ML framework is used as a backend; it might be PyTorch or TensorFlow, or Flax for some models. However, Transformer models only accept tensors as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It‚Äôs effectively a tensor; other ML frameworks‚Äô tensors behave similarly, and are usually as simple to instantiate as NumPy arrays.\n",
    "\n",
    "To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f1166",
   "metadata": {},
   "source": [
    "### 1. Extracting tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51833476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52bc0d",
   "metadata": {},
   "source": [
    "### 2. Extracting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087d8386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1537815",
   "metadata": {},
   "source": [
    "This architecture contains only the base Transformer module: given some inputs, it outputs what we‚Äôll call hidden states, also known as features. For each model input, we‚Äôll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
    "\n",
    "If this doesn‚Äôt make sense, don‚Äôt worry about it. We‚Äôll explain it all later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a099e",
   "metadata": {},
   "source": [
    "A high-dimensional vector?\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "Batch size: The number of sequences processed at a time (2 in our example).\n",
    "Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "Hidden size: The vector dimension of each model input.\n",
    "It is said to be ‚Äúhigh dimensional‚Äù because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more).\n",
    "\n",
    "We can see this if we feed the inputs we preprocessed to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37680b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d28edf",
   "metadata": {},
   "source": [
    "Note that the outputs of ü§ó Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs[\"last_hidden_state\"]), or even by index if you know exactly where the thing you are looking for is (outputs[0]).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da83e7d",
   "metadata": {},
   "source": [
    "We will actually be using a sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00768e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dae4f1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926acbc8",
   "metadata": {},
   "source": [
    "### 3. The model Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390aa14",
   "metadata": {},
   "source": [
    "The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b6951d",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/course/static/chapter2/transformer_and_head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd09e88",
   "metadata": {},
   "source": [
    "The output of the Transformer model is sent directly to the model head to be processed.\n",
    "\n",
    "In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences.\n",
    "\n",
    "There are many different architectures available in ü§ó Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:\n",
    "\n",
    "    *Model (retrieve the hidden states)\n",
    "    *ForCausalLM\n",
    "    *ForMaskedLM\n",
    "    *ForMultipleChoice\n",
    "    *ForQuestionAnswering\n",
    "    *ForSequenceClassification\n",
    "    *ForTokenClassification\n",
    "    and others ü§ó\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6ff37a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae2c892",
   "metadata": {},
   "source": [
    "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer (all ü§ó Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc0d536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a772d570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78596915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So predictions: \n",
      "tensor(0.0402, grad_fn=<SelectBackward0>) : NEGATIVE\n",
      "tensor(0.9598, grad_fn=<SelectBackward0>) : POSITIVE\n",
      "tensor(0.9995, grad_fn=<SelectBackward0>) : NEGATIVE\n",
      "tensor(0.0005, grad_fn=<SelectBackward0>) : POSITIVE\n"
     ]
    }
   ],
   "source": [
    "print(\"So predictions: \")\n",
    "print(predictions[0][0], \":\", model.config.id2label[0])\n",
    "print(predictions[0][1], \":\", model.config.id2label[1])\n",
    "print(predictions[1][0], \":\", model.config.id2label[0])\n",
    "print(predictions[1][1], \":\", model.config.id2label[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
