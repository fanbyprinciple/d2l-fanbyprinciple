{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964cf62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dfe3ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0466f",
   "metadata": {},
   "source": [
    "But since it is not trained it has random parameters. Lets check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9dba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d53048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0137ffd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.4633,  0.9221,  1.4738,  ..., -1.7280,  0.5668, -1.6914],\n",
       "         [-1.1234,  0.6128,  0.3426,  ..., -0.6325,  0.6520, -1.3859],\n",
       "         [ 0.1628,  0.0812, -0.0209,  ..., -1.2084, -0.0387, -1.9182],\n",
       "         ...,\n",
       "         [-0.7513,  0.5510,  1.7098,  ..., -1.4004, -0.4175, -1.4172],\n",
       "         [ 0.1127,  1.3571,  2.1423,  ..., -0.3554, -0.0497, -3.1752],\n",
       "         [-0.4074,  1.0997,  0.0817,  ..., -0.0177,  2.7905, -2.3667]],\n",
       "\n",
       "        [[ 0.2139, -0.4123,  0.6040,  ..., -2.3215,  0.5043, -2.4587],\n",
       "         [ 0.3541, -0.0529, -0.2186,  ..., -1.1369, -0.5337, -1.4087],\n",
       "         [-0.7447, -0.0781, -0.2492,  ..., -2.1190, -0.2622, -1.2992],\n",
       "         ...,\n",
       "         [ 0.4526,  0.3877,  0.3314,  ..., -1.4987, -0.2838, -2.3548],\n",
       "         [-0.1296, -0.2364,  1.6389,  ...,  0.1611, -0.3991, -2.7038],\n",
       "         [ 0.2671,  1.0204, -0.1802,  ..., -0.2364,  0.4967, -1.8255]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.5370, -0.8687,  0.0546,  ...,  0.6845,  0.1056, -0.3892],\n",
       "        [ 0.6010, -0.5507,  0.2539,  ...,  0.7602, -0.7441,  0.2469]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7259978",
   "metadata": {},
   "source": [
    "### Getting a bert model from pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f792cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb98485108754642a081a36770041ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a35314ae388442c9a5abf8242e3443d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a065d8",
   "metadata": {},
   "source": [
    "As you saw earlier, we could replace BertModel with the equivalent AutoModel class. Weâ€™ll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task).\n",
    "\n",
    "In the code sample above we didnâ€™t use BertConfig, and instead loaded a pretrained model via the bert-base-cased identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its model card.\n",
    "\n",
    "This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.\n",
    "\n",
    "The weights have been downloaded and cached (so future calls to the from_pretrained() method wonâ€™t re-download them) in the cache folder, which defaults to ~/.cache/huggingface/transformers. You can customize your cache folder by setting the HF_HOME environment variable.\n",
    "\n",
    "The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture. The entire list of available BERT checkpoints can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29108f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./savedModel/bert-base-cased\")\n",
    "#This saves two files to your disk:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979937f",
   "metadata": {},
   "source": [
    "If you take a look at the config.json file, youâ€™ll recognize the attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what ðŸ¤— Transformers version you were using when you last saved the checkpoint.\n",
    "\n",
    "The pytorch_model.bin file is known as the state dictionary; it contains all your modelâ€™s weights. The two files go hand in hand; the configuration is necessary to know your modelâ€™s architecture, while the model weights are your modelâ€™s parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
