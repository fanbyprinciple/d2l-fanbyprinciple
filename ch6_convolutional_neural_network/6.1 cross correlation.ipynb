{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unexpected-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "colored-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X,K):\n",
    "    h,w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] -h+1), (X.shape[1] -w +1))\n",
    "    \n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            mul = (X[i:i+h, j:j+w] * K)\n",
    "#             print(mul)\n",
    "            Y[i,j] = mul.sum()\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "mysterious-factor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18., 26.],\n",
       "        [26., 34.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0,2.0,3.0], [2.0,3.0,4.0],[3.0,4.0,5.0]])\n",
    "K = torch.tensor([[1.0,2.0], [2.0,3.0]])\n",
    "\n",
    "corr2d(X,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "posted-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return corr2d(X, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "quantitative-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Conv2d(K.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sound-balance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5964, -1.3331],\n",
       "        [-1.3331, -2.0698]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-costa",
   "metadata": {},
   "source": [
    "### Edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "competent-writing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((8,8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "metropolitan-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0,-1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "radio-arkansas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X,K)\n",
    "# detects horizontal edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "assured-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(),K)\n",
    "# but not vertical edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "nominated-runner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.]]),\n",
       " torch.Size([8, 7]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((8,8))\n",
    "\n",
    "X[:,2:3] = 0\n",
    "X[:,4:6] = 0\n",
    "\n",
    "print(X)\n",
    "Y = corr2d(X,K)\n",
    "Y, Y.shape\n",
    "# interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "awful-medicaid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-contrast",
   "metadata": {},
   "source": [
    "### Learning a convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "local-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones((6,8))\n",
    "X[:,2:6] = 0\n",
    "\n",
    "Y =  corr2d(X,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "blind-antibody",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 6, 8])\n",
      "torch.Size([1, 1, 6, 7])\n",
      "batch 2, loss 12.246731758117676\n",
      "batch 4, loss 4.0018134117126465\n",
      "batch 6, loss 1.468946099281311\n",
      "batch 8, loss 0.5731261372566223\n",
      "batch 10, loss 0.2299618273973465\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1,1,kernel_size=(1,2), bias=False)\n",
    "\n",
    "X = X.reshape((1,1,6,8))\n",
    "Y = Y.reshape((1,1,6,7))\n",
    "lr = 3e-2\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    \n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    \n",
    "    if (i+1) % 2 == 0:\n",
    "        print(f'batch {i+1}, loss {l.sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "common-color",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0385, -0.9402]]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-buying",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Construct an image X with diagonal edges.\n",
    "    1. What happens if you apply the kernel K in this section to it?\n",
    "        * zero matrix.\n",
    "    2. What happens if you transpose X?\n",
    "        * No change\n",
    "    3. What happens if you transpose K?\n",
    "        * zero matrix.\n",
    "        \n",
    "2. When you try to automatically find the gradient for the Conv2D class we created, what kind\n",
    "of error message do you see?\n",
    "    * I am able to do `net.weights.grad`, when I try `net.grad` I get the error `'Conv2d' object has no attribute 'grad'`\n",
    "\n",
    "3. How do you represent a cross-correlation operation as a matrix multiplication by changing\n",
    "the input and kernel tensors?\n",
    "    * cross correlation is basically matrix multiplication between slices of tensorfrom X of the shape of kernel and summing.\n",
    "    * It can be done by padding Kand X based on what is needed to multiply\n",
    "\n",
    "4. Design some kernels manually.\n",
    "    1. What is the form of a kernel for the second derivative?\n",
    "        * okay in order to compute one way would be to manually compute the second derivative and then let see a kernel be made using backpropogation\n",
    "    2. What is the kernel for an integral?\n",
    "        * how do you actually make it manually\n",
    "3. What is the minimum size of a kernel to obtain a derivative of degree d\n",
    "        * dont know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "greatest-reform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "\n",
    "X =  torch.zeros((8,8))\n",
    "for i in range(X.shape[0]):\n",
    "    X[i][i] = 1\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "assumed-architect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.tensor([[1.0,-1.0]])\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "olympic-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (Y[i:i+h, j : j +w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "complex-settlement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "matched-groove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
       " tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.t(), X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "extreme-buffer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X, K.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "mineral-delta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 7])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "olympic-kidney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.]]]]) tensor([[[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 1.]]]])\n",
      "for epoch 0 loss: 1.5457851886749268\n",
      "for epoch 1 loss: 0.5200021266937256\n",
      "for epoch 2 loss: 0.17492875456809998\n",
      "for epoch 3 loss: 0.05884603410959244\n",
      "for epoch 4 loss: 0.01979580894112587\n",
      "for epoch 5 loss: 0.006659309379756451\n",
      "for epoch 6 loss: 0.0022401916794478893\n",
      "for epoch 7 loss: 0.0007536003831773996\n",
      "for epoch 8 loss: 0.00025351124349981546\n",
      "for epoch 9 loss: 8.528117905370891e-05\n"
     ]
    }
   ],
   "source": [
    "net = nn.Conv2d(1,1,K.shape, bias=False)\n",
    "\n",
    "X = X.reshape(1,1,8,8)\n",
    "Y = Y.reshape(1,1,8,7)\n",
    "\n",
    "print(Y, X)\n",
    "lr = 3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    y_hat = net(X)\n",
    "#     print(y_hat.shape)\n",
    "    l = (y_hat - Y)**2\n",
    "    \n",
    "    net.zero_grad()\n",
    "    \n",
    "    l.sum().backward()\n",
    "    \n",
    "    net.weight.data -= lr * net.weight.grad\n",
    "    \n",
    "    print(f\"for epoch {i} loss: {l.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "available-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0008, -0.0018]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([], size=(1, 0))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net.weight.data[0][0])\n",
    "corr2d(X, net.weight.data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "lyric-poverty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Conv2d' object has no attribute 'grad'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    net.grad\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-queensland",
   "metadata": {},
   "source": [
    "### Strides and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "better-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "pregnant-characteristic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6,8))\n",
    "X[:,2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "macro-birmingham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,1)+ X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "turkish-alfred",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9080, 0.2015, 0.8998, 0.4439, 0.7717, 0.4837, 0.0411, 0.9311],\n",
       "          [0.5910, 0.5630, 0.2239, 0.5713, 0.5696, 0.6923, 0.0312, 0.3428],\n",
       "          [0.8309, 0.5663, 0.7552, 0.7841, 0.0527, 0.8695, 0.9922, 0.9605],\n",
       "          [0.5985, 0.4539, 0.1946, 0.7523, 0.1578, 0.0621, 0.5854, 0.0146],\n",
       "          [0.0588, 0.5586, 0.4790, 0.0938, 0.8431, 0.8413, 0.4735, 0.0134],\n",
       "          [0.1204, 0.5902, 0.2952, 0.9974, 0.6445, 0.9038, 0.0709, 0.2500],\n",
       "          [0.2286, 0.5401, 0.0302, 0.5498, 0.4925, 0.8790, 0.6081, 0.7232],\n",
       "          [0.5011, 0.5229, 0.2495, 0.5114, 0.8894, 0.3726, 0.3668, 0.9662]]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = X.reshape((1,1) +X.shape)\n",
    "Z\n",
    "# 1 ,1 we add for batch and channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "metric-obligation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0983,  1.0385,  0.0000,  0.0000,  0.0000, -0.9402,  0.0983],\n",
       "          [ 0.0983,  1.0385,  0.0000,  0.0000,  0.0000, -0.9402,  0.0983],\n",
       "          [ 0.0983,  1.0385,  0.0000,  0.0000,  0.0000, -0.9402,  0.0983],\n",
       "          [ 0.0983,  1.0385,  0.0000,  0.0000,  0.0000, -0.9402,  0.0983],\n",
       "          [ 0.0983,  1.0385,  0.0000,  0.0000,  0.0000, -0.9402,  0.0983],\n",
       "          [ 0.0983,  1.0385,  0.0000,  0.0000,  0.0000, -0.9402,  0.0983]]]],\n",
       "       grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dominant-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "    X = X.reshape((1,1)+ X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # we dont need batch and channels\n",
    "    return Y.reshape(Y.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "thousand-church",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d= nn.Conv2d(1,1,kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8,8))\n",
    "comp_conv2d(conv2d, X).shape\n",
    "# note that here since padding 1 means to either side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "shared-fossil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d= nn.Conv2d(1,1,kernel_size=(5,3), padding=(2,1))\n",
    "\n",
    "\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "square-occasions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1,1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "allied-powell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1,1, kernel_size=(5,3), padding=(2,1), stride=2)\n",
    "print(X.shape)\n",
    "comp_conv2d(conv2d, X).shape\n",
    "\n",
    "# it halves the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "hearing-auckland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 2])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1,1, kernel_size=(3,5), padding=(0,1), stride=(3,4))\n",
    "conv2d(X.reshape((1,1)+ X.shape)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-mirror",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "1. For the last example in this section, use mathematics to calculate the output shape to see if\n",
    "it is consistent with the experimental result.\n",
    "\n",
    "* it is consistent, |(8 -3 + 0 + 3)/3| , |(8-5+1+4)/4|\n",
    "\n",
    "2. Try other padding and stride combinations on the experiments in this section.\n",
    "\n",
    "*  hmm tried\n",
    "\n",
    "3. For audio signals, what does a stride of 2 correspond to?\n",
    "\n",
    "* it might be two time peridod long\n",
    "\n",
    "4. What are the computational benefits of a stride larger than 1\n",
    "\n",
    "* efficiency in calculation,downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-wilson",
   "metadata": {},
   "source": [
    "##Multiple input and output channels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
