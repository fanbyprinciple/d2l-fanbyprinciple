{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unexpected-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "colored-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X,K):\n",
    "    h,w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] -h+1), (X.shape[1] -w +1))\n",
    "    \n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            mul = (X[i:i+h, j:j+w] * K)\n",
    "#             print(mul)\n",
    "            Y[i,j] = mul.sum()\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mysterious-factor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18., 26.],\n",
       "        [26., 34.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0,2.0,3.0], [2.0,3.0,4.0],[3.0,4.0,5.0]])\n",
    "K = torch.tensor([[1.0,2.0], [2.0,3.0]])\n",
    "\n",
    "corr2d(X,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "posted-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return corr2d(X, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quantitative-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Conv2d(K.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sound-balance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.1519, -5.4360],\n",
       "        [-5.4360, -6.7201]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-costa",
   "metadata": {},
   "source": [
    "### Edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "competent-writing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((8,8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "metropolitan-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0,-1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "radio-arkansas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X,K)\n",
    "# detects horizontal edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "assured-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(),K)\n",
    "# but not vertical edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nominated-runner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.],\n",
       "         [ 0.,  1., -1.,  1.,  0., -1.,  0.]]),\n",
       " torch.Size([8, 7]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((8,8))\n",
    "\n",
    "X[:,2:3] = 0\n",
    "X[:,4:6] = 0\n",
    "\n",
    "print(X)\n",
    "Y = corr2d(X,K)\n",
    "Y, Y.shape\n",
    "# interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "awful-medicaid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-contrast",
   "metadata": {},
   "source": [
    "### Learning a convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "local-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones((6,8))\n",
    "X[:,2:6] = 0\n",
    "\n",
    "Y =  corr2d(X,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "blind-antibody",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 6, 8])\n",
      "torch.Size([1, 1, 6, 7])\n",
      "batch 2, loss 14.947685241699219\n",
      "batch 4, loss 4.777310848236084\n",
      "batch 6, loss 1.7310880422592163\n",
      "batch 8, loss 0.6711881756782532\n",
      "batch 10, loss 0.2685655653476715\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1,1,kernel_size=(1,2), bias=False)\n",
    "\n",
    "X = X.reshape((1,1,6,8))\n",
    "Y = Y.reshape((1,1,6,7))\n",
    "lr = 3e-2\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    \n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    \n",
    "    if (i+1) % 2 == 0:\n",
    "        print(f'batch {i+1}, loss {l.sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "common-color",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0408, -0.9347]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-buying",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Construct an image X with diagonal edges.\n",
    "    1. What happens if you apply the kernel K in this section to it?\n",
    "        * zero matrix.\n",
    "    2. What happens if you transpose X?\n",
    "        * No change\n",
    "    3. What happens if you transpose K?\n",
    "        * zero matrix.\n",
    "        \n",
    "2. When you try to automatically find the gradient for the Conv2D class we created, what kind\n",
    "of error message do you see?\n",
    "    * I am able to do `net.weights.grad`, when I try `net.grad` I get the error `'Conv2d' object has no attribute 'grad'`\n",
    "\n",
    "3. How do you represent a cross-correlation operation as a matrix multiplication by changing\n",
    "the input and kernel tensors?\n",
    "    * cross correlation is basically matrix multiplication between slices of tensorfrom X of the shape of kernel and summing.\n",
    "    * It can be done by padding Kand X based on what is needed to multiply\n",
    "\n",
    "4. Design some kernels manually.\n",
    "    1. What is the form of a kernel for the second derivative?\n",
    "        * okay in order to compute one way would be to manually compute the second derivative and then let see a kernel be made using backpropogation\n",
    "    2. What is the kernel for an integral?\n",
    "        * how do you actually make it manually\n",
    "3. What is the minimum size of a kernel to obtain a derivative of degree d\n",
    "        * dont know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "greatest-reform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "\n",
    "X =  torch.zeros((8,8))\n",
    "for i in range(X.shape[0]):\n",
    "    X[i][i] = 1\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "assumed-architect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.tensor([[1.0,-1.0]])\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "olympic-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (Y[i:i+h, j : j +w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "complex-settlement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "matched-groove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
       " tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.t(), X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "extreme-buffer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X, K.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "mineral-delta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 7])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "olympic-kidney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.]]]]) tensor([[[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 1.]]]])\n",
      "for epoch 0 loss: 1.8485668897628784\n",
      "for epoch 1 loss: 0.6218580007553101\n",
      "for epoch 2 loss: 0.20919303596019745\n",
      "for epoch 3 loss: 0.07037252932786942\n",
      "for epoch 4 loss: 0.023673322051763535\n",
      "for epoch 5 loss: 0.007963704876601696\n",
      "for epoch 6 loss: 0.0026789901312440634\n",
      "for epoch 7 loss: 0.0009012123919092119\n",
      "for epoch 8 loss: 0.0003031678788829595\n",
      "for epoch 9 loss: 0.00010198566451435909\n"
     ]
    }
   ],
   "source": [
    "net = nn.Conv2d(1,1,K.shape, bias=False)\n",
    "\n",
    "X = X.reshape(1,1,8,8)\n",
    "Y = Y.reshape(1,1,8,7)\n",
    "\n",
    "print(Y, X)\n",
    "lr = 3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    y_hat = net(X)\n",
    "#     print(y_hat.shape)\n",
    "    l = (y_hat - Y)**2\n",
    "    \n",
    "    net.zero_grad()\n",
    "    \n",
    "    l.sum().backward()\n",
    "    \n",
    "    net.weight.data -= lr * net.weight.grad\n",
    "    \n",
    "    print(f\"for epoch {i} loss: {l.sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "available-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0019,  0.0012]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([], size=(1, 0))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net.weight.data[0][0])\n",
    "corr2d(X, net.weight.data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "lyric-poverty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Conv2d' object has no attribute 'grad'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    net.grad\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
